---
title: "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information"
date: 2024-03-01T15:26:23+08:00
lastmod: 2024-03-01T15:26:23+08:00
author: ["SwimmingLiu"]

categories:
- 📝 Paper Notes

tags:
- YOLO

keywords:
- YOLOv9

description: "" # 文章描述，与搜索优化相关
summary: "" # 文章简单描述，会展示在主页
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
math: true
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
autonumbering: true # 目录自动编号
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
searchHidden: false # 该页面可以被搜索到
showbreadcrumbs: true #顶部显示当前路径
mermaid: true
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

## Abstract

> 如今的深度学习方法主要关注如何设计**最合适的目标函数**，使模型的预测结果能够最接近真实情况。同时，必须设计一个**适当的架构**，可以帮助**获取足够的信息进行预测**。现有方法忽略了一个事实，即**当输入数据经过逐层特征提取和空间变换时**，**大量信息将会丢失**。本文将深入研究数据通过**深度网络传输时数据丢失的重要问题**，即**信息瓶颈和可逆函数**。我们提出了**可编程梯度信息（PGI）**的概念来应对深度网络实现**多个目标所需的各种变化**。
> PGI可以为**目标任务计算目标函数**提供**完整的输入信息**，从而获得**可靠的梯度信息来更新网络权值**。此外，还设计了一种基于**梯度路径规划的新型轻量级网络架构**——**通用高效层聚合网络（GELAN）**。GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用**传统的卷积算子**即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得**train-from-scratch (从零开始训练) 模型能够比使用大数据集预训练**的state-of-theart模型获得更好的结果，对比结果如图1所示。源代码位于：https： //github.com/WongKinYiu/yolov9。

核心创新点:  依然是网络结构的创新

1. Programmable Gradient Information (PGI) 	
2. Generalized Efficient Layer Aggregation Network（GELAN）

![image-20240301113226341](https://oss.swimmingliu.cn/f6b3ea45-d79d-11ee-a66b-c858c0c1debd)

## Introduction

> 基于深度学习的模型在计算机视觉、语言处理和语音识别等各个领域都表现出了比过去的人工智能系统更好的性能。近年来，深度学习领域的研究人员主要关注如何开发更强大的系统架构和学习方法，例如CNN，Transformers[8,9,40] 、41、60、69、70]，Perceivers[26、26、32、52、56、81、81]和Mambas[17、38、80]。此外，一些研究人员尝试开发更通用的目标函数，例如损失函数[5,45,46,50,77,78]，标签分配[10,12,33,67,79]和辅助监督[18] 、20、24、28、29、51、54、68、76]。上述研究都试图精确地找到**输入和目标任务之间的映射**。然而，大多数过去的方法都忽略了**输入数据在前馈过程中可能会产生不可忽略的信息丢失量**。这种**信息丢失**可能会导致**有偏差的梯度流**，随后用于更新模型。上述问题可能导致深度网络**在目标和输入之间建立不正确的关联**，导致训练后的模型产生不正确的预测。
>
> 在深度网络中，**输入数据在前馈过程中丢失信息的现象**俗称**信息瓶颈**[59]，其示意图如图2所示。目前可以缓解这种现象的主要方法有：（1）**可逆架构**的使用[3,16,19]：该方法主要**使用重复的输入数据，并以显式的方式维护输入数据的信息**； （2）使用**Masked建模**[1,6,9,27,71,73]：主要利用重构损失，采用**隐式方式最大化提取特征并保留输入信息**； （3）引入**深度监督**概念[28,51,54,68]：它利用**没有丢失太多重要信息的浅层特征来预先建立从特征到目标的映射**，以确保**重要信息能够被传递到更深的层次**。然而，上述方法在训练过程和推理过程中都存在不同的缺点。例如，**可逆架构需要额外的层来组合重复馈送的输入数据**，这将显着增加推理成本。另外，由于**输入数据层到输出层不能有太深的路径**，这种限制将导致**在训练过程中难以对高阶语义信息进行建模**。对于 **Masked 建模**，其**重建损失有时与目标损失相冲突**。此外，大多数**掩码机制还会产生与数据的不正确关联**。 对于**深层监督**机制来说，会产生**误差累积**，如果**浅层监督在训练过程中丢失信息**，**后续层将无法检索到所需信息**。上述现象在**困难任务**和**小模型上**会更加**显着**。
>
> 针对上述问题，我们提出了一个新的概念，即**可编程梯度信息（PGI）**。其概念是通过**辅助可逆分支生成可靠的梯度**，使得**深层特征仍然能够保持执行目标任务的关键特征**。 
>
> **辅助可逆分支的设计**可以避免传统的**融合多路径特征的深度监督过程**可能造成的**语义损失**。换句话说，我们在**不同语义层面上编程梯度信息传播**，从而达到最佳的训练结果。 PGI的**可逆架构建立在辅助分支上**，因此**没有额外的成本**。由于PGI可以**自由选择适合目标任务的损失函数**，因此也克服了**Masked建模**遇到的问题。所提出的**PGI机制**可以应用于各种规模的**深度神经网络**，并且比仅适用于**非常深的神经网络**的**深度监督机制更通用**。
>
> 在本文中，我们还基于ELAN[65]设计了**广义ELAN（GELAN）**，GELAN的设计同时**考虑了参数量、计算复杂度、准确性和推理速度**。这种设计允许用户**针对不同的推理设备任意选择合适的计算块**。我们将提出的PGI和GELAN结合起来，然后设计了新一代YOLO系列物体检测系统，我们称之为YOLOv9。我们使用MS COCO数据集进行实验，实验结果验证了我们提出的YOLOv9在所有比较中都取得了顶尖的性能。
>
> 我们总结本文的贡献如下： 
>
> 1. 我们从**可逆函数的角度**对**现有的深度神经网络架构进行了理论分析**，通过这个过程**我们成功地解释了许多过去难以解释的现象**。我们还基于此分析**设计了PGI和辅助可逆分支**，并取得了优异的结果。
>
> 2. 我们设计的PGI解决了**深度监督**只能用于**极深的神经网络架构的问题**，从而让**新的轻量级架构真正应用于日常生活中**。
>
> 3. 我们设计的GELAN仅使用**常规卷积**来实现比基于最先进技术的**深度卷积设计更高的参数利用率**，同时表现出**轻、快速、准确**的巨大优势。
>
> 4. 结合所提出的PGI和GELAN，YOLOv9在MS COCO数据集上的目标检测性能在各个方面都大大超过了现有的实时目标检测器。

Programmable Gradient Information (PGI)：

1. 自由选择适合目标任务的损失函数 

2. 可逆结构建立辅助分支，不增加推理成本
3. 适用于各种规模的深度神经网络

GELAN：

1. 轻、快速、准确 
2. 采用常规卷积吊打其他新颖卷积

## Related work

### 2.1 Real-time Object Detectors

> 目前主流的实时目标检测器是YOLO系列[2,7,13–15,25,30,31,47–49,61–63,74,75]，这些模型大多数使用CSPNet[64]或 ELAN [65] 及其变体作为主要计算单元。在特征集成方面，通常使用改进的PAN[37]或FPN[35]作为工具，然后使用改进的YOLOv3头[49]或FCOS头[57, 58]作为预测头。最近也提出了一些实时目标检测器，例如 RT DETR [43]，其基础是 DETR [4]。然而，由于DETR系列目标检测器在没有相应领域预训练模型的情况下很难应用于新领域，因此目前应用最广泛的实时目标检测器仍然是YOLO系列。本文选择 YOLOv7 [63] 作为开发该方法的基础，该方法已在各种计算机视觉任务和各种场景中被证明有效。
>
> 我们使用 GELAN 来改进所提出的 PGI 的架构和训练过程。上述新颖方法使所提出的 YOLOv9 成为新一代顶级实时目标检测器。

### 2.2 Reversible Architectures

> **可逆架构**[3,16,19]的运算单元必须保持**可逆转换**的特性，因此可以保证**每层运算单元的输出特征图**都能保留**完整的原始信息**。之前，RevCol[3]将传统的**可逆单元推广到多个层次**，这样做可以扩展**不同层单元表达的语义层次**。通过对各种神经网络架构的文献回顾，我们发现有许多高性能架构具有**不同程度的可逆特性**。例如，Res2Net模块[11]以**分层方式将不同的输入分区与下一个分区组合起来**，并在**向后传递之前连接所有转换后的分区**。 CBNet [34, 39]通过**复合主干网重新引入原始输入数据**以获得**完整的原始信息**，并通过各种组合方法获得**不同级别的多级可逆信息**。这些网络架构通常**具有出色的参数利用率**，但额外的复合层**导致推理速度缓慢**。 DynamicDet [36]结合了CBNet [34]和高效实时目标检测器YOLOv7 [63]，在速度、参数数量和精度之间实现了非常好的权衡。本文介绍了 DynamicDet 架构作为**设计可逆分支的基础**。此外，可逆信息被进一步引入到所提出的PGI中。所提出的新架构在**推理过程中不需要额外的连接**，因此可以充分保留速度、参数量和准确性的优势。

### 2.3 Auxiliary Supervision

> **深度监督**[28,54,68]是最常见的**辅助监督方法**，它通过在中**间层插入额外的预测层**来进行训练。尤其是基于Transformer的方法中引入的**多层解码器**的应用是最常见的一种。
>
> 另一种常见的辅助监督方法是利用**相关元信息**来指导**中间层产生的特征图**，并使它们**具有目标任务所需的属性**[18,20,24,29,76]。这种类型的示例包括使用**分割损失**或**深度损失**来提高对象检测器的准确性。
>
> 最近，文献[53,67,82]中有许多报告使用不同的**标签分配方法**来生成不同的**辅助监督机制**，以加快模型的收敛速度，同时提高鲁棒性。然而，**辅助监督机制**通常只适用于大型模型，因此当其应用于轻量级模型时，很容易造成**欠参数化现象**，从而使性能变差。我们提出的PGI设计了一种**重新编程多级语义信息的方法**，这种设计让**轻量级模型也受益于辅助监督机制**。

##  Problem Statement

> 通常，人们将**深度神经网络收敛**问题的困难归因于**梯度消失或梯度饱和**等因素，而这些现象在**传统深度神经网络中确实存在**。然而，现代**深度神经网络已经通过设计各种归一化和激活函数**从根本上解决了上述问题。尽管如此，深度神经网络**仍然存在收敛速度慢或收敛结果差**的问题。
>
> 在本文中，我们进一步探讨上述问题的本质。通过对**信息瓶颈**的深入分析，我们推断出这个问题的根本原因是原本来自**很深网络的初始梯度**在**传输后很快就丢失了实现目标所需的大量信息**。为了证实这一推论，我们将**不同架构的深度网络前馈了初始权重**，**然后将其可视化并在图2中进行说明**。显然，**PlainNet丢失了深层物体检测所需的大量重要信息**。至于**ResNet、CSPNet、GELAN能够保留重要信息的比例**，确实与**训练后能够获得的准确率呈正相关**。我们进一步设计了基于可逆网络的方法来解决上述问题的原因。本节我们将详细阐述对信息瓶颈原理和可逆函数的分析。

![image-20240301141144309](https://oss.swimmingliu.cn/f71e0bea-d79d-11ee-855e-c858c0c1debd)

### 3.1. Information Bottleneck Principle

>根据信息瓶颈原理，我们知道数据X在进行变换时可能会造成信息丢失，如式(1)所示。 
>
>![image-20240301141448840](https://oss.swimmingliu.cn/f75db47c-d79d-11ee-8409-c858c0c1debd)
>
>其中 $I$ 表示相互信息，$f$  和 $g$ 是变换函数，$θ$  和  $ϕ$ 分别是 $f$  和 $g$ 的参数。
>
>在深度神经网络中，$f_θ (·)$ 和 $g_ψ (·)$ 分别表示深度神经网络中**两个连续层的操作**。从方程（1）我们可以预测**，随着网络层数越深，原始数据丢失的可能性就越大**。然而**深度神经网络的参数是基于网络的输出以及给定的目标**，然后通过**计算损失函数生成新的梯度后更新网络**。
>
>可以想象**，更深的神经网络的输出**不太能够**保留有关预测目标的完整信息**。这将使得**在网络训练期间**使用**不完整的信息成为可能**，从而导**致梯度不可靠和收敛性差**。
>
>解决上述问题的一种方法是**直接增加模型的尺寸**。当我们使用**大量的参数来构建模型时**，它**更有能力对数据进行更完整的转换**。上述方法使得**即使在数据前馈过程中信息丢失**，仍然有机会**保留足够的信息来执行到目标的映射**。上述现象解释了为什么在大多数现代模型中**宽度比深度更重要**。然而，上述结论并不能从**根本上解决非常深的神经网络中梯度不可靠**的问题。
>
>下面，我们将介绍如何利用可逆函数来解决问题并进行相关分析。

### 3.2. Reversible Functions

> 当函数 $r$ 有一个逆变换函数 $v$ 时，我们称该函数为可逆函数，如式(2)所示。
>
> ![image-20240301142046119](https://oss.swimmingliu.cn/f76a4bfa-d79d-11ee-9dd7-c858c0c1debd)
>
> 其中 $ψ$ 和  $ζ$ 分别是 $r$ 和 $v$ 的参数。数据 $X$ 通过可逆函数转换而不会丢失信息，如式(3)所示。
>
> ![image-20240301142216688](https://oss.swimmingliu.cn/f776463a-d79d-11ee-8857-c858c0c1debd)
>
> 当网络的变换函数由**可逆函数组成时**，可以获得**更可靠的梯度来更新模型**。当今流行的深度学习方法几乎都是**符合可逆性质的架构**，例如式（4）。
>
> ![image-20240301142339542](https://oss.swimmingliu.cn/f7905b3e-d79d-11ee-ae41-c858c0c1debd)
>
> 其中 $l$ 表示 PreAct ResNet 的第 $l$ 层，$f$ 是第 $l$ 层的变换函数。 PreAct ResNet [22] 以**显式方式重复将原始数据 X 传递到后续层**。这样的设计虽然**可以让一千多层的深度神经网络收敛得很好**，但却破坏了**我们需要深度神经网络的一个重要原因**。也就是说，**对于困难的问题**，我们很难直接找到**简单的映射函数将数据映射到目标**。这也解释了**为什么当层数较少时**，PreAct ResNet 的性能比 ResNet [21] 差。

【Remind】PreAct ResNet  和 ResNet 结构图比较一下

> 此外，我们尝试使用**Masked建模**，使 **Transformer 模型取得重大突破**。我们使用近似方法，例如方程 (5) 尝试求 $r$ 的逆变换 $v$，使得**变换后的特征**能够利用**稀疏特征保留足够的信息**。方程(5) 如下：
>
> ![image-20240301143131016](https://oss.swimmingliu.cn/f7aab2ef-d79d-11ee-8df5-c858c0c1debd)
>
> 其中 $M$ 是**动态二进制掩码**。其他常用于执行上述任务的方法是**扩散模型**和**可变化自动编码器**，它们都具有**查找反函数的功能**。然而，当我们将上述方法应用于轻量级模型时，**就会存在缺陷**，因为**轻量级模型对大量原始数据的参数化不足**。由于上述原因，将**数据 $X$ 映射到目标 $Y$ 的重要信息** $I(Y，X)$ 也会面临同样的问题。对于这个问题，我们将使用**信息瓶颈的概念来探讨它**[59]。信息瓶颈的计算公式如下：
>
> ![image-20240301143317744](https://oss.swimmingliu.cn/f7bd2281-d79d-11ee-8f92-c858c0c1debd)
>
> 一般来说，$I(Y,X)$ 只会占据 $I(X,X)$ 的很小一部分。然而，这对于**目标任务至关重要**。因此，即使**前馈阶段丢失的信息量并不大**，只要覆盖了 $I(Y,X)$，**训练效果就会受到很大影响**。轻量级模型本身处于**欠参数化状态**，因此在**前馈阶段很容易丢失很多重要信息**。因此，我们轻量级模型的目标是如何从 $I(X, X)$ 中准确**过滤**出 $I(Y, X)$。至于**完全保留 $X$ 的信息**，这是**很难做到的**。基于上述分析，我们希望提出一种新的深度神经网络训练方法，不仅能够**生成可靠的梯度来更新模型**，而且适用于**浅层和轻量级神经网络**。

## Methodology

### 4.1 Programmable Gradient Information

> 为了解决上述问题，我们提出了一种新的**辅助监督框架**，称为**可编程梯度信息**（PGI），如图3（d）所示。 PGI主要包括三个组成部分，即（1）**主分支**，（2）辅助可逆分支，（3）**多级辅助信息**。从图3(d)中我们可以看出，**PGI的推理过程仅使用主分支**，因此**不需要任何额外的推理成本**。

![image-20240301143756951](https://oss.swimmingliu.cn/f7d03aa8-d79d-11ee-a606-c858c0c1debd)

> 至于其他两个组件，**它们用于解决或减缓深度学习方法中的几个重要问题**。其中，**辅助可逆分支**是为了**处理神经网络加深带来的问题**而设计的。网络加深会造成**信息瓶颈**，导致**损失函数无法生成可靠的梯度**。对于多级辅助信息，旨在**处理深度监督带来的误差累积问题**，特别是**针对多个预测分支的架构和轻量级模型**。接下来我们将逐步介绍这两个组件。

#### 4.1.1 Auxiliary Reversible Branch

> 在PGI中，我们提出了**辅助可逆分支**来生成**可靠的梯度**并更新网络参数。通过提供从**数据映射到目标的信息**，损失函数可以提供**指导并避免从与目标不太相关的不完整前馈特征中发现错误相关性的可能性**。我们提出通过**引入可逆架构来维护完整信息**，但是**在可逆架构中添加主分支会消耗大量的推理成本**。
>
> 我们分析了图3(b)的架构，发现当**添加从深层到浅层的额外连接时**，**推理时间将增加20%**。当我们反复将输入数据添加到网络的**高分辨率计算层（黄色框）**时，**推理时间甚至超过了两倍**。
>
> 由于我们的目标是使用**可逆架构**来获得可靠的梯度，因此**“可逆”并不是推理阶段的唯一必要条件**。鉴于此，我们将可逆分支视为**深度监督分支的扩展**，然后设计辅助可逆分支，如图3(d)所示。对于由于**信息瓶颈而丢失重要信息**的主分支深度特征，它们将能够从**辅助可逆分支接收可靠的梯度信息**。
>
> 这些**梯度信息**将**驱动参数学习来协助提取正确且重要的信息**，上述动作可以**使主分支获得对目标任务更有效的特征**。此外，**可逆架构在浅层网络上**的表现比**在一般网络上差**，因为复杂的任务需要在更深的网络中进行转换。我们提出的方法并**不强迫主分支保留完整的原始信息**，而是**通过辅助监督机制生成有用的梯度来更新它**。这种设计的优点是所提出的方法也可以**应用于较浅的网络**。
>
> 最后，由于在推理阶段可以去除辅助可逆分支，因此可以**保留原始网络的推理能力**。我们也可以**选择PGI中的任意可逆架构来起到辅助可逆分支的作用**。

#### 4.1.2 Multi-level Auxiliary Information

> 在本节中，我们将讨论**多级辅助信息**如何工作。包括**多个预测分支**的**深度监督架构**如图 3 (c) 所示。对于目标检测，**不同的特征金字塔可用于执行不同的任务**，例如它们一起可以**检测不同大小的目标**。因此，连接到**深度监督分支后**，会引导**浅层特征学习小物体检测所需的特征**，此时**系统会将其他尺寸的物体的位置**视为**背景**。然而，上述行为会导致**深层特征金字塔丢失大量预测目标对象所需的信息**。关于这个问题，我们认为**每个特征金字塔**都需要**接收所有目标对象的信息**，以便**后续的主分支可以保留完整的信息**来学习对各种目标的预测。

辅助可逆分支： (1) 可以还原所有的特征信息 （2）推理的时候，不会额外增加计算量

多级辅助信息： 不分大、中、小目标，把所有物体的特征都汇聚在一起

### 4.2. Generalized ELAN

> 在本节中，我们将描述所提出的新网络架构——**GELAN**。通过结合采用**梯度路径规划设计的两种神经网络架构**CSPNet [64]和ELAN [65]，我们设计了兼顾轻量级、推理速度和准确性的广义高效层聚合网络（GELAN）。其整体架构如图 4 所示。我们将最初**仅使用卷积层堆叠的 ELAN** [65] 的功能推广到**可以使用任何计算块的新架构**。

ELAN 和 GELAN 的区别： 

- ELAN使用的是常规的卷积操作

- GELAN是把卷积换成了任意的模块，最后转到相同的通道数、相同维度大小即可

![image-20240301145407527](https://oss.swimmingliu.cn/f7eef375-d79d-11ee-bdb6-c858c0c1debd)

## Experiments

实验是一篇论文，审稿人看的比较仔细的地方。学习一下别人的写法

附录可以给审稿人和读者，更直观的看到实现的细节

### 5.1. Experimental Setup

> 我们使用 MS COCO 数据集验证了所提出的方法。
>
> 所有实验设置均遵循 YOLOv7 AF [63]，而数据集为 MS COCO 2017 分割。我们提到的所有模型都是使用从头开始训练策略进行训练的，总训练次数为 500 epoch。在设置学习率时，我们在前三个epoch中使用线性预热，随后的epoch根据模型规模设置相应的衰减方式。至于最后 15 个时期，我们关闭马赛克数据增强。更多设置请参考附录。

### 5.2 Implementation Details

> 我们分别基于 **YOLOv7** [63] 和 **Dynamic YOLOv7** [36] 构建了 YOLOv9 的通用版本和扩展版本。
>
> 在网络架构的设计中，我们使用 **CSPNet** 块 [64] 和计划的 RepConv [63] 作为计算块，用 GELAN 替换了 ELAN [65]。我们还简化了下采样模块并优化了无锚预测头。至于PGI的辅助损失部分，我们完全遵循YOLOv7的辅助头设置。详情请参阅附录。

![image-20240301145938462](https://oss.swimmingliu.cn/f822dbbc-d79d-11ee-9116-c858c0c1debd)

### 5.3 Comparison with state-of-the-arts

> 表 1 列出了我们提出的 YOLOv9 与其他从头开始训练的实时目标检测器的比较。总体而言，现有方法中性能最好的方法是**用于轻量级模型的 YOLO MS-S** [7]、用于**中型模型的 YOLO MS** [7]、用于通用模型的 YOLOv7 AF [63] 和**用于大型模型的 YOLOv8-X** [15]。与**轻量级和中型模型YOLO MS**[7]相比，**YOLOv9的参数减少了约10%，计算量减少了5∼15%，但AP仍然有0.4∼0.6%的提升**。与**YOLOv7 AF相比**，**YOLOv9-C的参数减少了42%，计算量减少了21%，但达到了相同的AP（53%）**。与**YOLOv8-X相比**，YOLOv9-X**参数减少15%**，**计算量减少25%**，AP显着提升1.7%。上述对比结果表明，我们提出的YOLOv9与现有方法相比在各方面都有显着改进。
>
> 另一方面，我们也将ImageNet预训练模型纳入对比，**结果如图5所示**。我们分别根据参数和计算量进行比较。就参数数量而言，性能最好的大型模型是 **RT DETR** [43]。从图5中我们可以看到，使用传统卷积的**YOLOv9在参数利用率上**甚至比使用**深度卷积的YOLO MS**还要好。至于大型模型的参数利用率，也大大超过了使用ImageNet预训练模型的**RT DETR**。更棒的是，**在深度模型中，YOLOv9展示了使用PGI的巨大优势**。通过准确**保留和提取将数据映射到目标所需的信息**，我们的方法仅**需要 64% 的参数**，同时保持 **RT DETR-X** 的精度。

![image-20240301150308525](https://oss.swimmingliu.cn/f83c3be4-d79d-11ee-a260-c858c0c1debd)

> 至于计算量，现有最好的模型从最小到最大依次是**YOLO MS** [7]、**PP YOLOE** [74]和**RT DETR** [43]。从图5中我们可以看到，**YOLOv9在计算复杂度方面远远优于从头开始训练的方法**。另外，如果与基于深度卷积和基于ImageNet的预训练模型相比，YOLOv9也很有竞争力。

### 5.4 Ablation Studies

#### 5.4.1 Generalized ELAN

> 对于 **GELAN**，我们首先对**计算模块进行消融研究**。我们分别使用**Res块**[21]、**Dark**块[49]和**CSP**块[64]进行实验。表2表明，用不同的计算块替换ELAN中的卷积层后，系统可以保持良好的性能。用户确实可以自由更换计算块并在各自的推理设备上使用它们。在不同的计算块替换中，**CSP 块的性能特别好**。它们**不仅减少了参数量和计算量**，而且**将 AP 提高了 0.7%**。因此，我们选择CSPELAN作为YOLOv9中GELAN的组成单元。

![image-20240301150606317](https://oss.swimmingliu.cn/f86e2a95-d79d-11ee-b7d5-c858c0c1debd)

>接下来，我们对不同尺寸的**GELAN**进行**ELAN块深度和CSP块深度实验**，并将结果显示在表3中。我们可以看到，当**ELAN的深度从1增加到2时**，**精度显着提高**。但当**深度大于等于2时**，无论是**提高ELAN深度还是CSP深度**，**参数数量、计算量和精度总是呈现线性关系**。这意味着 GELAN 对深度不敏感。
>
>也就是说，用户可以任意组合GELAN中的组件来设计网络架构，无需特殊设计即可拥有性能稳定的模型。在表3中，对于YOLOv9-{S,M,C}，我们将ELAN深度和CSP深度的配对设置为{{2, 3}, {2, 1}, {2, 1}}。

![image-20240301150722270](https://oss.swimmingliu.cn/f8979acd-d79d-11ee-8ed1-c858c0c1debd)

#### 5.4.2 Programmable Gradient Information

> 在PGI方面，我们分别对**backbone**和**neck**的**辅助可逆分支和多级辅助信息**进行了消融研究。我们设计了**辅助可逆分支ICN**来使用**DHLC[34]链接来获取多级可逆信息**。对于多级辅助信息，我们使**用FPN和PAN进行消融研究**，**PFH的作用相当于传统的深度监督**。所有实验的结果列于表4中。从表4中我们可以看出，**PFH仅在深度模型中有效**，而我们提出的**PGI可以在不同组合下提高精度**。尤其是使用**ICN**时，我们得到了稳定且更好的结果。我们还尝试将YOLOv7[63]中提出的lead-head指导分配应用于PGI的辅助监督，并取得了更好的性能。

![image-20240301151231610](https://oss.swimmingliu.cn/f8af98a0-d79d-11ee-9a8b-c858c0c1debd)

> 我们进一步将**PGI和深度监督的概念应用到不同规模**的模型上，并比较结果，结果如表5所示。正如一开始分析的那样，**深度监督的引入会导致浅层模型精度的损失**。对于一般模型来说，引入深度监督会导致性能不稳定，**而深度监督的设计理念只能在极深的模型中带来收益**。所提出的**PGI可以有效处理信息瓶颈和信息破碎等问题**，并且**可以全面提高不同规模模型的准确性**。 PGI 的概念带来了两个宝贵的贡献。第一个是让**辅助监督方法适用于浅层模型**，第二个是**让深层模型训练过程获得更可靠的梯度**。这些梯度使深度模型能够使用更准确的信息来建立数据和目标之间的正确相关性

![image-20240301151535996](https://oss.swimmingliu.cn/f8da863f-d79d-11ee-822a-c858c0c1debd)

> 最后，我们在表中显示了从基线 YOLOv7 到 YOLOv9E 逐渐增加组件的结果。我们提出的GELAN和PGI给模型带来了全面的改进。

![image-20240301151749920](https://oss.swimmingliu.cn/f908a8cb-d79d-11ee-b28a-c858c0c1debd)

### 5.5 Visualization

> 本节将**探讨信息瓶颈问题并将其可视化**。此外，我们**还将可视化所提出的 PGI 如何使用可靠的梯度来找到数据和目标之间的正确相关性**。在图6中，**我们展示了在不同架构下使用随机初始权重作为前馈获得的特征图的可视化结果**。我们可以看到，**随着层数的增加，所有架构的原始信息逐渐减少**。例如，**在PlainNet的第50层，很难看到物体的位置**，并且**所有可区分的特征将在第100层丢失**。对于ResNet，虽然在**第50层仍然可以看到物体的位置，但边界信息已经丢失**。当深度达到第100层时，整个图像变得模糊。 **CSPNet 和提出的 GELAN 都表现得非常好**，并且它们都可以保**持支持清晰识别对象的特征直到第 200 层**。对比中，GELAN结果更稳定，边界信息更清晰

![image-20240301152036432](https://oss.swimmingliu.cn/f91e409b-d79d-11ee-8ec9-c858c0c1debd)

> 图7用于展示PGI是否可以**在训练过程中提供更可靠的梯度**，使得用于**更新的参数能够有效捕获输入数据与目标之间的关系**。图7显示了**GELAN和YOLOv9（GELAN + PGI）**的特征图在**PAN偏置预热中的可视化结果**。从图7（b）和（c）的比较中，我们可以清楚地看到PGI准确而简洁地捕获了包含对象的区域**。对于不使用PGI的GELAN，我们发现它在检测物体边界时存在发散**，并且在某些**背景区域也产生了意想不到的响应**。这个**实验证实了PGI确实可以提供更好的梯度来更新参数，并使主分支的前馈阶段能够保留更重要的特征**。

![image-20240301152145009](https://oss.swimmingliu.cn/f9995762-d79d-11ee-b458-c858c0c1debd)

## Conclusions

> 在本文中，我们提出使用**PGI来解决信息瓶颈问题**以及深**度监督机制**不适**合轻量级神经网络的问题**。我们设计了 GELAN，一个高效、轻量级的神经网络。在物体检测方面，**GELAN在不同的计算块和深度设置下都具有强大且稳定的性能**。它**确实可以广泛扩展为适合各种推理设备的模型**。针对以上两个问题，PGI的引入**使得轻量级模型和深度模型**都获得了精度的显着提升。 PGI和GELAN相结合设计的YOLOv9已经展现出强大的竞争力。**其出色的设计使得深度模型相比YOLOv8减少了49%的参数数量和43%的计算量**，但在MS COCO数据集上仍然有0.6%的AP提升。