[{"content":"Abstract 肺癌是全球最致命的癌症之一，早期诊断对于患者的生存至关重要。肺结节是早期肺癌的主要表现，通常通过 CT 扫描进行评估。如今，计算机辅助诊断系统被广泛用于辅助医生进行疾病诊断。肺结节的准确分割受到内部异质性和外部数据因素的影响。为了克服结节的细微、混合、粘附型、良性和不确定类别的分割挑战，提出了一种新的混合手动特征网络，可增强灵敏度和准确性。该方法通过双分支网络框架和多维融合模块集成特征信息。通过使用多个数据源和不同数据质量进行训练和验证，我们的方法在 LUNA16、多厚度切片图像数据集 (Multi-thickness Slice Image dataset)、LIDC 和 UniToChest 上表现出领先的性能，Dice 相似系数达到 86.89%、75.72%、84.12% 和 80.74分别超过了当前大多数肺结节分割方法。我们的方法进一步提高了肺结节分割任务的准确性、可靠性和稳定性，即使是在具有挑战性的 CT 扫描中也是如此。本研究中使用的代码发布在 GitHub 上，可通过以下 URL (https://github.com/BITEWKRER/DBNet) 获取。\nIntroduction 肺癌是全球癌症相关死亡的主要原因[1]。仅在美国，预计 2023 年将有 127,070 人死于肺癌，占所有癌症死亡的 21% [2]。不幸的是，超过 50% 的肺癌病例发生在发展中国家或不发达国家，与发达国家相比，这些国家的医疗资源有限[3]。\n为了增加生存机会，早期诊断和治疗肺癌仍然至关重要。在中国，研究表明，小于1厘米的I期肺癌的5年生存率为92%。然而，晚期肺癌的5年生存率低得多，仅为7.0%[4]。利用计算机断层扫描 (CT) 进行肺癌筛查已显示出可大幅降低死亡率的潜力 [5]、[6]。低剂量CT是目前肺癌筛查最常用的方法。此外，移动CT的引入有助于解决欠发达国家和偏远地区缺乏CT扫描仪的问题[6]。由于可能没有明显的症状，检测早期肺癌的存在可能会带来重大挑战。\n这种医学背景数据可以直接借鉴，Chatgpt润色改写就完事儿\n在 CT 图像上识别肺结节提供了疾病的关键指标 [1], [3]。这些结节代表圆形异常，其大小各异，直径范围为 3 至 30 毫米 [7]。为了进一步研究肺结节，美国国家癌症研究所组装了“肺部图像数据库联盟和图像数据库资源计划（LIDC）”数据集[8]。\n欠发达地区设备不足、人员不足，导致医生的诊断和治疗时间有限[9]。在这种情况下，医生的工作量很大、重复且耗时[10]、[5]。此外，由于与CT切片相比，肺部结节性病变占据相对较小的面积，长时间和密集的CT筛查可能会导致漏检小的、细微的或 GGO (肺磨玻璃结节) [3]，[6]。为了解决这些问题，计算机辅助诊断系统（CAD）出现并得到了快速发展，特别是随着基于深度学习技术的诊断方法的进步。 CAD系统大大减轻了医生的工作量，最大限度地降低了未发现结节的风险，并提高了肺结节诊断的效率和可靠性。然而，当前用于肺结节分割的 CAD 系统仍然面临一些挑战。\n下面详细阐述了肺结节分割的几个现有挑战，可以从这些挑战入手\n首先，放射科医生标记的肺结节包含九个诊断特征[11]，异质性表型阻碍了肺结节分割的发展。如图1所示，实心结节（a，b）具有清晰的形状和边界，而微妙的GGO结节（e）具有低对比度和模糊的边界[4]，使得网络很容易将它们分类为背景区域。空洞（g）结节降低了网络分割的敏感性，并且由于背景和分割目标之间的极度不平衡，小结节很容易被遗漏[12]。\n由于周围多余的组织结构，血管旁或胸膜旁（c、d、f）可能会导致网络分类错误[13]。此外，部分实性结节（h）比纯GGO更致密，产生更复杂的异质纹理，更容易发展成恶性结节[14]。\n其次，肺结节内部因素造成的分割困难在于医生注释、层厚、数据来源和数据质量。数据质量差或不同医生的经验可能会导致不同的注释和注释者数量。由多名医生注释的病变区域通常更可靠，减少了潜在的临床风险。在资源有限的地区，由于 CT 扫描仪短缺和成像设备陈旧，CT 扫描质量差的情况很常见。较厚的切片更有可能产生“体积平均效应”和伪影，使医生难以达成一致的诊断。即使使用移动 CT 扫描仪也可能无法提供完整的诊断详细信息。最后，目前大多数肺结节分割方法都是基于2D图像，但这些方法忽略了空间关系，因此提出一种有效的3D肺结节分割模型来捕获肺结节的空间位置、纹理和其他详细信息变得越来越重要以避免误诊和漏诊。\nChallenge:\n异质性: 肺结节的形状多异 （实心结节、磨玻璃结节 (GGO) 、空洞结节、血管和胸膜旁边的结节）\n数据集缺陷：数据集质量不好、较厚的切片和医生的不同标注都可能影响最后的分割结果\n2D网络缺陷：忽略了CT信息中的空间关系\n背景知识补充：在医学成像，特别是在使用计算机断层扫描（CT）进行诊断时，“体积平均效应”和“伪影”是两个可能影响图像质量和解读准确性的问题。\n体积平均效应：这是一种由于扫描切片厚度较大而引起的现象。在一个较厚的切片中，多个不同密度的结构可能被平均到同一个体素（三维像素）里。这意味着高密度和低密度区域可能会混合在一起，从而降低了图像的对比度和分辨率。这种效应使得较小的结构，如小肺结节，更难以被准确识别和分割，因为它们可能在较厚的切片中“消失”或与周围组织混合。 伪影：伪影是指在成像过程中由各种原因产生的非实际存在于原始对象中的图像特征。在CT扫描中，伪影可能由患者移动、成像设备的限制、软件处理算法或扫描参数设置不当等因素引起。较厚的切片可能加剧这些伪影，因为每个切片覆盖更大的体积，增加了平均和重建过程中出现错误的机会。 这段话中提到，“较厚的切片更有可能产生‘体积平均效应’和伪影”，意味着在使用较厚切片进行CT扫描时，上述两个问题更容易发生，这会降低图像的质量和可解释性。结果，医生在解读这些图像时可能会遇到困难，因为图像的不清晰和伪影可能导致诊断不一致或误诊。这就强调了使用高分辨率、薄层切片扫描和高质量成像设备的重要性，以提高诊断的准确性和一致性。\n针对上述肺结节的异质性特征，我们提出了一种新的混合手动特征，旨在提供额外的边界和病灶信息，显着提高肺结节分割的灵敏度，减少漏诊的发生，降低像素点的可能性错误分类。为了保证分割结果的可靠性，我们采用多位医师标注结果的平均值作为GroundTruth，并对单位医师标注的样本进行二次筛选。为了确保模型在不同数据源和数据质量上的鲁棒性，我们在不同来源、质量和尺度的数据集上训练模型，并在不同数据源和切片厚度上进行验证。为了有效地整合主、辅分支的特征信息，我们设计了多维融合模块，并从空间和通道维度进行学习，以增强网络的表示和泛化能力。最后，所提出的网络能够进行三维分割，有利于病变信息的全面获取。\n下面是 Main Contribution\n（a）提出了一种新的端到端双分支网络和融合模块，有效完成肺结节分割任务，简单易用，泛化性较好能力。\n（b）提出了混合手动特征来同时增强肺结节边界信息和对比度信息。\n（c）我们的方法显示了分割各种类型的肺结节的性能改进，特别是在细微的、混合纹理的、粘连的、良性的和不确定的结节中。\n(d) 研究并总结了不同层厚和尺寸下模型的性能变化和模式。\nRelated work 目前，肺部结节的分割方法有很多，可以分为基于算法的传统分割方法和数据驱动的深度学习分割方法。然而，传统的肺结节分割方法存在明显的缺陷，特别是当肺结节的边缘变得模糊时，导致性能急剧下降[15]。相比之下，数据驱动的方法表现出更好的性能[16]。\n数据驱动算法 \u0026gt; 传统分割算法 ==\u0026gt; 调研数据驱动算法的论文，利用数据驱动来创新\n多分支网络架构引起了广泛的关注和研究。这些结构擅长集成不同规模、视角和维度的不同特征，从而提高性能。为了解决肺结节的异质性以及结节与其周围环境的相似性的挑战，Wang等人[17]引入了中央聚焦卷积神经网络。该网络从 2D 和 3D CT 图像块中提取肺结节信息，并通过中央池化层保留中央位置的基本细节。 Chen等人[18]引入了Fast Multi-crop Guided Attention，一种基于残差的多尺度引导分割网络来解决这个问题。该方法最初将 2D 和 3D 图像输入单独的分支网络，以从多维相邻轴捕获上下文特征信息。随后，采用全局卷积层来感知和融合上下文特征，同时通过中央池化层增强对图像块的中心区域的关注。 Wang等[19]提出了一种提高不规则肺结节分割效率的方法，同时保持简单结节的准确分割。他们利用双分支框架来处理 CT 图像和边界梯度信息，使用密集注意力模块来关注关键结节特征，并使用多尺度选择性注意力模块来连接不同尺度的特征。此外，他们引入了边界上下文增强模块来合并和增强边缘相关的体素特征，从而实现简单型肺结节的准确分割。 Xu等人[20]提出了一种用于分割非典型结节的双编码融合网络。他们首先使用金字塔上采样方法来创建平滑的病变图像，并减少 CT 图像中高粒度像素的干扰。然后，他们使用全局和局部分支对上采样的 CT 图像块和原始图像块进行编码，以捕获全局和局部信息。此外，他们采用了金字塔池化模块来增强本地分支的输出特征。最后，他们融合并解码了来自双分支的特征信息。 Wang等人[21]提出了一种混合深度学习模型（H-DL），用于分割各种大小和形状的肺结节。该模型结合了基于VGG19的浅层U-Net网络和基于密集连接的深层U-Net网络，增强了复杂肺结节分割的学习能力。与独立的 UNet 结构模型相比，将这两个模型集成到混合模型 H-DL 中证明了分割结果得到了改善。\n基本块的设计对于肺结节分割也具有重要意义。研究人员探索了多种设计概念来提高模型性能和效率。 Wang等人[22]设计了一个基于空洞卷积的深度尺度感知模块来聚合上下文。该模块将具有不同扩张率（1、2和3）的并行分支嵌入到瓶颈层中，以捕获更丰富的语义信息。 Agnes等人[23]提出了一种多尺度全卷积3D UNet模型，其中作者设计了一个多尺度基本块，使用 $3×3×3$ 和$5×5×5$ 卷积从各种尺度中提取特征信息。他们使用 Maxout 激活函数优化多尺度特征信息，抑制低贡献特征。 Chen等人[15]提出了一种用于分割GGO的注意力级联残差网络。该网络通过残差结构和扩张的空间金字塔池模块捕获肺结节的特征信息。在后处理阶段，应用基于体素的条件随机场来进一步细化分割结果。 Zhou等[3]介绍了一种级联的2.5D肺结节检测和分割方法。在分割网络中，作者将卷积块注意力模块（CBAM）[24]合并到编码器中，以增强网络的编码能力。在瓶颈层设计了不同的多尺度卷积扩张率，以实现结节区域的精细分割。\n另一种方法是使用对抗性生成网络进行肺结节分割。训练数据稀缺和类别不平衡一直是影响肺结节分割性能的关键因素。数据稀缺会导致过度拟合或模型收敛失败，而类别不平衡则增加了目标区域分割的挑战。为了解决这些问题，Song等人[25]提出了一种基于生成对抗网络对多种类型肺结节进行全自动分割的端到端架构。该模型包括两个分支。第一个分支用于潜在的肺结节分割和结节生成。第二个分支旨在减少第一个分支产生的潜在假阳性结节。此外，Tyagi 等人[26]引入了一种基于条件生成网络的分割方法。该方法利用生成器（基于具有空间和通道挤压和激励模块的 UNet）和判别器进行对抗训练来学习训练数据集的样本分布，从而提高分割性能。在Luna16数据集上，其DSC得分为80.74%，灵敏度为85.46%。这两种方法背后的动机是利用生成对抗网络的特征来学习肺结节内的抽象特征并生成适用于临床环境的训练样本。虽然这种方法在有足够数量的训练样本时是可行的，但由于肺结节数据集中的样本分布不平衡，它面临着挑战。目前，生成高质量和多样化的数据仍然是一个重大挑战。\nMethod A. Pre-processing 肺结节CT图像预处理步骤如下：\n(a) Ground Truth: 同一结节的Mask 根据质心坐标进行聚类，并使用开源Pyldc工具库对不同医生的注释进行平均，级别设置为0.5[27]。然后获得真实值 (GT)，如图 2 所示。\n(b) 重新采样：将裁剪后的肺结节区域重新采样为 1 mm，并将生成的图像块大小调整为 64×64×64 [28]，[29].\n(c) 归一化和混合特征：使用Z-score对原始图像块进行标准化，将CT图像转换为标准正态分布，提高网络训练的稳定性。混合特征通过变换和加权求和，同时保留了切归一化后的对比度信息（Cut-norm，eq（1））和 Sobel算子的边界信息，如图3所示。肺HU值的切归一化范围为**[-1000, 400]** [3]，然后将截断的 CT 图像映射到 [0,1]，从而提高 CT 图像中低对比度组织和细节的可视性，同时增强网络的灵敏度。 Sobel算子是一种简单且稳定的算子，可以有效突出细微、血管旁和胸膜旁结节的边界信息。另外，Prewitt算子的成像结果与Sobel算子相似，因此没有进行进一步的研究。\nB. Overall Design of the Dual-branch network 双分支网络 (DBNet) 的架构如图 4 所示。它代表了一种端到端肺结节分割方法，包括信息编码和解码两个阶段。在编码阶段，我们采用主网络和辅助网络概念。主网络（$En_raw$）以原始CT图像作为输入，而辅助网络（$En_aid$）则利用混合特征图像作为输入。主、辅分支的输入图像块大小为 $Image^{C×H×W×D}{raw}$ , $Image^{C×H×W×D}{help}$ ∈ $R^{1×64×64×64}$，其中 $C$、$H$、$W$、$D$表示输入通道、长度、宽度和深度。编码阶段总共包括5个编码块和4次下采样操作，不同编码阶段的特征图可以表示为 $f^i_r$ , $f^i_a$ , $i$ ∈ [1, 5]，$r$ 和 $a$ 表示主分支和辅助分支，特征图从 $32 × 64 × 64 × 64$ 过渡到 $64 × 32 × 32 × 32$，然后过渡到 $128 × 16 × 16 × 16$、$256 × 8 × 8 × 8$，最后过渡到 $512 × 4 × 4 × 4$\n在解码阶段，采用双分支设计，减少网络参数和计算量，同时提供更多的特征选择和更强的泛化能力。针对双分支特征信息的融合设计，提出了多维融合模块，从多个维度的空间通道中提取特征信息。然后，注意力模块的输出特征图将被上采样并与相应的编码阶段特征图 $ f^i_r $ 和 $f^i_a$ 融合，其中 $i$ ∈ [1,4]，以补偿下采样过程造成的信息损失。该融合过程重复四次，以逐渐预测肺结节区域。经过 $3×3×3$ 卷积的特征提取和优化后，得到预测的肺结节区域为 $Output^{1×64×64×64}$ ∈ [0,1]。设置阈值0.5，如果该值大于阈值，则该像素被认为是肺结节的一部分；否则，它被视为背景的一部分。\n网络的基本单元包括卷积（Conv）、BatchNorm（BN）和Swish激活函数[30]，统称为CBS。 BN 加速网络收敛并增强其表达能力，而 Swish 激活函数可以减轻与更深网络相关的梯度消失问题。可训练参数 $beta$ = 1.0 动态调整 Swish 激活函数的形状，以更好地适应肺结节分割任务，如（2）所示，其中 $σ$ 表示 sigmoid 激活函数。将基本块的数量增加到2，并添加残差连接，得到残差基本块RCBS。\nC. Design of the Multi-Dimensional Fusion Module 多维融合模块（MDFM）由两部分组成：通道挤压洗牌注意力模块（CESA）和轴向多尺度空间注意力模块（AMSA），模块设计和内部注意力变化[31]如图5所示，整个过程可以表示为方程3至方程5。\n1) Channel Extrusion Shuffle Attention Module 通道挤压洗牌注意力模块 (CESA) 的输入来自双分支编码器的级联输出特征图。在优化通道特征之前，首先使用自适应均值池化和自适应最大池化将特征图聚合为 $C×1×1×1$，并使用CBS基本块来融合两类特征信息。多尺度设计可以扩大网络的感受野，获得更丰富的局部细节和全局信息。受这一思想的启发，在通道维度上对通道特征进行多尺度采样，在不改变融合特征图大小的情况下优化特征信息。通道多尺度涉及通道维度上的降维和扩展操作，深度依次为 $C/2$、$C/4$、$C/8$、$C/16$。此外，通过密集连接设计，网络的表达和泛化能力得到进一步增强，该过程称为“挤出”。通道混洗操作将通道分为8组，并通过混洗通道内的特征信息引入一定程度的随机性和多样性。然后将打乱后的特征添加到挤压操作后的特征图上，最后通过点积操作恢复特征图。利用Sigmoid（σ）方法，将特征图转换为[0,1]之间的概率分布，并将注意力权重映射到原始特征图。\n2) Axial Multiscale Spatial Attention Module 准确捕获肺结节的空间位置、纹理和形状信息对于分割任务至关重要。因此，提出了轴向多尺度空间注意力模块。首先，我们使用 $7 × 7 × 7 $ 窗口大小的 CBS 块来感知全局上下文信息，同时以 8、16、32 和 64 的压缩比 (r) 压缩特征图通道。压缩后的特征信息不仅更加关注空间维度特征，而且减少了网络参数和计算量。然后，我们利用窗口大小 $H×3×3$ 、$3×W×3$ 和 $3×3×D$ 的基本块来感知 Coronal、Sagittal 和 Axis 方向的特征图，其中 H、W 和 D 分别是特征图尺寸，从而捕获不同平面**（冠状矢状、矢状轴、冠状矢状）**的局部信息以及不同轴向的全局信息。这种设计有助于强调和突出肺结节的关键特征，使模型能够更好地理解和表达多维数据中的特征。对多维特征信息进行汇总，并利用残差连接补充梯度信息。最后利用 $7×7×7$ 的卷积核来恢复特征图，完成空间特征信息的提取。\nD. Loss Function Dice相似系数[32]（DSC）是一种相似性测量函数，通常用于计算两个样本的相似性。 DSC ∈ [0,1]，值越小表明**模型预测结果 **与 真实标签差距越大。\n其中 $P$ 表示二进制预测结果像素的集合，$G$ 表示二进制真实标签结果的集合。 $| P ∩ G |$ 表示 $P $ 和 $G$ 的交集。\nExperiments A. Datasets 本文在其实验设置中介绍了四个数据集：\n（1）LIDC： 肺部图像数据库联盟和图像数据库资源倡议 (LIDC) 数据集是全球最大的公开肺癌数据集。它包括来自多家医院的 1, 018 个研究病例，包含 11 种不同厚度类型的数据，最多有四名医生在 CT 图像中注释病变信息 [8]，如图 6 所示。首先，直径为 3mm 或 3mm 的结节由至少两名医生注释的较大的被保留[21]。然后，对由一名医生注释的结节进行二次筛查，以去除不确定或注释错误的样本，总共得到 2, 615 个肺结节样本。\n2）Luna16： Luna16 数据集是 LIDC 数据集的子集，旨在解决 LIDC 数据集中 CT 切片厚度变化和数据质量等问题。 Luna16数据集总共包括1186个肺结节样本，这些样本由三名或更多医生注释，直径为3毫米或更大，切片厚度为2.5毫米或更小[33]。\n(3) 多厚度切片图像数据集(mThickSImg)： 剔除Luna16肺结节样本后，共获得1429个多层结节，主要用于内模型性能测试，实验数据的样本分布如图7所示。\n(4) UniToChest： UniToChest [34] 数据集包括 306,440 个匿名胸部 CT 扫描切片和相应的肺结节分割掩模。这些数据来自623名不同的患者，经过处理和筛选后，总共获得了211张结节大小从3到35mm的多层CT图像用于外部测试集。\nB. Lung nodule characteristic attributes classification LIDC 数据集总共包含由医生注释的 9 个视觉特征。在该实验中，去除了内部结构和钙化特性，并重新定义了良性和恶性的分类[35]，同时还纳入了肺结节的大小[24]。为了保证多个医师对同一肺结节标注的一致性，肺结节属性分类采用两种策略：（1）良恶性肺结节的定义是多个医师对结节标注结果的中位数 [35]。 (2)其他特征属性通过投票过程获得。肺结节属性分类如表1所示。\nC. Evaluation Metrics 在本文中，我们使用精度（PRE）、灵敏度（SEN）、骰子相似系数（DSC）和并集平均交集（mIoU）作为评估指标，如方程（7）至（10）所示。 True Positive（TP）表示正确分割的焦点区域，True Negative（TN）表示正确分割的正常组织区域，False Positive（FP）表示正常组织区域被错误地分割为焦点区域，False Negative（FN）表示焦点区域被错误地分割为正常组织区域。\nD. Training Details 本实验使用Ubuntu 18.04.3 LTS操作系统作为实验基础平台。 CPU型号为Intel(R)Xeon(R)-Gold 6140，内存大小为187.4G，软件环境为Python 3.8、Conda 10.1、Pytorch 1.8.1在Tesla V100-SXM2-32GB显卡上实验。\n实验遵循所有 CAD 方法的一致设置。在训练阶段，数据集被分为5个子集，每个子集循环用作验证集，其余4个子集作为训练数据。应用数据增强技术，包括水平和垂直翻转、旋转和平移，来增强训练样本并增强模型的鲁棒性。最终的性能评估基于5倍交叉验证获得的平均结果。 Adam 优化器 [37] 的初始学习率为 3e-4，批量大小为 12，最多 500 次训练迭代。使用的损失函数是 Dice 损失。另外，我们使用提前停止机制来防止过拟合。在 50 轮训练中，如果没有达到较小的损失，模型训练将结束。\nResult A. CAD model performance comparison 如表2所示，所提出的方法与一些最近优秀的CAD模型进行了比较。根据样本数量和选择策略，这些方法分为小样本肺结节分割方法和一般样本数量肺结节分割方法。这些方法的筛选标准列于表中。在使用小样本量的肺结节分割方法中，我们提出的模型在包含 1,186 个结节的 Luna16 数据集上实现了最先进的性能，通过 5 folder 交叉验证实现了最高 DSC 87.68%（最佳）和 86.89%（平均）。就整体 DSC 而言，这比当前顶级小样本方法 DS-CMSF 领先 0.93%。这证明了我们的方法在相对较小的训练样本量下的有效性。此外，为了研究大样本量的性能，我们的方法在 LIDC 数据集中的 2,618 个节点集上进行了训练。我们取得了最佳 DSC 分数和平均 DSC 分数分别为 85.42% 和 84.12%，显着优于当前领先的 LNHG 模型（82.05%）和其他主流算法。\n总体而言，不同数据量下的测试结果表明本文方法具有较强的稳定性和泛化能力。\nB. Ablation studies 在本节中，我们对所提出的 CAD 模型的不同部分进行了消融研究，以评估它们对性能的贡献。消融研究中使用的模型是基于 Luna16 数据集进行训练的。\n在影响模型性能的因素中，研究人员普遍认为模型规模是关键因素。目前，大多数学者倾向于使用 $64×64×64$ 的 patch大小作为神经网络的输入。按照此设置，模型的初始通道数设置为32，考虑到输入大小的约束，模型的最大下采样次数设置为4。此时，瓶颈层的输入大小为 $4 × 4 × 4$ ，512 个通道。基于此，我们设计了四种双分支分割模型（称为模型A、B、C和D），深度分别为2、3、4和5层。这些模型均使用CBAM模块作为双分支模型的融合模块。实验结果表明，随着网络深度和通道数的增加，模型性能呈现出不断增加的趋势，DSC从80.61%上升到85.8%，如表3所示。因此，得出结论：在输入尺寸为64×64×64的情况下，本研究模型的最佳深度为5，瓶颈层有512个通道。\n实验中主要探索了两类辅助特征。一种是利用Cut-norm来突出病变的特征信息，另一种方法主要利用Sobel梯度算子来增强边界信息。在最佳模型规模的讨论中，所有模型均使用Z-score标准方法进行训练。训练这两个手动特征并与模型 D 进行比较后，发现使用 Cut-norm 的模型 E 和使用 Sobel 算子的模型 F 在 DSC 中分别与模型 D 相差 0.61% 和 0.78%。这说明了该辅助分支策略的有效性。此外，进一步验证了Sobel算子在不同数据尺度下的有效性，发现模型D和模型F在LIDC数据集上的DSC性能分别达到83.56%和83.82%，相差0.26%。这表明，即使在更大规模的测试集中，Sobel算子仍然有效，因此，暂时将Sobel算子用作辅助特征。\n进一步基于Sobel算子作为辅助特征，提出了通道挤压洗牌注意力模块和轴向多尺度空间注意力模块，即模型G和模型H，模型性能最优达到86.63%。最后，为了同时保留高对比度病变区域和突出显示的边界细节以方便学习，我们使用变换操作和加权求和来整合这两个特征。通过这种策略，本文提出的方法获得了 86.89% DSC 的最佳结果，超过了单独使用 Sobel 滤波器特征的性能。直接的性能提升验证了我们辅助功能的有效性。\n虽然模型 H 和我们的整体性能差距并不显着，但模型 H 在内部数据集中的 DSC 性能为 63.03%，对于 GGO 类型评估的灵敏度为 62.42%。然而，使用混合特征的进一步替代导致灵敏度和 DSC 性能显着提高，分别达到 65.27% 和 65.08%。观察模型G、H和Ours的收敛速度，随着注意力模块数量的增加和混合特征的替换，模型拟合所需的epoch逐渐增加，但综合考虑，总体费用是值得的。\nC. Comparison experiments of different datasets and segmentation CADs 本节将我们的方法与开源医学分割模型 UNet [38]、UNet++ [28]、ReconNet [39]、Unetr [40] 和 Asa [41] 在不同数据集上的测试结果进行比较。结果如表IV所示。总体而言，该方法在 Luna16 数据集上实现了 86.89% 的 DSC，分别领先 UNet 和 ReconNet 模型 1.38% 和 0.77%。在基于Transformer的方法中，在DSC方面，它以4.01%和2.66%的优势超过了Unetr和ASA。整体性能超过了经典和现有的优秀分割模型。具体而言，该方法的精度为87.13%，灵敏度为87.02%。更高的灵敏度可以更好地识别结节，防止漏诊；而更高的精度为医生提供了更可靠的分割区域，减少误诊。这对于肺结节分割任务至关重要。此外，该方法实现了高 mIoU，表明预测与真实情况之间有更大的重叠。可以很好地分割大部分肺部病变区域，辅助合理诊断，有利于临床使用。\n随后，所有方法都在内部 mThickImgs 数据集和外部 UniToChest 数据集上进行了验证。在这两个数据集上，UNet++ 达到了最高的精度，但灵敏度较低。这表明 UNet++ 在真阳性区域中假阴性的比例较大，丢失了更多样本并最终影响了灵敏度指标。这种不平衡对于临床目的是有害的。此外，ReconNet在mThickImgs数据集中具有较高的灵敏度。我们的方法在两个数据集中实现了 75.32% 和 80.64% DSC，分别超过次优方法 0.57% 和 0.59%，实验结果再次证明了我们方法强大的泛化能力。\n总体而言，我们的方法在不同数据集的指标上比其他最先进的3D CAD模型具有明显的优势，特别是在灵敏度和DSC这两个最关键的分割性能指标方面，验证了我们方法的分割能力在多源医学图像中。\nAnalysis of lung nodule performance A. Effect of different layer thicknesses on model performance 我们分析了 mThickSImg 数据集中肺结节的分布，并检查了不同 CT 切片厚度和尺寸的结节性能变化。我们将数据分为六组，数据样本分布如表六所示。然后对不同的CAD模型进行测试，并将不同模型的平均测试结果作为最终的性能趋势，如表5所示。\n我们的方法是基于2.5mm层厚度以下的样本进行训练的，通过观察精度，我们发现在1.5mm和1.5mm ~ 2.5mm的比较中，网络的灵敏度随着层厚度的减小而增加。\n另外，在观察DSC性能时，注意到DSC性能随着层厚度的增加而增加。然而，似乎存在随着层厚度减小灵敏度增加而DSC降低的现象。为什么会出现这种现象呢？一方面，这种趋势可能会受到数据分布、样本数量和样本标签的影响而有些偏差。一般来说，随着 CT 层厚度的增加，图像分辨率会降低，可能导致神经网络难以捕获更精细的特征和结构。\n因此，由于较厚的 CT 层中详细信息的丢失，网络随着层厚度的增加而表现出较低的灵敏度。另一方面，虽然层厚度的减少提高了 CT 图像的层内分辨率，并能够更准确地呈现解剖结构和病变，但同时也减少了接收到的光子数量。与较厚的图像相比，光子计数的减少可能会导致噪声增加和对比度降低。这些因素会对肺结节（尤其是 GGO）的分割产生负面影响，可能导致验证集的性能下降。\n我们还发现，在基于薄层图像进行训练然后在厚层图像上进行验证后，网络表现出更好的性能。我们相信，更薄的 CT 图像由于具有更高的分辨率和更详细的信息，使网络能够学习更微妙的特征。这些细微的特征在2.5mm以上的层厚图像中具有更强的泛化能力，这是基于厚层数据所不具备的。因此，这种现象可能表明关于层厚度效应的更好的性能。\nB. Heterogeneity analysis of lung nodules 本节演示不同分割模型在Luna16数据集和mThickSImg数据集上的测试性能。总的来说，我们的方法在两个数据集上的大多数肺结节分割属性上都取得了领先的结果\n1) Subtle feature 肺结节的早期诊断和治疗对于疾病的诊断具有重要意义。\n可以看出，在Luna16数据集中，我们的方法和UNet在 “Extremely Subtle” 属性和“Moderately Subtle”属性的分割上表现更好，并且我们的方法仅次于UNet的方法，达到了70.99%和76.69%。在mThickImgs测试集中，我们的方法在所有Subtlety特征属性上都达到了最佳性能，“Extremely Subtle”和“Moderately Subtle”属性分别超过UNet和ReconNet 2.75％、1.03％和2.33％、0.94％ 。 UNet网络在两个数据集中表现出不同的性能变化，这并不排除数据分布和数据质量的影响。由于双分支和混合特征的设计，我们的方法在 mThickImgs 测试集上对“Extremely Subtle”属性和“Moderately Subtle”属性分割有较大的性能提升，这证明了我们的方法具有很强的泛化能力和鲁棒性，并提高了临床使用的可靠性。 Extremely Subtle GGO 的分割结果如图 10-(1) 所示。\n2) Texture feature 观察两个数据集中“Texture”特征的测试结果可以发现，在Luna16数据集中，我们的方法在“Solid/Mixed”和“NonSolid/Mixed”类别上表现出了很大的进步，分别为2.73%与次优方法相比，分别提高了 1.62% 和 1.62%。在 mThickSImg 数据集中，我们的方法在 GGO 类和“NonSolid/Mixed”类中达到了 65.08% 和 70.2%，分别领先次优 0.35% 和 1.33%。\n虽然对于 GGO 样肺结节，我们的方法没有显着改进，但对于具有混合纹理的结节，我们的方法进一步减轻了分割挑战。固体/混合分割结果如图10-（3至7）所示。\n3) Maligancy feature 良性和恶性的区分是肺结节分割任务的关键特征。良性结节常呈圆形或椭圆形，而恶性结节往往体积较大，并有明显的针状或分叶状特征。两个数据集的测试结果表明，不确定属性的分割结果最差，其次是良性结节。我们的方法改进了这三类属性的分割，与次优方法、不确定和恶性分割相比，在 Luna16 数据集中增加了 0.73%（良性）、0.28%（不确定）和 0.87%（恶性）如图10（1、2～7）所示。在mThickSImg测试集中，该方法的性能提升更为显着，分别领先1%（良性）、0.83%（不确定）和0.61%（恶性）。提高肺结节良性和不确定性的分割性能对于临床诊断至关重要。采用相同的CAD方法对这两类结节进行分割可以有效观察其发展趋势。与医生手工勾画相比，该方法提高了诊断的准确性和可靠性。\n4) Other features 除了上述三种常见且具有挑战性的分割特征外，肺结节的“球形”、“边缘”、“分叶”和“毛刺”特征对临床诊断具有一定的指导意义。观察“球形”特征，可以发现当前方法在常见的“卵形或圆形”属性上表现出更好的性能。一方面它有更多的训练样本，另一方面也更简单地分割该类的样本。我们的方法在 mThickSImg 数据集中的“线性”和“卵形/线性”病变形状属性中表现出显着优势，导致“线性”和“卵形/线性”属性中的 DSC 次优，分别为 2.74% 和 1.81% 。线性分割结果如图10-(6)所示。还值得注意的是，我们的方法在 Luna16 数据集的 Margin 特征中的五个属性上显示出巨大的改进，特别是“Poorly Defined”属性。\n在 mThickImgs 测试集中，我们的方法在五个属性上分别与次优值相差 0.65%、0.93%、0.7%、0.94% 和 1.02%，这也表明辅助特征可以减轻图像边界的复杂性。病变区域。分叶和毛刺是判断恶性程度的重要指标。这些特征可以帮助医生区分结节边缘的规律性，并有助于对结节进行初步分类和观察。\n可以看出，与次优相比，我们的方法在 mThickImgs 数据集的性能方面表现出了显着的改进。然而，“标记分叶”和“标记毛刺”特征属性的分割仍然具有挑战性。在不同尺寸的结节中，我们的方法在分割3到6mm范围内的实性结节方面也表现出了一定的性能改进。\n与次优方法相比，我们的方法在两个测试集中分别提高了 0.73% 和 1.39%。然而，在“SubSolid”中，大多数方法没有表现出显着差异。\nCONCLUSION 我们提出了一种简单有效的双分支网络分割方法。五折交叉验证的结果表明，该方法能够有效、稳定地执行分割任务。此外，我们的方法对大多数肺结节的特征特征和复杂数据样本的分割具有一定的适用性，特别是对于细微的、纹理混合的、边界模糊的、良性的和不确定的结节，可以大大降低临床应用的难度，并提供为医生提供更可靠的参考。\n这种方法也有一些局限性。首先，虽然混合特征设计保留了边界和对比度信息，但它间接引入了额外的噪声，这可能会干扰网络的特征学习。其次，我们的方法在“Subsolid”小结节的分割方面没有表现出显着的性能改进，未来可能会尝试更深或更详细的多尺度设计来缓解这个问题。另外，由于实验整体采用多位医师标注的平均值作为最终的GT，一定程度上保证了结果的准确性。但也可能导致最终的GT与实际情况存在差异，导致区域不完整或有刺状等详细信息丢失，如图10-(7)所示，对“Marked Lobulation”的分割提出挑战”和“标记毛刺”属性。未来，可能会开发多置信区域方法来缓解这个问题。\n读完的第一感觉： 太长了\u0026hellip;.\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-dbnet/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e肺癌是全球最致命的癌症之一，早期诊断对于患者的生存至关重要。\u003cstrong\u003e肺结节\u003c/strong\u003e是\u003cstrong\u003e早期肺癌的主要表现\u003c/strong\u003e，通常通过 \u003cstrong\u003eCT 扫描\u003c/strong\u003e进行评估。如今，计算机辅助诊断系统被广泛用于辅助医生进行疾病诊断。\u003cstrong\u003e肺结节的准确分割\u003c/strong\u003e受到\u003cstrong\u003e内部异质性和外部数据\u003c/strong\u003e因素的影响。为了克服\u003cstrong\u003e结节的细微、混合、粘附型、良性和不确定类别\u003c/strong\u003e的分割挑战，提出了一种新的\u003cstrong\u003e混合手动特征网络\u003c/strong\u003e，可\u003cstrong\u003e增强灵敏度和准确性\u003c/strong\u003e。该方法通过\u003cstrong\u003e双分支网络框架和多维融合模块\u003c/strong\u003e集成\u003cstrong\u003e特征信息\u003c/strong\u003e。通过使用\u003cstrong\u003e多个数据源和不同数据质量\u003c/strong\u003e进行训练和验证，我们的方法在 \u003cstrong\u003eLUNA16\u003c/strong\u003e、\u003cstrong\u003e多厚度切片图像数据集 (Multi-thickness Slice Image dataset)\u003c/strong\u003e、\u003cstrong\u003eLIDC\u003c/strong\u003e 和 \u003cstrong\u003eUniToChest\u003c/strong\u003e 上表现出领先的性能，Dice 相似系数达到 86.89%、75.72%、84.12% 和 80.74分别超过了当前大多数肺结节分割方法。我们的方法进一步提高了肺结节分割任务的\u003cstrong\u003e准确性、可靠性和稳定性\u003c/strong\u003e，即使是在具有挑战性的 CT 扫描中也是如此。本研究中使用的代码发布在 GitHub 上，可通过以下 URL (\u003ca href=\"https://github.com/BITEWKRER/DBNet\"\u003ehttps://github.com/BITEWKRER/DBNet\u003c/a\u003e) 获取。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e肺癌是全球癌症相关死亡的主要原因[1]。仅在美国，预计 2023 年将有 127,070 人死于肺癌，占所有癌症死亡的 21% [2]。不幸的是，超过 50% 的肺癌病例发生在发展中国家或不发达国家，与发达国家相比，这些国家的医疗资源有限[3]。\u003c/p\u003e\n\u003cp\u003e为了增加生存机会，早期诊断和治疗肺癌仍然至关重要。在中国，研究表明，\u003cstrong\u003e小于1厘米的I期肺癌的5年生存率为92%\u003c/strong\u003e。然而，\u003cstrong\u003e晚期肺癌的5年生存率低得多\u003c/strong\u003e，仅为\u003cstrong\u003e7.0%\u003c/strong\u003e[4]。\u003cstrong\u003e利用计算机断层扫描 (CT) 进行肺癌筛查已显示出可大幅降低死亡率的潜力\u003c/strong\u003e [5]、[6]。低剂量CT是目前肺癌筛查最常用的方法。此外，移动CT的引入有助于解决欠发达国家和偏远地区缺乏CT扫描仪的问题[6]。由于可能没有明显的症状，检测早期肺癌的存在可能会带来重大挑战。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e这种医学背景数据可以直接借鉴，Chatgpt润色改写就完事儿\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在 \u003cstrong\u003eCT 图像上识别肺结节提供了疾病的关键指标\u003c/strong\u003e [1], [3]。这些结节代表\u003cstrong\u003e圆形异常\u003c/strong\u003e，其\u003cstrong\u003e大小各异\u003c/strong\u003e，直径范围为 \u003cstrong\u003e3 至 30 毫米\u003c/strong\u003e [7]。为了进一步研究肺结节，美国国家癌症研究所组装了“肺部图像数据库联盟和图像数据库资源计划（LIDC）”数据集[8]。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e欠发达地区设备不足、人员不足，导致医生的诊断和治疗时间有限\u003c/strong\u003e[9]。在这种情况下，\u003cstrong\u003e医生的工作量很大、重复且耗时\u003c/strong\u003e[10]、[5]。此外，由于与CT切片相比，\u003cstrong\u003e肺部结节性病变\u003c/strong\u003e占据相对\u003cstrong\u003e较小的面积\u003c/strong\u003e，\u003cstrong\u003e长时间和密集的CT筛查\u003c/strong\u003e可能会导致\u003cstrong\u003e漏检小的、细微的或 GGO\u003c/strong\u003e (肺磨玻璃结节) [3]，[6]。为了解决这些问题，计算机辅助诊断系统（CAD）出现并得到了快速发展，特别是随着基于深度学习技术的诊断方法的进步。 \u003cstrong\u003eCAD系统大大减轻了医生的工作量，最大限度地降低了未发现结节的风险，并提高了肺结节诊断的效率和可靠性\u003c/strong\u003e。然而，当前用于肺结节分割的 CAD 系统仍然面临一些挑战。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e下面详细阐述了肺结节分割的几个现有挑战，可以从这些挑战入手\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e首先，\u003cstrong\u003e放射科医生标记的肺结节\u003c/strong\u003e包含\u003cstrong\u003e九个诊断特征\u003c/strong\u003e[11]，\u003cstrong\u003e异质性表型\u003c/strong\u003e阻碍了\u003cstrong\u003e肺结节分割的发展\u003c/strong\u003e。如图1所示，\u003cstrong\u003e实心结节（a，b）具有清晰的形状和边界\u003c/strong\u003e，而\u003cstrong\u003e微妙的GGO结节（e）具有低对比度和模糊的边界\u003c/strong\u003e[4]，使得网络很容易将\u003cstrong\u003e它们分类为背景区域\u003c/strong\u003e。\u003cstrong\u003e空洞（g）结节降低了网络分割的敏感性\u003c/strong\u003e，并且由于\u003cstrong\u003e背景和分割目标之间的极度不平衡，小结节很容易被遗漏\u003c/strong\u003e[12]。\u003c/p\u003e\n\u003cp\u003e由于周围多余的组织结构，\u003cstrong\u003e血管旁或胸膜旁（c、d、f）可能会导致网络分类错误\u003c/strong\u003e[13]。此外，\u003cstrong\u003e部分实性结节（h）比纯GGO更致密\u003c/strong\u003e，产生更\u003cstrong\u003e复杂的异质纹理，更容易发展成恶性结节\u003c/strong\u003e[14]。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/262f8e71-d931-11ee-b68c-c858c0c1debd\" alt=\"image-20240303102609243\"  /\u003e\r\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e其次，肺结节内部因素造成的分割困难在于\u003cstrong\u003e医生注释、层厚、数据来源和数据质量\u003c/strong\u003e。\u003cstrong\u003e数据质量差\u003c/strong\u003e或\u003cstrong\u003e不同医生的经验\u003c/strong\u003e可能会导致\u003cstrong\u003e不同的注释和注释者数量\u003c/strong\u003e。由\u003cstrong\u003e多名医生注释的病变区域\u003c/strong\u003e通常更\u003cstrong\u003e可靠\u003c/strong\u003e，减少了潜在的临床风险。\u003cstrong\u003e在资源有限的地区\u003c/strong\u003e，由于 \u003cstrong\u003eCT 扫描仪短缺和成像设备陈旧\u003c/strong\u003e，\u003cstrong\u003eCT 扫描质量差的情况很常见\u003c/strong\u003e。\u003cstrong\u003e较厚的切片\u003c/strong\u003e更有可能产生“\u003cstrong\u003e体积平均效应”和伪影\u003c/strong\u003e，使医生\u003cstrong\u003e难以达成一致的诊断\u003c/strong\u003e。即使使用\u003cstrong\u003e移动 CT 扫描仪\u003c/strong\u003e也可能\u003cstrong\u003e无法提供完整的诊断详细信息\u003c/strong\u003e。最后，目前\u003cstrong\u003e大多数肺结节分割方法\u003c/strong\u003e都是基于\u003cstrong\u003e2D图像\u003c/strong\u003e，但这些方法忽略了\u003cstrong\u003e空间关系\u003c/strong\u003e，因此提出一种有效的\u003cstrong\u003e3D肺结节分割模型\u003c/strong\u003e来\u003cstrong\u003e捕获肺结节的空间位置\u003c/strong\u003e、\u003cstrong\u003e纹理和其他详细信息变得越来越重要以避免误诊和漏诊\u003c/strong\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e异质性: 肺结节的形状多异 （实心结节、磨玻璃结节 (GGO) 、空洞结节、血管和胸膜旁边的结节）\u003c/p\u003e","title":"A Dual-Branch Framework with Prior Knowledge for Precise Segmentation of Lung Nodules in Challenging CT Scans"},{"content":"Abstract 如今的深度学习方法主要关注如何设计最合适的目标函数，使模型的预测结果能够最接近真实情况。同时，必须设计一个适当的架构，可以帮助获取足够的信息进行预测。现有方法忽略了一个事实，即当输入数据经过逐层特征提取和空间变换时，大量信息将会丢失。本文将深入研究数据通过深度网络传输时数据丢失的重要问题，即信息瓶颈和可逆函数。我们提出了可编程梯度信息（PGI）的概念来应对深度网络实现多个目标所需的各种变化。 PGI可以为目标任务计算目标函数提供完整的输入信息，从而获得可靠的梯度信息来更新网络权值。此外，还设计了一种基于梯度路径规划的新型轻量级网络架构——通用高效层聚合网络（GELAN）。GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用传统的卷积算子即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得train-from-scratch (从零开始训练) 模型能够比使用大数据集预训练的state-of-theart模型获得更好的结果，对比结果如图1所示。源代码位于：https： //github.com/WongKinYiu/yolov9。\n核心创新点: 依然是网络结构的创新\nProgrammable Gradient Information (PGI) Generalized Efficient Layer Aggregation Network（GELAN） Introduction 基于深度学习的模型在计算机视觉、语言处理和语音识别等各个领域都表现出了比过去的人工智能系统更好的性能。近年来，深度学习领域的研究人员主要关注如何开发更强大的系统架构和学习方法，例如CNN，Transformers[8,9,40] 、41、60、69、70]，Perceivers[26、26、32、52、56、81、81]和Mambas[17、38、80]。此外，一些研究人员尝试开发更通用的目标函数，例如损失函数[5,45,46,50,77,78]，标签分配[10,12,33,67,79]和辅助监督[18] 、20、24、28、29、51、54、68、76]。上述研究都试图精确地找到输入和目标任务之间的映射。然而，大多数过去的方法都忽略了输入数据在前馈过程中可能会产生不可忽略的信息丢失量。这种信息丢失可能会导致有偏差的梯度流，随后用于更新模型。上述问题可能导致深度网络在目标和输入之间建立不正确的关联，导致训练后的模型产生不正确的预测。\n在深度网络中，输入数据在前馈过程中丢失信息的现象俗称信息瓶颈[59]，其示意图如图2所示。目前可以缓解这种现象的主要方法有：（1）可逆架构的使用[3,16,19]：该方法主要使用重复的输入数据，并以显式的方式维护输入数据的信息； （2）使用Masked建模[1,6,9,27,71,73]：主要利用重构损失，采用隐式方式最大化提取特征并保留输入信息； （3）引入深度监督概念[28,51,54,68]：它利用没有丢失太多重要信息的浅层特征来预先建立从特征到目标的映射，以确保重要信息能够被传递到更深的层次。然而，上述方法在训练过程和推理过程中都存在不同的缺点。例如，可逆架构需要额外的层来组合重复馈送的输入数据，这将显着增加推理成本。另外，由于输入数据层到输出层不能有太深的路径，这种限制将导致在训练过程中难以对高阶语义信息进行建模。对于 Masked 建模，其重建损失有时与目标损失相冲突。此外，大多数掩码机制还会产生与数据的不正确关联。 对于深层监督机制来说，会产生误差累积，如果浅层监督在训练过程中丢失信息，后续层将无法检索到所需信息。上述现象在困难任务和小模型上会更加显着。\n针对上述问题，我们提出了一个新的概念，即可编程梯度信息（PGI）。其概念是通过辅助可逆分支生成可靠的梯度，使得深层特征仍然能够保持执行目标任务的关键特征。\n辅助可逆分支的设计可以避免传统的融合多路径特征的深度监督过程可能造成的语义损失。换句话说，我们在不同语义层面上编程梯度信息传播，从而达到最佳的训练结果。 PGI的可逆架构建立在辅助分支上，因此没有额外的成本。由于PGI可以自由选择适合目标任务的损失函数，因此也克服了Masked建模遇到的问题。所提出的PGI机制可以应用于各种规模的深度神经网络，并且比仅适用于非常深的神经网络的深度监督机制更通用。\n在本文中，我们还基于ELAN[65]设计了广义ELAN（GELAN），GELAN的设计同时考虑了参数量、计算复杂度、准确性和推理速度。这种设计允许用户针对不同的推理设备任意选择合适的计算块。我们将提出的PGI和GELAN结合起来，然后设计了新一代YOLO系列物体检测系统，我们称之为YOLOv9。我们使用MS COCO数据集进行实验，实验结果验证了我们提出的YOLOv9在所有比较中都取得了顶尖的性能。\n我们总结本文的贡献如下：\n我们从可逆函数的角度对现有的深度神经网络架构进行了理论分析，通过这个过程我们成功地解释了许多过去难以解释的现象。我们还基于此分析设计了PGI和辅助可逆分支，并取得了优异的结果。\n我们设计的PGI解决了深度监督只能用于极深的神经网络架构的问题，从而让新的轻量级架构真正应用于日常生活中。\n我们设计的GELAN仅使用常规卷积来实现比基于最先进技术的深度卷积设计更高的参数利用率，同时表现出轻、快速、准确的巨大优势。\n结合所提出的PGI和GELAN，YOLOv9在MS COCO数据集上的目标检测性能在各个方面都大大超过了现有的实时目标检测器。\nProgrammable Gradient Information (PGI)：\n自由选择适合目标任务的损失函数\n可逆结构建立辅助分支，不增加推理成本\n适用于各种规模的深度神经网络\nGELAN：\n轻、快速、准确 采用常规卷积吊打其他新颖卷积 Related work 2.1 Real-time Object Detectors 目前主流的实时目标检测器是YOLO系列[2,7,13–15,25,30,31,47–49,61–63,74,75]，这些模型大多数使用CSPNet[64]或 ELAN [65] 及其变体作为主要计算单元。在特征集成方面，通常使用改进的PAN[37]或FPN[35]作为工具，然后使用改进的YOLOv3头[49]或FCOS头[57, 58]作为预测头。最近也提出了一些实时目标检测器，例如 RT DETR [43]，其基础是 DETR [4]。然而，由于DETR系列目标检测器在没有相应领域预训练模型的情况下很难应用于新领域，因此目前应用最广泛的实时目标检测器仍然是YOLO系列。本文选择 YOLOv7 [63] 作为开发该方法的基础，该方法已在各种计算机视觉任务和各种场景中被证明有效。\n我们使用 GELAN 来改进所提出的 PGI 的架构和训练过程。上述新颖方法使所提出的 YOLOv9 成为新一代顶级实时目标检测器。\n2.2 Reversible Architectures 可逆架构[3,16,19]的运算单元必须保持可逆转换的特性，因此可以保证每层运算单元的输出特征图都能保留完整的原始信息。之前，RevCol[3]将传统的可逆单元推广到多个层次，这样做可以扩展不同层单元表达的语义层次。通过对各种神经网络架构的文献回顾，我们发现有许多高性能架构具有不同程度的可逆特性。例如，Res2Net模块[11]以分层方式将不同的输入分区与下一个分区组合起来，并在向后传递之前连接所有转换后的分区。 CBNet [34, 39]通过复合主干网重新引入原始输入数据以获得完整的原始信息，并通过各种组合方法获得不同级别的多级可逆信息。这些网络架构通常具有出色的参数利用率，但额外的复合层导致推理速度缓慢。 DynamicDet [36]结合了CBNet [34]和高效实时目标检测器YOLOv7 [63]，在速度、参数数量和精度之间实现了非常好的权衡。本文介绍了 DynamicDet 架构作为设计可逆分支的基础。此外，可逆信息被进一步引入到所提出的PGI中。所提出的新架构在推理过程中不需要额外的连接，因此可以充分保留速度、参数量和准确性的优势。\n2.3 Auxiliary Supervision 深度监督[28,54,68]是最常见的辅助监督方法，它通过在中间层插入额外的预测层来进行训练。尤其是基于Transformer的方法中引入的多层解码器的应用是最常见的一种。\n另一种常见的辅助监督方法是利用相关元信息来指导中间层产生的特征图，并使它们具有目标任务所需的属性[18,20,24,29,76]。这种类型的示例包括使用分割损失或深度损失来提高对象检测器的准确性。\n最近，文献[53,67,82]中有许多报告使用不同的标签分配方法来生成不同的辅助监督机制，以加快模型的收敛速度，同时提高鲁棒性。然而，辅助监督机制通常只适用于大型模型，因此当其应用于轻量级模型时，很容易造成欠参数化现象，从而使性能变差。我们提出的PGI设计了一种重新编程多级语义信息的方法，这种设计让轻量级模型也受益于辅助监督机制。\nProblem Statement 通常，人们将深度神经网络收敛问题的困难归因于梯度消失或梯度饱和等因素，而这些现象在传统深度神经网络中确实存在。然而，现代深度神经网络已经通过设计各种归一化和激活函数从根本上解决了上述问题。尽管如此，深度神经网络仍然存在收敛速度慢或收敛结果差的问题。\n在本文中，我们进一步探讨上述问题的本质。通过对信息瓶颈的深入分析，我们推断出这个问题的根本原因是原本来自很深网络的初始梯度在传输后很快就丢失了实现目标所需的大量信息。为了证实这一推论，我们将不同架构的深度网络前馈了初始权重，然后将其可视化并在图2中进行说明。显然，PlainNet丢失了深层物体检测所需的大量重要信息。至于ResNet、CSPNet、GELAN能够保留重要信息的比例，确实与训练后能够获得的准确率呈正相关。我们进一步设计了基于可逆网络的方法来解决上述问题的原因。本节我们将详细阐述对信息瓶颈原理和可逆函数的分析。\n3.1. Information Bottleneck Principle 根据信息瓶颈原理，我们知道数据X在进行变换时可能会造成信息丢失，如式(1)所示。\n其中 $I$ 表示相互信息，$f$ 和 $g$ 是变换函数，$θ$ 和 $ϕ$ 分别是 $f$ 和 $g$ 的参数。\n在深度神经网络中，$f_θ (·)$ 和 $g_ψ (·)$ 分别表示深度神经网络中两个连续层的操作。从方程（1）我们可以预测**，随着网络层数越深，原始数据丢失的可能性就越大**。然而深度神经网络的参数是基于网络的输出以及给定的目标，然后通过计算损失函数生成新的梯度后更新网络。\n可以想象**，更深的神经网络的输出不太能够保留有关预测目标的完整信息**。这将使得在网络训练期间使用不完整的信息成为可能，从而导致梯度不可靠和收敛性差。\n解决上述问题的一种方法是直接增加模型的尺寸。当我们使用大量的参数来构建模型时，它更有能力对数据进行更完整的转换。上述方法使得即使在数据前馈过程中信息丢失，仍然有机会保留足够的信息来执行到目标的映射。上述现象解释了为什么在大多数现代模型中宽度比深度更重要。然而，上述结论并不能从根本上解决非常深的神经网络中梯度不可靠的问题。\n下面，我们将介绍如何利用可逆函数来解决问题并进行相关分析。\n3.2. Reversible Functions 当函数 $r$ 有一个逆变换函数 $v$ 时，我们称该函数为可逆函数，如式(2)所示。\n其中 $ψ$ 和 $ζ$ 分别是 $r$ 和 $v$ 的参数。数据 $X$ 通过可逆函数转换而不会丢失信息，如式(3)所示。\n当网络的变换函数由可逆函数组成时，可以获得更可靠的梯度来更新模型。当今流行的深度学习方法几乎都是符合可逆性质的架构，例如式（4）。\n其中 $l$ 表示 PreAct ResNet 的第 $l$ 层，$f$ 是第 $l$ 层的变换函数。 PreAct ResNet [22] 以显式方式重复将原始数据 X 传递到后续层。这样的设计虽然可以让一千多层的深度神经网络收敛得很好，但却破坏了我们需要深度神经网络的一个重要原因。也就是说，对于困难的问题，我们很难直接找到简单的映射函数将数据映射到目标。这也解释了为什么当层数较少时，PreAct ResNet 的性能比 ResNet [21] 差。\n【Remind】PreAct ResNet 和 ResNet 结构图比较一下\n此外，我们尝试使用Masked建模，使 Transformer 模型取得重大突破。我们使用近似方法，例如方程 (5) 尝试求 $r$ 的逆变换 $v$，使得变换后的特征能够利用稀疏特征保留足够的信息。方程(5) 如下：\n其中 $M$ 是动态二进制掩码。其他常用于执行上述任务的方法是扩散模型和可变化自动编码器，它们都具有查找反函数的功能。然而，当我们将上述方法应用于轻量级模型时，就会存在缺陷，因为轻量级模型对大量原始数据的参数化不足。由于上述原因，将数据 $X$ 映射到目标 $Y$ 的重要信息 $I(Y，X)$ 也会面临同样的问题。对于这个问题，我们将使用信息瓶颈的概念来探讨它[59]。信息瓶颈的计算公式如下：\n一般来说，$I(Y,X)$ 只会占据 $I(X,X)$ 的很小一部分。然而，这对于目标任务至关重要。因此，即使前馈阶段丢失的信息量并不大，只要覆盖了 $I(Y,X)$，训练效果就会受到很大影响。轻量级模型本身处于欠参数化状态，因此在前馈阶段很容易丢失很多重要信息。因此，我们轻量级模型的目标是如何从 $I(X, X)$ 中准确过滤出 $I(Y, X)$。至于完全保留 $X$ 的信息，这是很难做到的。基于上述分析，我们希望提出一种新的深度神经网络训练方法，不仅能够生成可靠的梯度来更新模型，而且适用于浅层和轻量级神经网络。\nMethodology 4.1 Programmable Gradient Information 为了解决上述问题，我们提出了一种新的辅助监督框架，称为可编程梯度信息（PGI），如图3（d）所示。 PGI主要包括三个组成部分，即（1）主分支，（2）辅助可逆分支，（3）多级辅助信息。从图3(d)中我们可以看出，PGI的推理过程仅使用主分支，因此不需要任何额外的推理成本。\n至于其他两个组件，它们用于解决或减缓深度学习方法中的几个重要问题。其中，辅助可逆分支是为了处理神经网络加深带来的问题而设计的。网络加深会造成信息瓶颈，导致损失函数无法生成可靠的梯度。对于多级辅助信息，旨在处理深度监督带来的误差累积问题，特别是针对多个预测分支的架构和轻量级模型。接下来我们将逐步介绍这两个组件。\n4.1.1 Auxiliary Reversible Branch 在PGI中，我们提出了辅助可逆分支来生成可靠的梯度并更新网络参数。通过提供从数据映射到目标的信息，损失函数可以提供指导并避免从与目标不太相关的不完整前馈特征中发现错误相关性的可能性。我们提出通过引入可逆架构来维护完整信息，但是在可逆架构中添加主分支会消耗大量的推理成本。\n我们分析了图3(b)的架构，发现当添加从深层到浅层的额外连接时，推理时间将增加20%。当我们反复将输入数据添加到网络的**高分辨率计算层（黄色框）**时，推理时间甚至超过了两倍。\n由于我们的目标是使用可逆架构来获得可靠的梯度，因此**“可逆”并不是推理阶段的唯一必要条件**。鉴于此，我们将可逆分支视为深度监督分支的扩展，然后设计辅助可逆分支，如图3(d)所示。对于由于信息瓶颈而丢失重要信息的主分支深度特征，它们将能够从辅助可逆分支接收可靠的梯度信息。\n这些梯度信息将驱动参数学习来协助提取正确且重要的信息，上述动作可以使主分支获得对目标任务更有效的特征。此外，可逆架构在浅层网络上的表现比在一般网络上差，因为复杂的任务需要在更深的网络中进行转换。我们提出的方法并不强迫主分支保留完整的原始信息，而是通过辅助监督机制生成有用的梯度来更新它。这种设计的优点是所提出的方法也可以应用于较浅的网络。\n最后，由于在推理阶段可以去除辅助可逆分支，因此可以保留原始网络的推理能力。我们也可以选择PGI中的任意可逆架构来起到辅助可逆分支的作用。\n4.1.2 Multi-level Auxiliary Information 在本节中，我们将讨论多级辅助信息如何工作。包括多个预测分支的深度监督架构如图 3 (c) 所示。对于目标检测，不同的特征金字塔可用于执行不同的任务，例如它们一起可以检测不同大小的目标。因此，连接到深度监督分支后，会引导浅层特征学习小物体检测所需的特征，此时系统会将其他尺寸的物体的位置视为背景。然而，上述行为会导致深层特征金字塔丢失大量预测目标对象所需的信息。关于这个问题，我们认为每个特征金字塔都需要接收所有目标对象的信息，以便后续的主分支可以保留完整的信息来学习对各种目标的预测。\n辅助可逆分支： (1) 可以还原所有的特征信息 （2）推理的时候，不会额外增加计算量\n多级辅助信息： 不分大、中、小目标，把所有物体的特征都汇聚在一起\n4.2. Generalized ELAN 在本节中，我们将描述所提出的新网络架构——GELAN。通过结合采用梯度路径规划设计的两种神经网络架构CSPNet [64]和ELAN [65]，我们设计了兼顾轻量级、推理速度和准确性的广义高效层聚合网络（GELAN）。其整体架构如图 4 所示。我们将最初仅使用卷积层堆叠的 ELAN [65] 的功能推广到可以使用任何计算块的新架构。\nELAN 和 GELAN 的区别：\nELAN使用的是常规的卷积操作\nGELAN是把卷积换成了任意的模块，最后转到相同的通道数、相同维度大小即可\nExperiments 实验是一篇论文，审稿人看的比较仔细的地方。学习一下别人的写法\n附录可以给审稿人和读者，更直观的看到实现的细节\n5.1. Experimental Setup 我们使用 MS COCO 数据集验证了所提出的方法。\n所有实验设置均遵循 YOLOv7 AF [63]，而数据集为 MS COCO 2017 分割。我们提到的所有模型都是使用从头开始训练策略进行训练的，总训练次数为 500 epoch。在设置学习率时，我们在前三个epoch中使用线性预热，随后的epoch根据模型规模设置相应的衰减方式。至于最后 15 个时期，我们关闭马赛克数据增强。更多设置请参考附录。\n5.2 Implementation Details 我们分别基于 YOLOv7 [63] 和 Dynamic YOLOv7 [36] 构建了 YOLOv9 的通用版本和扩展版本。\n在网络架构的设计中，我们使用 CSPNet 块 [64] 和计划的 RepConv [63] 作为计算块，用 GELAN 替换了 ELAN [65]。我们还简化了下采样模块并优化了无锚预测头。至于PGI的辅助损失部分，我们完全遵循YOLOv7的辅助头设置。详情请参阅附录。\n5.3 Comparison with state-of-the-arts 表 1 列出了我们提出的 YOLOv9 与其他从头开始训练的实时目标检测器的比较。总体而言，现有方法中性能最好的方法是用于轻量级模型的 YOLO MS-S [7]、用于中型模型的 YOLO MS [7]、用于通用模型的 YOLOv7 AF [63] 和用于大型模型的 YOLOv8-X [15]。与轻量级和中型模型YOLO MS[7]相比，YOLOv9的参数减少了约10%，计算量减少了5∼15%，但AP仍然有0.4∼0.6%的提升。与YOLOv7 AF相比，YOLOv9-C的参数减少了42%，计算量减少了21%，但达到了相同的AP（53%）。与YOLOv8-X相比，YOLOv9-X参数减少15%，计算量减少25%，AP显着提升1.7%。上述对比结果表明，我们提出的YOLOv9与现有方法相比在各方面都有显着改进。\n另一方面，我们也将ImageNet预训练模型纳入对比，结果如图5所示。我们分别根据参数和计算量进行比较。就参数数量而言，性能最好的大型模型是 RT DETR [43]。从图5中我们可以看到，使用传统卷积的YOLOv9在参数利用率上甚至比使用深度卷积的YOLO MS还要好。至于大型模型的参数利用率，也大大超过了使用ImageNet预训练模型的RT DETR。更棒的是，在深度模型中，YOLOv9展示了使用PGI的巨大优势。通过准确保留和提取将数据映射到目标所需的信息，我们的方法仅需要 64% 的参数，同时保持 RT DETR-X 的精度。\n至于计算量，现有最好的模型从最小到最大依次是YOLO MS [7]、PP YOLOE [74]和RT DETR [43]。从图5中我们可以看到，YOLOv9在计算复杂度方面远远优于从头开始训练的方法。另外，如果与基于深度卷积和基于ImageNet的预训练模型相比，YOLOv9也很有竞争力。\n5.4 Ablation Studies 5.4.1 Generalized ELAN 对于 GELAN，我们首先对计算模块进行消融研究。我们分别使用Res块[21]、Dark块[49]和CSP块[64]进行实验。表2表明，用不同的计算块替换ELAN中的卷积层后，系统可以保持良好的性能。用户确实可以自由更换计算块并在各自的推理设备上使用它们。在不同的计算块替换中，CSP 块的性能特别好。它们不仅减少了参数量和计算量，而且将 AP 提高了 0.7%。因此，我们选择CSPELAN作为YOLOv9中GELAN的组成单元。\n接下来，我们对不同尺寸的GELAN进行ELAN块深度和CSP块深度实验，并将结果显示在表3中。我们可以看到，当ELAN的深度从1增加到2时，精度显着提高。但当深度大于等于2时，无论是提高ELAN深度还是CSP深度，参数数量、计算量和精度总是呈现线性关系。这意味着 GELAN 对深度不敏感。\n也就是说，用户可以任意组合GELAN中的组件来设计网络架构，无需特殊设计即可拥有性能稳定的模型。在表3中，对于YOLOv9-{S,M,C}，我们将ELAN深度和CSP深度的配对设置为{{2, 3}, {2, 1}, {2, 1}}。\n5.4.2 Programmable Gradient Information 在PGI方面，我们分别对backbone和neck的辅助可逆分支和多级辅助信息进行了消融研究。我们设计了辅助可逆分支ICN来使用DHLC[34]链接来获取多级可逆信息。对于多级辅助信息，我们使用FPN和PAN进行消融研究，PFH的作用相当于传统的深度监督。所有实验的结果列于表4中。从表4中我们可以看出，PFH仅在深度模型中有效，而我们提出的PGI可以在不同组合下提高精度。尤其是使用ICN时，我们得到了稳定且更好的结果。我们还尝试将YOLOv7[63]中提出的lead-head指导分配应用于PGI的辅助监督，并取得了更好的性能。\n我们进一步将PGI和深度监督的概念应用到不同规模的模型上，并比较结果，结果如表5所示。正如一开始分析的那样，深度监督的引入会导致浅层模型精度的损失。对于一般模型来说，引入深度监督会导致性能不稳定，而深度监督的设计理念只能在极深的模型中带来收益。所提出的PGI可以有效处理信息瓶颈和信息破碎等问题，并且可以全面提高不同规模模型的准确性。 PGI 的概念带来了两个宝贵的贡献。第一个是让辅助监督方法适用于浅层模型，第二个是让深层模型训练过程获得更可靠的梯度。这些梯度使深度模型能够使用更准确的信息来建立数据和目标之间的正确相关性\n最后，我们在表中显示了从基线 YOLOv7 到 YOLOv9E 逐渐增加组件的结果。我们提出的GELAN和PGI给模型带来了全面的改进。\n5.5 Visualization 本节将探讨信息瓶颈问题并将其可视化。此外，我们还将可视化所提出的 PGI 如何使用可靠的梯度来找到数据和目标之间的正确相关性。在图6中，我们展示了在不同架构下使用随机初始权重作为前馈获得的特征图的可视化结果。我们可以看到，随着层数的增加，所有架构的原始信息逐渐减少。例如，在PlainNet的第50层，很难看到物体的位置，并且所有可区分的特征将在第100层丢失。对于ResNet，虽然在第50层仍然可以看到物体的位置，但边界信息已经丢失。当深度达到第100层时，整个图像变得模糊。 CSPNet 和提出的 GELAN 都表现得非常好，并且它们都可以保持支持清晰识别对象的特征直到第 200 层。对比中，GELAN结果更稳定，边界信息更清晰\n图7用于展示PGI是否可以在训练过程中提供更可靠的梯度，使得用于更新的参数能够有效捕获输入数据与目标之间的关系。图7显示了GELAN和YOLOv9（GELAN + PGI）的特征图在PAN偏置预热中的可视化结果。从图7（b）和（c）的比较中，我们可以清楚地看到PGI准确而简洁地捕获了包含对象的区域**。对于不使用PGI的GELAN，我们发现它在检测物体边界时存在发散**，并且在某些背景区域也产生了意想不到的响应。这个实验证实了PGI确实可以提供更好的梯度来更新参数，并使主分支的前馈阶段能够保留更重要的特征。\nConclusions 在本文中，我们提出使用PGI来解决信息瓶颈问题以及深度监督机制不适合轻量级神经网络的问题。我们设计了 GELAN，一个高效、轻量级的神经网络。在物体检测方面，GELAN在不同的计算块和深度设置下都具有强大且稳定的性能。它确实可以广泛扩展为适合各种推理设备的模型。针对以上两个问题，PGI的引入使得轻量级模型和深度模型都获得了精度的显着提升。 PGI和GELAN相结合设计的YOLOv9已经展现出强大的竞争力。其出色的设计使得深度模型相比YOLOv8减少了49%的参数数量和43%的计算量，但在MS COCO数据集上仍然有0.6%的AP提升。\n","permalink":"https://swimmingliu.cn/posts/papernotes/2024-yolov9/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e如今的深度学习方法主要关注如何设计\u003cstrong\u003e最合适的目标函数\u003c/strong\u003e，使模型的预测结果能够最接近真实情况。同时，必须设计一个\u003cstrong\u003e适当的架构\u003c/strong\u003e，可以帮助\u003cstrong\u003e获取足够的信息进行预测\u003c/strong\u003e。现有方法忽略了一个事实，即\u003cstrong\u003e当输入数据经过逐层特征提取和空间变换时\u003c/strong\u003e，\u003cstrong\u003e大量信息将会丢失\u003c/strong\u003e。本文将深入研究数据通过\u003cstrong\u003e深度网络传输时数据丢失的重要问题\u003c/strong\u003e，即\u003cstrong\u003e信息瓶颈和可逆函数\u003c/strong\u003e。我们提出了\u003cstrong\u003e可编程梯度信息（PGI）\u003cstrong\u003e的概念来应对深度网络实现\u003c/strong\u003e多个目标所需的各种变化\u003c/strong\u003e。\nPGI可以为\u003cstrong\u003e目标任务计算目标函数\u003c/strong\u003e提供\u003cstrong\u003e完整的输入信息\u003c/strong\u003e，从而获得\u003cstrong\u003e可靠的梯度信息来更新网络权值\u003c/strong\u003e。此外，还设计了一种基于\u003cstrong\u003e梯度路径规划的新型轻量级网络架构\u003c/strong\u003e——\u003cstrong\u003e通用高效层聚合网络（GELAN）\u003c/strong\u003e。GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用\u003cstrong\u003e传统的卷积算子\u003c/strong\u003e即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得\u003cstrong\u003etrain-from-scratch (从零开始训练) 模型能够比使用大数据集预训练\u003c/strong\u003e的state-of-theart模型获得更好的结果，对比结果如图1所示。源代码位于：https： //github.com/WongKinYiu/yolov9。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e核心创新点:  依然是网络结构的创新\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eProgrammable Gradient Information (PGI)\u003c/li\u003e\n\u003cli\u003eGeneralized Efficient Layer Aggregation Network（GELAN）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/f6b3ea45-d79d-11ee-a66b-c858c0c1debd\" alt=\"image-20240301113226341\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e基于深度学习的模型在计算机视觉、语言处理和语音识别等各个领域都表现出了比过去的人工智能系统更好的性能。近年来，深度学习领域的研究人员主要关注如何开发更强大的系统架构和学习方法，例如CNN，Transformers[8,9,40] 、41、60、69、70]，Perceivers[26、26、32、52、56、81、81]和Mambas[17、38、80]。此外，一些研究人员尝试开发更通用的目标函数，例如损失函数[5,45,46,50,77,78]，标签分配[10,12,33,67,79]和辅助监督[18] 、20、24、28、29、51、54、68、76]。上述研究都试图精确地找到\u003cstrong\u003e输入和目标任务之间的映射\u003c/strong\u003e。然而，大多数过去的方法都忽略了\u003cstrong\u003e输入数据在前馈过程中可能会产生不可忽略的信息丢失量\u003c/strong\u003e。这种\u003cstrong\u003e信息丢失\u003c/strong\u003e可能会导致\u003cstrong\u003e有偏差的梯度流\u003c/strong\u003e，随后用于更新模型。上述问题可能导致深度网络\u003cstrong\u003e在目标和输入之间建立不正确的关联\u003c/strong\u003e，导致训练后的模型产生不正确的预测。\u003c/p\u003e\n\u003cp\u003e在深度网络中，\u003cstrong\u003e输入数据在前馈过程中丢失信息的现象\u003c/strong\u003e俗称\u003cstrong\u003e信息瓶颈\u003c/strong\u003e[59]，其示意图如图2所示。目前可以缓解这种现象的主要方法有：（1）\u003cstrong\u003e可逆架构\u003c/strong\u003e的使用[3,16,19]：该方法主要\u003cstrong\u003e使用重复的输入数据，并以显式的方式维护输入数据的信息\u003c/strong\u003e； （2）使用\u003cstrong\u003eMasked建模\u003c/strong\u003e[1,6,9,27,71,73]：主要利用重构损失，采用\u003cstrong\u003e隐式方式最大化提取特征并保留输入信息\u003c/strong\u003e； （3）引入\u003cstrong\u003e深度监督\u003c/strong\u003e概念[28,51,54,68]：它利用\u003cstrong\u003e没有丢失太多重要信息的浅层特征来预先建立从特征到目标的映射\u003c/strong\u003e，以确保\u003cstrong\u003e重要信息能够被传递到更深的层次\u003c/strong\u003e。然而，上述方法在训练过程和推理过程中都存在不同的缺点。例如，\u003cstrong\u003e可逆架构需要额外的层来组合重复馈送的输入数据\u003c/strong\u003e，这将显着增加推理成本。另外，由于\u003cstrong\u003e输入数据层到输出层不能有太深的路径\u003c/strong\u003e，这种限制将导致\u003cstrong\u003e在训练过程中难以对高阶语义信息进行建模\u003c/strong\u003e。对于 \u003cstrong\u003eMasked 建模\u003c/strong\u003e，其\u003cstrong\u003e重建损失有时与目标损失相冲突\u003c/strong\u003e。此外，大多数\u003cstrong\u003e掩码机制还会产生与数据的不正确关联\u003c/strong\u003e。 对于\u003cstrong\u003e深层监督\u003c/strong\u003e机制来说，会产生\u003cstrong\u003e误差累积\u003c/strong\u003e，如果\u003cstrong\u003e浅层监督在训练过程中丢失信息\u003c/strong\u003e，\u003cstrong\u003e后续层将无法检索到所需信息\u003c/strong\u003e。上述现象在\u003cstrong\u003e困难任务\u003c/strong\u003e和\u003cstrong\u003e小模型上\u003c/strong\u003e会更加\u003cstrong\u003e显着\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e针对上述问题，我们提出了一个新的概念，即\u003cstrong\u003e可编程梯度信息（PGI）\u003c/strong\u003e。其概念是通过\u003cstrong\u003e辅助可逆分支生成可靠的梯度\u003c/strong\u003e，使得\u003cstrong\u003e深层特征仍然能够保持执行目标任务的关键特征\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e辅助可逆分支的设计\u003c/strong\u003e可以避免传统的\u003cstrong\u003e融合多路径特征的深度监督过程\u003c/strong\u003e可能造成的\u003cstrong\u003e语义损失\u003c/strong\u003e。换句话说，我们在\u003cstrong\u003e不同语义层面上编程梯度信息传播\u003c/strong\u003e，从而达到最佳的训练结果。 PGI的\u003cstrong\u003e可逆架构建立在辅助分支上\u003c/strong\u003e，因此\u003cstrong\u003e没有额外的成本\u003c/strong\u003e。由于PGI可以\u003cstrong\u003e自由选择适合目标任务的损失函数\u003c/strong\u003e，因此也克服了\u003cstrong\u003eMasked建模\u003c/strong\u003e遇到的问题。所提出的\u003cstrong\u003ePGI机制\u003c/strong\u003e可以应用于各种规模的\u003cstrong\u003e深度神经网络\u003c/strong\u003e，并且比仅适用于\u003cstrong\u003e非常深的神经网络\u003c/strong\u003e的\u003cstrong\u003e深度监督机制更通用\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在本文中，我们还基于ELAN[65]设计了\u003cstrong\u003e广义ELAN（GELAN）\u003c/strong\u003e，GELAN的设计同时\u003cstrong\u003e考虑了参数量、计算复杂度、准确性和推理速度\u003c/strong\u003e。这种设计允许用户\u003cstrong\u003e针对不同的推理设备任意选择合适的计算块\u003c/strong\u003e。我们将提出的PGI和GELAN结合起来，然后设计了新一代YOLO系列物体检测系统，我们称之为YOLOv9。我们使用MS COCO数据集进行实验，实验结果验证了我们提出的YOLOv9在所有比较中都取得了顶尖的性能。\u003c/p\u003e\n\u003cp\u003e我们总结本文的贡献如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e我们从\u003cstrong\u003e可逆函数的角度\u003c/strong\u003e对\u003cstrong\u003e现有的深度神经网络架构进行了理论分析\u003c/strong\u003e，通过这个过程\u003cstrong\u003e我们成功地解释了许多过去难以解释的现象\u003c/strong\u003e。我们还基于此分析\u003cstrong\u003e设计了PGI和辅助可逆分支\u003c/strong\u003e，并取得了优异的结果。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e我们设计的PGI解决了\u003cstrong\u003e深度监督\u003c/strong\u003e只能用于\u003cstrong\u003e极深的神经网络架构的问题\u003c/strong\u003e，从而让\u003cstrong\u003e新的轻量级架构真正应用于日常生活中\u003c/strong\u003e。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e我们设计的GELAN仅使用\u003cstrong\u003e常规卷积\u003c/strong\u003e来实现比基于最先进技术的\u003cstrong\u003e深度卷积设计更高的参数利用率\u003c/strong\u003e，同时表现出\u003cstrong\u003e轻、快速、准确\u003c/strong\u003e的巨大优势。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e结合所提出的PGI和GELAN，YOLOv9在MS COCO数据集上的目标检测性能在各个方面都大大超过了现有的实时目标检测器。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eProgrammable Gradient Information (PGI)：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e自由选择适合目标任务的损失函数\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e可逆结构建立辅助分支，不增加推理成本\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e适用于各种规模的深度神经网络\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eGELAN：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e轻、快速、准确\u003c/li\u003e\n\u003cli\u003e采用常规卷积吊打其他新颖卷积\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"related-work\"\u003eRelated work\u003c/h2\u003e\n\u003ch3 id=\"21-real-time-object-detectors\"\u003e2.1 Real-time Object Detectors\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e目前主流的实时目标检测器是YOLO系列[2,7,13–15,25,30,31,47–49,61–63,74,75]，这些模型大多数使用CSPNet[64]或 ELAN [65] 及其变体作为主要计算单元。在特征集成方面，通常使用改进的PAN[37]或FPN[35]作为工具，然后使用改进的YOLOv3头[49]或FCOS头[57, 58]作为预测头。最近也提出了一些实时目标检测器，例如 RT DETR [43]，其基础是 DETR [4]。然而，由于DETR系列目标检测器在没有相应领域预训练模型的情况下很难应用于新领域，因此目前应用最广泛的实时目标检测器仍然是YOLO系列。本文选择 YOLOv7 [63] 作为开发该方法的基础，该方法已在各种计算机视觉任务和各种场景中被证明有效。\u003c/p\u003e","title":"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information"},{"content":"Introduction YOLOSHOW is a graphical user interface (GUI) application embed withYOLOv5 YOLOv7 YOLOv8 YOLOv9 YOLOv10 RT-DETR algorithm.\nEnglish \u0026nbsp; | \u0026nbsp; 简体中文 Demo Video YOLOSHOW v1.x : YOLOSHOW-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\nYOLOSHOW v2.x : YOLOSHOWv2.0-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\nTodo List Add YOLOv9 Algorithm Adjust User Interface (Menu Bar) Complete Rtsp Function Support Instance Segmentation （ YOLOv5 \u0026amp; YOLOv8 ） Add RT-DETR Algorithm ( Ultralytics repo) Add Model Comparison Mode（VS Mode） Support Pose Estimation （ YOLOv8 ） Support Http Protocol in Rtsp Function ( Single Mode ) Support Oriented Bounding Boxes ( YOLOv8 ) Add YOLOv10 Algorithm Support Dragging File Input Tracking \u0026amp; Counting ( Industrialization ) Functions 1. Support Image / Video / Webcam / Folder (Batch) / IPCam Object Detection Choose Image / Video / Webcam / Folder (Batch) / IPCam in the menu bar on the left to detect objects.\n2. Change Models / Hyper Parameters dynamically When the program is running to detect targets, you can change models / hyper Parameters\nSupport changing model in YOLOv5 / YOLOv7 / YOLOv8 / YOLOv9 / RTDETR / YOLOv5-seg / YOLOv8-seg / YOLOv10 dynamically Support changing IOU / Confidence / Delay time / line thickness dynamically 3. Loading Model Automatically Our program will automatically detect pt files including YOLOv5 Models / YOLOv7 Models / YOLOv8 Models / YOLOv9 Models / YOLOv10 Models that were previously added to the ptfiles folder.\nIf you need add the new pt file, please click Import Model button in Settings box to select your pt file. Then our program will put it into ptfiles folder.\nNotice :\nAll pt files are named including yolov5 / yolov7 / yolov8 / yolov9 / yolov10 / rtdetr . (e.g. yolov8-test.pt) If it is a pt file of segmentation mode, please name it including yolov5n-seg / yolov8s-seg . (e.g. yolov8n-seg-test.pt) If it is a pt file of pose estimation mode, please name it including yolov8n-pose . (e.g. yolov8n-pose-test.pt) If it is a pt file of oriented bounding box mode, please name it including yolov8n-obb . (e.g. yolov8n-obb-test.pt) 4. Loading Configures After startup, the program will automatically loading the last configure parameters. After closedown, the program will save the changed configure parameters. 5. Save Results If you need Save results, please click Save Mode before detection. Then you can save your detection results in selected path.\n6. Support Object Detection, Instance Segmentation and Pose Estimation From YOLOSHOW v3.0，our work supports both Object Detection , Instance Segmentation, Pose Estimation and Oriented Bounding Box. Meanwhile, it also supports task switching between different versions，such as switching from YOLOv5 Object Detection task to YOLOv8 Instance Segmentation task.\n7. Support Model Comparison among Object Detection, Instance Segmentation, Pose Estimation and Oriented Bounding Box From YOLOSHOW v3.0，our work supports compare model performance among Object Detection, Instance Segmentation, Pose Estimation and Oriented Bounding Box.\nPreparation Experimental environment OS : Windows 11 CPU : Intel(R) Core(TM) i7-10750H CPU @2.60GHz 2.59 GHz GPU : NVIDIA GeForce GTX 1660Ti 6GB 1. Create virtual environment create a virtual environment equipped with python version 3.9, then activate environment.\nconda create -n yoloshow python=3.9 conda activate yoloshow 2. Install Pytorch frame Windows: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Linux: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Change other pytorch version in 3. Install dependency package Switch the path to the location of the program\ncd {the location of the program} Install dependency package of program\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 4. Add Font Windows User Copy all font files *.ttf in fonts folder into C:\\Windows\\Fonts\nLinux User mkdir -p ~/.local/share/fonts sudo cp fonts/Shojumaru-Regular.ttf ~/.local/share/fonts/ sudo fc-cache -fv MacOS User The MacBook is so expensive that I cannot afford it, please install .ttf by yourself. 😂\n5. Run Program python main.py Frames Reference YOLO Algorithm YOLOv5 YOLOv7 YOLOv8 YOLOv9 YOLOv10\nYOLO Graphical User Interface YOLOSIDE\tPyQt-Fluent-Widgets\n","permalink":"https://swimmingliu.cn/posts/diary/yoloshow/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eYOLOSHOW\u003c/strong\u003e\u003c/em\u003e is a graphical user interface (GUI) application embed with\u003ccode\u003eYOLOv5\u003c/code\u003e \u003ccode\u003eYOLOv7\u003c/code\u003e \u003ccode\u003eYOLOv8\u003c/code\u003e \u003ccode\u003eYOLOv9\u003c/code\u003e \u003ccode\u003eYOLOv10\u003c/code\u003e \u003ccode\u003eRT-DETR\u003c/code\u003e algorithm.\u003c/p\u003e\n \u003cp align=\"center\"\u003e \n  English \u0026nbsp; | \u0026nbsp; \u003ca href=\"https://github.com/SwimmingLiu/YOLOSHOW/blob/master/README_cn.md\"\u003e简体中文\u003c/a\u003e\n \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/YOLOSHOW-SCREENSHOT.png\" alt=\"YOLOSHOW-Screen\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"demo-video\"\u003eDemo Video\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eYOLOSHOW v1.x\u003c/code\u003e : \u003ca href=\"https://www.bilibili.com/video/BV1BC411x7fW\"\u003eYOLOSHOW-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eYOLOSHOW v2.x\u003c/code\u003e : \u003ca href=\"https://www.bilibili.com/video/BV1ZD421E7m3\"\u003eYOLOSHOWv2.0-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"todo-list\"\u003eTodo List\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Add \u003ccode\u003eYOLOv9\u003c/code\u003e Algorithm\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Adjust User Interface (Menu Bar)\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Complete Rtsp Function\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Support Instance Segmentation （ \u003ccode\u003eYOLOv5\u003c/code\u003e \u0026amp; \u003ccode\u003eYOLOv8\u003c/code\u003e ）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Add \u003ccode\u003eRT-DETR\u003c/code\u003e Algorithm ( \u003ccode\u003eUltralytics\u003c/code\u003e repo)\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Add Model Comparison Mode（VS Mode）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Support Pose Estimation （ \u003ccode\u003eYOLOv8\u003c/code\u003e ）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Support Http Protocol in Rtsp Function ( Single Mode )\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Support Oriented Bounding Boxes ( \u003ccode\u003eYOLOv8\u003c/code\u003e )\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Add \u003ccode\u003eYOLOv10\u003c/code\u003e Algorithm\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e Support Dragging File Input\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e Tracking \u0026amp; Counting ( \u003ccode\u003eIndustrialization\u003c/code\u003e )\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"functions\"\u003eFunctions\u003c/h2\u003e\n\u003ch3 id=\"1-support-image--video--webcam--folder-batch--ipcam-object-detection\"\u003e1. Support Image / Video / Webcam / Folder (Batch) / IPCam Object Detection\u003c/h3\u003e\n\u003cp\u003eChoose Image / Video / Webcam / Folder (Batch) / IPCam in the menu bar on the left to detect objects.\u003c/p\u003e","title":"YOLOSHOW - YOLOv5/YOLOv7/YOLOv8/YOLOv9/RTDETR GUI based on Pyside6"},{"content":"介绍 YOLOSHOW 是一款集合了 YOLOv5 YOLOv7 YOLOv8 YOLOv9 YOLOv10 RT-DETR 的图形化界面程序\nEnglish \u0026nbsp; | \u0026nbsp; 简体中文 演示视频 YOLOSHOW v1.x : YOLOSHOW-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\nYOLOSHOW v2.x : YOLOSHOWv2.0-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\n待做清单 加入 YOLOv9 算法 调整UI (菜单栏) 完成Rtsp功能 支持实例分割 （ YOLOv5 \u0026amp; YOLOv8 ） 加入 RT-DETR 算法 ( Ultralytics 仓库) 加入模型对比模式（VS Mode） 支持姿态估计 （ YOLOv8 ） 支持 Http 协议 ( Single Mode ) 支持旋转框 ( YOLOv8 ) 加入 YOLOv10 算法 支持拖拽文件输入 追踪和计数模型 ( 工业化 ) 功能 1. 支持 图片 / 视频 / 摄像头 / 文件夹（批量）/ 网络摄像头 目标检测 选择左侧菜单栏的图片 / 视频 / 摄像头 / 文件夹（批量）/ 网络摄像头 进行目标检测\n2. 动态切换模型 / 调整超参数 程序开始检测时，支持动态切换模型 / 调整超参数\n支持动态切换 YOLOv5 / YOLOv7 / YOLOv8 / YOLOv9 / RTDETR / YOLOv5-seg / YOLOv8-seg / YOLOv10 模型 支持动态修改 IOU / Confidence / Delay time / line thickness 超参数 3. 动态加载模型 程序可以自动检测ptfiles 文件夹中包含 YOLOv5 Models / YOLOv7 Models / YOLOv8 Models / YOLOv9 Models / YOLOv10 Models pt 模型.\n如果你需要导入新的 pt 文件, 请点击 Settings 框中的 Import Model 按钮 来选择需要导入的 pt 文件. 然后程序会把该文件复制到 ptfiles 文件夹下.\nNotice :\n所有的 pt 模型文件命名必须包含 yolov5 / yolov7 / yolov8 / yolov9 / yolov10 / rtdetr 中的任意一个版本. (如 yolov8-test.pt) 如果是分割类型的 pt 文件, 命名中应包含 yolov5n-seg / yolov8s-seg 中的任意一个版本. (如 yolov8n-seg-test.pt) 如果是姿态检测类型的 pt 文件, 命名中应包含yolov8n-pose 中的任意一个版本. (如 yolov8n-pose-test.pt) 如果是旋转框类型的 pt 文件, 命名中应包含yolov8n-obb 中的任意一个版本. (e.g. yolov8n-obb-test.pt) 4. 加载超参数配置 程序启动后, 自动加载最近保存的超参数配置. 程序关闭后, 自动保存最近修改的超参数配置. 5. 保存检测结果 如果需要保存检测结果，请在检测前点击 Save Mode . 然后等待检测完毕，选择需要保存的路径进行结果保存.\n6. 同时支持目标检测、实例分割和姿态估计 从 YOLOSHOW v3.0 起 ，支持目标检测、实例分割、姿态估计和旋转框多种任务。同时支持不同版本的任务切换，如从YOLOv5 目标检测任务 切换到 YOLOv8 实例分割任务。\n7. 支持目标检测、实例分割、姿态估计和旋转框模型对比模式 从 YOLOSHOW v3.0 起，支持目标检测、实例分割、姿态估计和旋转框模型对比模式。\n运行准备工作 实验环境 OS : Windows 11 CPU : Intel(R) Core(TM) i7-10750H CPU @2.60GHz 2.59 GHz GPU : NVIDIA GeForce GTX 1660Ti 6GB 1. 创建虚拟环境 创建内置Python 3.9的conda虚拟环境, 然后激活该环境.\nconda create -n yoloshow python=3.9 conda activate yoloshow 2. 安装Pytorch框架 Windows: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Linux: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 安装其他版本的 Pytorch : 3. 安装依赖包 切换到YOLOSHOW程序所在的路径\ncd {YOLOSHOW程序所在的路径} 安装程序所需要的依赖包\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 4. 添加字体 Windows 用户 把所有的fonts 文件夹中的字体文件 *.ttf 复制到 C:\\Windows\\Fonts\nLinux 用户 mkdir -p ~/.local/share/fonts sudo cp fonts/Shojumaru-Regular.ttf ~/.local/share/fonts/ sudo fc-cache -fv MacOS 用户 MacBook实在太贵了，我买不起。你们自己想办法安装吧~😂\n5. 运行程序 python main.py 使用框架 参考文献 YOLO 算法 YOLOv5 YOLOv7 YOLOv8\tYOLOv9 YOLOv10\nYOLO 图形化界面 YOLOSIDE\tPyQt-Fluent-Widgets\n","permalink":"https://swimmingliu.cn/posts/diary/yoloshow-cn/","summary":"\u003ch2 id=\"介绍\"\u003e介绍\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eYOLOSHOW\u003c/strong\u003e\u003c/em\u003e 是一款集合了 \u003ccode\u003eYOLOv5\u003c/code\u003e \u003ccode\u003eYOLOv7\u003c/code\u003e \u003ccode\u003eYOLOv8\u003c/code\u003e \u003ccode\u003eYOLOv9\u003c/code\u003e  \u003ccode\u003eYOLOv10\u003c/code\u003e \u003ccode\u003eRT-DETR\u003c/code\u003e 的图形化界面程序\u003c/p\u003e\n\u003cp align=\"center\"\u003e \n  \u003ca href=\"https://github.com/SwimmingLiu/YOLOSHOW/blob/master/README.md\"\u003e English\u003c/a\u003e \u0026nbsp; | \u0026nbsp; 简体中文\u003c/a\u003e\n \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/YOLOSHOW-SCREENSHOT.png\" alt=\"YOLOSHOW-Screen\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"演示视频\"\u003e演示视频\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eYOLOSHOW v1.x\u003c/code\u003e : \u003ca href=\"https://www.bilibili.com/video/BV1BC411x7fW\"\u003eYOLOSHOW-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eYOLOSHOW v2.x\u003c/code\u003e : \u003ca href=\"https://www.bilibili.com/video/BV1ZD421E7m3\"\u003eYOLOSHOWv2.0-YOLOv9/YOLOv8/YOLOv7/YOLOv5/RTDETR GUI\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"待做清单\"\u003e待做清单\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 加入 \u003ccode\u003eYOLOv9\u003c/code\u003e 算法\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 调整UI (菜单栏)\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 完成Rtsp功能\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 支持实例分割 （ \u003ccode\u003eYOLOv5\u003c/code\u003e \u0026amp; \u003ccode\u003eYOLOv8\u003c/code\u003e ）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 加入 \u003ccode\u003eRT-DETR\u003c/code\u003e 算法 ( \u003ccode\u003eUltralytics\u003c/code\u003e 仓库)\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 加入模型对比模式（VS Mode）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 支持姿态估计 （ \u003ccode\u003eYOLOv8\u003c/code\u003e ）\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 支持 Http 协议 ( Single Mode )\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 支持旋转框 ( \u003ccode\u003eYOLOv8\u003c/code\u003e )\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 加入 \u003ccode\u003eYOLOv10\u003c/code\u003e 算法\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 支持拖拽文件输入\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 追踪和计数模型 ( \u003ccode\u003e工业化\u003c/code\u003e )\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"功能\"\u003e功能\u003c/h2\u003e\n\u003ch3 id=\"1-支持-图片--视频--摄像头--文件夹批量-网络摄像头-目标检测\"\u003e1. 支持 图片 / 视频 / 摄像头 / 文件夹（批量）/ 网络摄像头 目标检测\u003c/h3\u003e\n\u003cp\u003e选择左侧菜单栏的图片 / 视频 / 摄像头 / 文件夹（批量）/ 网络摄像头 进行目标检测\u003c/p\u003e","title":"YOLOSHOW 中文版 - YOLOv5/YOLOv7/YOLOv8/YOLOv9/RTDETR GUI based on Pyside6"},{"content":"安装 Xshell 和 Xftp https://www.netsarang.com/en/xshell-download/ # Xshell下载连接 https://blog.csdn.net/m0_67400972/article/details/125346023 # 安装教程 添加Xshell连接 其中 server.ip 为服务器公网ip地址，端口为 6969\n安装Anaconda3 每个用户均被分配 AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh 于主目录\nbash AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh # 安装anaconda3 输入 yes 后， 再按回车键 即可\n初始化Anaconda3 conda init bash\t# 初始化conda 然后重新使用Xshell 连接即可\nMagic Network 下载外网文件、克隆Github项目等操作，必须使用Magic Network\nexport http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 取消Magic Network\nunset http_proxy unset https_proxy 如果使用上面的命令，不能连接Google. 需要远程桌面连接，打开CFW (默认是打开的)\nnohup bash /home/dell/LYJ/Clash/cfw \u0026gt; cfw.out 国内镜像下载 pip 清华源下载\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple packge # packge为包名 conda 配置镜像\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ # 以上两条是Anaconda官方库的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ # 以上是Anaconda第三方库 Conda Forge的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ # 以上是Pytorch的Anaconda第三方镜像 远程桌面连接 远程连接直接找师兄问 向日葵 和 Teamviewer 密码，连接即可\n注意事项 建议非必要不使用远程连接（由于在同一时间段内，远程连接只能单用户使用） 使用远程连接，请先阅读服务器壁纸上的注意事项 如需上传文件，尽量使用移动硬盘，到918实验室拷贝至服务器上 ","permalink":"https://swimmingliu.cn/posts/diary/zstu_server_manuscript/","summary":"\u003ch2 id=\"安装-xshell-和-xftp\"\u003e安装 Xshell 和 Xftp\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehttps://www.netsarang.com/en/xshell-download/ \u003cspan class=\"c1\"\u003e# Xshell下载连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehttps://blog.csdn.net/m0_67400972/article/details/125346023 \u003cspan class=\"c1\"\u003e# 安装教程\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"添加xshell连接\"\u003e添加Xshell连接\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/B6xRW.png\" alt=\"image-20240105113129928\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003eserver.ip\u003c/code\u003e 为服务器公网ip地址，端口为 \u003ccode\u003e6969\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"安装anaconda3\"\u003e安装Anaconda3\u003c/h2\u003e\n\u003cp\u003e每个用户均被分配 \u003ccode\u003eAnacondaAnaconda3-2023.07-1-Linux-x86_64.sh \u003c/code\u003e 于主目录\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebash AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh \u003cspan class=\"c1\"\u003e# 安装anaconda3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/B6KTv.png\" alt=\"image-20240105113942737\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003e输入 \u003ccode\u003eyes\u003c/code\u003e 后， 再按回车键 即可\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/B6one.png\" alt=\"image-20240105114146047\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"初始化anaconda3\"\u003e初始化Anaconda3\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda init bash\t\u003cspan class=\"c1\"\u003e# 初始化conda\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e然后重新使用Xshell 连接即可\u003c/p\u003e\n\u003ch2 id=\"magic-network\"\u003eMagic Network\u003c/h2\u003e\n\u003cp\u003e下载外网文件、克隆Github项目等操作，必须使用Magic Network\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexport\u003c/span\u003e \u003cspan class=\"nv\"\u003ehttp_proxy\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ehttp://127.0.0.1:7890\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexport\u003c/span\u003e \u003cspan class=\"nv\"\u003ehttps_proxy\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ehttp://127.0.0.1:7890\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e取消Magic Network\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eunset\u003c/span\u003e http_proxy\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eunset\u003c/span\u003e https_proxy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果使用上面的命令，不能连接Google. 需要远程桌面连接，打开CFW (默认是打开的)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003enohup bash /home/dell/LYJ/Clash/cfw \u0026gt; cfw.out\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/B6So3.png\" alt=\"image-20240105115600487\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"国内镜像下载\"\u003e国内镜像下载\u003c/h2\u003e\n\u003cp\u003epip 清华源下载\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install -i https://pypi.tuna.tsinghua.edu.cn/simple packge      \u003cspan class=\"c1\"\u003e# packge为包名\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003econda 配置镜像\u003c/p\u003e","title":"ZSTU服务器使用教程 (Yang Li Lab)"},{"content":"FRP配置 跳板机 # frps.ini 配置 [common] bind_port = 7000 #frps服务监听的端口 token = 123 # 链接口令 ./frps -c frps.ini # 启动frps 服务器 # frpc.ini [common] server_addr = x.x.x.x # 此处为 跳板机 的公网ip server_port = 7000 # 跳板机上frps服务监听的端口 token = 123 # 链接口令 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 # 需要暴露的内网机器的端口 remote_port = 6000 # 暴露的内网机器的端口在vps上的端口 SSH连接 ssh -p 6000 swimmingliu@server.ip # 普通ssh 连接 ssh swimmingliu@server.ip 6000\t# xshell ssh连接 用户管理 添加用户 sudo adduser xxx 删除用户 sudo deluser xxx Magic Network export http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 Anaconda3 安装和配置 安装Anaconda3 wget --user-agent=\u0026#34;Mozilla\u0026#34; https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.07-1-Linux-x86_64.sh bash Anaconda3-2023.07-1-Linux-x86_64.sh # https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive 清华源 配置之前的envs cp -r old_envs_path anaconda/envs/\t#迁移之前的envs环境 完结撒花❀❀❀ ","permalink":"https://swimmingliu.cn/posts/diary/zstu_server_management/","summary":"\u003ch2 id=\"frp配置\"\u003eFRP配置\u003c/h2\u003e\n\u003ch3 id=\"跳板机\"\u003e跳板机\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# frps.ini 配置\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003ecommon\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003ebind_port\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e7000\u003c/span\u003e \u003cspan class=\"c1\"\u003e#frps服务监听的端口\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003etoken\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e123\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 链接口令\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e./frps -c frps.ini \u003cspan class=\"c1\"\u003e# 启动frps\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"服务器\"\u003e服务器\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# frpc.ini\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003ecommon\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eserver_addr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e x.x.x.x \u003cspan class=\"c1\"\u003e# 此处为 跳板机 的公网ip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eserver_port\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e7000\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 跳板机上frps服务监听的端口\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003etoken\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e123\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 链接口令\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003essh\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003etype\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e tcp\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003elocal_ip\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e 127.0.0.1 \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003elocal_port\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e22\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 需要暴露的内网机器的端口\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eremote_port\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"m\"\u003e6000\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 暴露的内网机器的端口在vps上的端口\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ssh连接\"\u003eSSH连接\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003essh -p \u003cspan class=\"m\"\u003e6000\u003c/span\u003e swimmingliu@server.ip \u003cspan class=\"c1\"\u003e# 普通ssh 连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003essh swimmingliu@server.ip 6000\t  \u003cspan class=\"c1\"\u003e# xshell ssh连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"用户管理\"\u003e用户管理\u003c/h2\u003e\n\u003ch3 id=\"添加用户\"\u003e添加用户\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo adduser xxx\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"删除用户\"\u003e删除用户\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo deluser xxx\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"magic-network\"\u003eMagic Network\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexport\u003c/span\u003e \u003cspan class=\"nv\"\u003ehttp_proxy\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ehttp://127.0.0.1:7890\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexport\u003c/span\u003e \u003cspan class=\"nv\"\u003ehttps_proxy\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ehttp://127.0.0.1:7890\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"anaconda3-安装和配置\"\u003eAnaconda3 安装和配置\u003c/h2\u003e\n\u003ch3 id=\"安装anaconda3\"\u003e安装Anaconda3\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ewget --user-agent\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Mozilla\u0026#34;\u003c/span\u003e https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.07-1-Linux-x86_64.sh\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebash Anaconda3-2023.07-1-Linux-x86_64.sh\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive 清华源\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"配置之前的envs\"\u003e配置之前的envs\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ecp -r old_envs_path anaconda/envs/\t\t\u003cspan class=\"c1\"\u003e#迁移之前的envs环境\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"完结撒花\"\u003e完结撒花❀❀❀\u003c/h2\u003e","title":"ZSTU Server Management"},{"content":"Introduction 基于知识图谱和neo4j图数据库的电影知识问答系统\nWorkflow DataBase 爬取豆瓣TOP1000电影信息数据\nFrontend 获取用户输入的信息 （语音输入 / 文本输入） 向电影知识问答后端服务器发送请求 获取返回结果 (成功 -\u0026gt; 4 / 失败 -\u0026gt; 5) 如果返回结果包含image信息，则显示图片和文字，否则只显示文字 请求基于gpt的AI模型服务器，并显示返回结果 Backend ​\t[准备工作] 训练 TF-IDF 向量算法和朴素贝叶斯分类器，用于预测用户文本所属的问题类别\n接受前端请求，获取用户输入信息 使用分词库解析用户输入的文本词性，提取关键词 根据贝叶斯分类器，分类出用户文本的问题类型 结合关键词与问题类别，在 Neo4j 中查询问题的答案 返回查询结果 （若问题类型为 演员信息 / 电影介绍，则附加图片url） WorkFlow Graph Frame DataBase Frontend Backend Reference Frontend 微信小程序：微信聊天机器人\nBackEnd 基于知识图谱的电影知识问答系统\n电影知识库问答机器人\n","permalink":"https://swimmingliu.cn/posts/diary/2023-moviekgqa/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e基于知识图谱和neo4j图数据库的电影知识问答系统\u003c/p\u003e\n\u003cdiv style=\"display:flex; justify-content: space-around; \"\u003e\r\n\u003cimg src=\"https://i.imgs.ovh/2023/12/12/mM4uR.png\" alt=\"image-20231212102658908\" style=\"box-shadow: 0 0 10px rgba(200, 200, 200);\" width=30% height:300px/\u003e\r\n\u003cimg src=\"https://i.imgs.ovh/2023/12/12/mM58p.png\" alt=\"image-20231212102738360\" style=\"box-shadow: 0 0 10px rgba(200, 200, 200);\" width=30% height:300px/\u003e\r\n\u003cimg src=\"https://i.imgs.ovh/2023/12/12/mMdFT.png\" alt=\"image-20231212103113278\" style=\"\" width=30% height:300px/\u003e\r\n\u003c/div\u003e\r\n\u003ch2 id=\"workflow\"\u003eWorkflow\u003c/h2\u003e\n\u003ch3 id=\"database\"\u003eDataBase\u003c/h3\u003e\n\u003cp\u003e爬取豆瓣TOP1000电影信息数据\u003c/p\u003e\n\u003ch3 id=\"frontend\"\u003eFrontend\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e获取用户输入的信息 （语音输入 / 文本输入）\u003c/li\u003e\n\u003cli\u003e向电影知识问答后端服务器发送请求\u003c/li\u003e\n\u003cli\u003e获取返回结果  (成功 -\u0026gt; 4 / 失败 -\u0026gt; 5)\u003c/li\u003e\n\u003cli\u003e如果返回结果包含image信息，则显示图片和文字，否则只显示文字\u003c/li\u003e\n\u003cli\u003e请求基于gpt的AI模型服务器，并显示返回结果\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"backend\"\u003eBackend\u003c/h3\u003e\n\u003cp\u003e​\t[准备工作]  训练 TF-IDF 向量算法和朴素贝叶斯分类器，用于预测用户文本所属的问题类别\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e接受前端请求，获取用户输入信息\u003c/li\u003e\n\u003cli\u003e使用分词库解析用户输入的文本词性，提取关键词\u003c/li\u003e\n\u003cli\u003e根据贝叶斯分类器，分类出用户文本的问题类型\u003c/li\u003e\n\u003cli\u003e结合关键词与问题类别，在 Neo4j 中查询问题的答案\u003c/li\u003e\n\u003cli\u003e返回查询结果 （若问题类型为 演员信息 / 电影介绍，则附加图片url）\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"workflow-graph\"\u003eWorkFlow Graph\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/0IEuW.png\" alt=\"workflow graph\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"frame\"\u003eFrame\u003c/h2\u003e\n\u003ch3 id=\"database-1\"\u003eDataBase\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://neo4j.com/\"\u003e\u003cimg loading=\"lazy\" src=\"https://img.shields.io/badge/neo4j-test?style=for-the-badge\u0026amp;logo=neo4j\u0026amp;logoColor=white\u0026amp;color=blue\" alt=\"Neo4j\"  /\u003e\r\n\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"frontend-1\"\u003eFrontend\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://developers.weixin.qq.com/\"\u003e\u003cimg loading=\"lazy\" src=\"https://img.shields.io/badge/wechat%20mini%20programs-test?style=for-the-badge\u0026amp;logo=wechat\u0026amp;logoColor=white\u0026amp;color=%2320B2AA\" alt=\"wechat mini programs\"  /\u003e\r\n\u003c/a\u003e\u003c/p\u003e","title":"MovieKGQA: 基于知识图谱和neo4j图数据库的电影知识问答系统"},{"content":"Abstract 在本文中，我们介绍了 U-Net v2，这是一种用于医学图像分割的新的稳健且高效的 U-Net 变体。它的目的是增强语义信息在低级特征中的注入，同时用更精细的细节来细化高级特征。对于输入图像，我们首先使用深度神经网络编码器提取多级特征。接下来，我们通过注入来自更高级别特征的语义信息并通过 Hadamard 乘积集成来自较低级别特征的更精细的细节来增强每个级别的特征图。我们新颖的跳跃连接赋予所有级别的功能以丰富的语义特征和复杂的细节。改进后的特征随后被传输到解码器以进行进一步处理和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。我们在几个公共医学图像分割数据集上评估了我们的方法，用于皮肤病变分割和息肉分割，实验结果证明了我们的新方法相对于最先进的方法的分割准确性，同时保留了内存和计算效率。代码位于：https://github.com/yaoppeng/U-Net_v2。\n主要工作就在于中间的skip-connection\nIntroduction 随着现代深度神经网络的进步，语义图像分割取得了重大进展。语义图像分割的典型范例涉及具有跳跃连接的编码器-解码器网络[1]。在此框架中，编码器从输入图像中提取层次和抽象特征，而解码器获取编码器生成的特征图并重建像素级分割掩模或图，为输入图像中的每个像素分配类标签。人们进行了一系列研究[2, 3]，将全局信息纳入特征图中并增强多尺度特征，从而大大提高了分割性能。 在医学图像分析领域，精确的图像分割在计算机辅助诊断和分析中起着至关重要的作用。 U-Net [4] 最初是为了医学图像分割而引入的，利用跳跃连接来连接每个级别的编码器和解码器阶段。跳跃连接使解码器能够访问早期编码器阶段的特征，从而保留高级语义信息和细粒度空间细节。这种方法有助于精确描绘对象边界并提取医学图像中的小结构。此外，还应用了密集连接机制，通过连接所有级别和所有阶段的特征来减少编码器和解码器中特征之间的差异[5]。设计了一种机制来通过连接较高和较低级别的不同尺度的特征来增强特征[6]。 然而，基于 U-Net 的模型中的这些连接在集成低级和高级特征方面可能不够有效。例如，在 ResNet [7] 中，深度神经网络是作为多个浅层网络的集合而形成的，并且显式添加的残差连接表明，即使在百万规模的训练中，网络也很难学习恒等映射函数图像数据集。\n对于编码器提取的特征，低级特征通常保留更多细节，但缺乏足够的语义信息，并且可能包含不需要的噪声。相反，高级特征包含更多语义信息，但由于分辨率显着降低而缺乏精确的细节（例如对象边界）。通过串联简单地融合特征将在很大程度上依赖于网络的学习能力，这通常与训练数据集的大小成正比。这是一个具有挑战性的问题，特别是在医学成像领域，通常受到有限数据的限制。这种信息融合是通过密集连接跨多个级别连接低级和高级特征来实现的，可能会限制来自不同级别的信息的贡献并可能引入噪声。另一方面，尽管引入的额外卷积并没有显着增加参数数量，但 GPU 内存消耗将会增加，因为必须存储所有中间特征图和相应的梯度以进行前向传递和后向梯度计算。这会导致 GPU 内存使用量和浮点运算 (FLOP) 增加。\n(a) U-Net v2 模型的整体架构，由编码器、SDI（语义和细节注入）模块和解码器组成。 (b) SDI模块的架构。为简单起见，我们仅显示第三级特征的细化（l = 3）。 SmoothConv 表示用于特征平滑的 3 × 3 卷积。$\\bigotimes$ 表示哈达玛积。\n在[8]中，利用反向注意力来明确地建立多尺度特征之间的联系。在[9]中，ReLU激活应用于较高级别的特征，并将激活的特征与较低级别的特征相乘。此外，在[10]中，作者提出分别从 CNN 和 Transformer 模型中提取特征，在多个级别上组合来自 CNN 和 Transformer 分支的特征来增强特征图。然而，这些方法都很复杂，而且它们的性能仍然不是很令人满意，因此需要进一步改进。\n在本文中，我们提出了 U-Net v2，这是一种基于 U-Net 的新分割框架，具有简单且高效的跳跃连接。我们的模型首先使用 CNN 或 Transformer 编码器提取多级特征图。接下来，对于第 i 层的特征图，我们通过简单的哈达玛乘积操作显式地注入高层特征（包含更多语义信息）和低层特征（捕获更精细的细节），从而增强语义和细节第 i 级特征。随后，细化的特征被传输到解码器进行分辨率重建和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。\n我们使用公开的数据集在两个医学图像分割任务（皮肤病变分割和息肉分割）上评估我们的新方法。实验结果表明，我们的 U-Net v2 在这些分割任务中始终优于最先进的方法，同时保持 FLOP 和 GPU 内存效率。\nMethod 2.1 Overall Architecture 我们的 U-Net v2 的整体架构如图 1（a）所示。它包括三个主要模块：编码器、SDI（语义和细节注入）模块和解码器。给定输入图像 I，其中 I ∈ $R^{H×W×C}$ ，编码器产生 M 个级别的特征。我们将第 i 级特征表示为$ f^0_i$ , 1 ≤ i ≤ M。这些收集到的特征，{$ f^0_1$ ,$ f^0_2$,… , $ f^0_M$}，然后传输到 SDI 模块进行进一步细化。\n2.2 Semantics and Detail Infusion (SDI) Module 利用编码器生成的分层特征图，我们首先将空间和通道注意机制[11]应用于每个级别 i 的特征 $ f^0_i$。此过程使特征能够集成局部空间信息和全局通道信息，如下所示：\n其中$f^1_i$表示第 i 层处理后的特征图，$φ^s_i$ 和 $\\phi^c_i$ 分别表示第 i 层空间注意力和通道注意力的参数。此外，我们应用 1 × 1 卷积将$f^1_i$的通道减少到 c，其中 c 是超参数。得到的特征图表示为$f^2_i$，其中 $f^2_i$ ∈ $R^{H_i × W_i × c}$ ，其中$H_i$、$W_i$ 和 c 分别表示 $f^2_i$ 的宽度、高度和通道。\n接下来，我们需要将精炼后的特征图发送到解码器。在每个解码器级别 i，我们使用 $f^2_i$ 作为目标参考。然后，我们调整每个第 j 层的特征图的大小，以匹配与 $f^2_i$ 相同的分辨率，公式为：\n其中 D 、 I 和 U 分别表示自适应平均池化、恒等映射和双线性插值 $f^2_i$ 到 $H_i$、$W_i$ 的分辨率，其中 1 ≤ i，j ≤ M。\n然后，应用 3 × 3 卷积来平滑每个调整大小的特征图 $f^3_{ij}$ ，公式为：\n其中$θ_{ij}$表示平滑卷积的参数， $f^4_{ij}$ 是第i层的第j个平滑特征图。\n将所有第 i 级特征图调整为相同的分辨率后，我们将元素级哈达玛积应用于所有调整大小的特征图，以通过更多语义信息和更精细的细节来增强第 i 级特征，如下所示：\n其中$H(·)$表示哈达玛积 (见图1(b))。然后，$f^5_i$ 被分派到第i级解码器以进行进一步的分辨率重建和分割。\nExperiments 3.1 Datasets 我们使用以下数据集评估新的 U-Net v2。 ISIC 数据集：使用两个皮肤病变分割数据集：ISIC 2017 [15, 16]，其中包含 2050 个皮肤镜图像，ISIC 2018 [15]，其中包含 2694 个皮肤镜图像。为了公平比较，我们遵循[13]中概述的训练/测试分割策略。 息肉分割数据集：使用五个数据集：Kvasir-SEG [17]、ClinicDB [18]、ColonDB [19]、Endoscene [20] 和 ETIS [21]。为了公平比较，我们使用[8]中的训练/测试分割策略。具体来说，使用来自 ClinicDB 的 900 张图像和来自 Kvasir-SEG 的 548 张图像作为训练集，其余图像作为测试集。\n3.2 Experimental Setup 我们使用 PyTorch 在 NVIDIA P100 GPU 上进行实验。我们的网络使用 Adam 优化器进行优化，初始学习率 = 0.001，β1 = 0.9，β2 = 0.999。\n我们采用幂为 0.9 的多项式学习率衰减。训练时期的最大数量设置为 300。超参数 c 设置为 32。按照[13]中的方法，我们报告 ISIC 数据集的 DSC（骰子相似系数）和 IoU（并集交集）分数。对于息肉数据集，我们报告 DSC、IoU 和 MAE（平均绝对误差）分数。每个实验运行 5 次，并报告平均结果。我们使用金字塔视觉变换器（PVT）[22]作为特征提取的编码器。\n3.3 Results and Analysis 表 1 列出了 ISIC 数据集上最先进方法的比较结果。如图所示，我们提出的 UNet v2 将 DSC 分数提高了 1.44% 和 2.48%，IoU 分数提高了 2.36% 和 3.90%。分别是 ISIC 2017 和 ISIC 2018 数据集。这些改进证明了我们提出的将语义信息和更精细的细节注入每个特征图的方法的有效性。\n表 2 列出了息肉分割数据集上最先进方法的比较结果。如图所示，我们提出的 U-Net v2 在 Kavasir-SEG、ClinicDB、ColonDB 和 ETIS 上优于 Poly-PVT [14]数据集，DSC 得分分别提高了 1.1%、0.7%、0.4% 和 0.3%。这强调了我们提出的方法在将语义信息和更精细的细节注入每个级别的特征图中的一致有效性。\n3.4 Ablation Study 我们使用 ISIC 2017 和 ColonDB 数据集进行消融研究，以检查 U-Net v2 的有效性，如表 3 所示。具体来说，我们使用 PVT [22] 模型作为 UNet++ [5] 的编码器。请注意，当我们的 SDI 模块被移除时，U-Net v2 将恢复为具有 PVT 主干的普通 U-Net。 **SC 表示 SDI 模块内的空间和通道关注点。**从表 3 可以看出，与不带 SDI 的 U-Net v2（即带 PVT 编码器的 U-Net）相比，UNet++ 的性能略有下降。\n这种下降可能归因于密集连接生成的多级特征的简单串联，这可能会混淆模型并引入噪声。表 3 表明 SDI 模块对整体性能贡献最大，突出显示我们提出的跳跃连接（即 SDI）持续带来性能改进。\n3.5 Qualitative Results 图 2 给出了 ISIC 2017 数据集的一些定性示例，这表明我们的 U-Net v2 能够将语义信息和更精细的细节合并到每个级别的特征图中。因此，我们的分割模型可以捕获对象边界的更精细的细节。\n3.6 Computation, GPU Memory, and Inference Time 为了检查 U-Net v2 的计算复杂性、GPU 内存使用情况和推理时间，我们报告了我们的方法 U-Net [4] 的参数、GPU 内存使用情况、FLOP 和 FPS（每秒帧数），以及表 4 中的 UNet++ [5]。实验使用 float32 作为数据类型，这导致每个变量使用 4B 的内存。 GPU内存使用记录了前向/后向传递过程中存储的参数和中间变量的大小。\n(1, 3, 256, 256) 表示输入图像的大小。所有测试均在 NVIDIA P100 GPU 上进行。\n从表4中可以看出，UNet++引入了更多的参数，并且由于在密集前向过程中存储中间变量（例如特征图），其GPU内存使用量更大。通常，此类中间变量比参数消耗更多的 GPU 内存。\n此外，U-Net v2 的 FLOPs 和 FPS 也优于 UNet++。与 U-Net (PVT) 相比，我们的 U-Net v2 的 FPS 降低是有限的。\nConclusion 引入了新的 U-Net 变体 U-Net v2，它采用新颖且简单的跳跃连接设计，以改进医学图像分割。该设计明确地将来自较高级别特征的语义信息和来自较低级别特征的更精细细节集成到编码器使用 Hadamard 乘积生成的每个级别的特征映射中。在皮肤病变和息肉分割数据集上进行的实验验证了我们的 UNet v2 的有效性。复杂性分析表明 U-Net v2 在 FLOP 和 GPU 内存使用方面也很高效\n这篇文章比较简单，整体的行文风格一看就是会议论文。核心创新点就一个SDI（Semantic and Detail Infusion）模块。SDI模块作用就是 连接高级特征的语义信息和低级特征的细节信息。首先通过通道和特征注意力机制，分别关注不同级别的通道和空间信息。然后将所有的通道都 padding / scaling 到和第i级别相同的通道数，然后通过双线性插值 / 自适应平均池化到相同大小的尺寸。最后，使用哈达码乘积进行特征融合。 对，没错就这么简单！！！\n（思考: 能不能用减法单元来融合差异性？？？）\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-unet_v2/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在本文中，我们介绍了 U-Net v2，这是一种用于医学图像分割的新的稳健且高效的 U-Net 变体。它的目的是\u003cstrong\u003e增强语义信息在低级特征中的注入\u003c/strong\u003e，同时\u003cstrong\u003e用更精细的细节来细化高级特征\u003c/strong\u003e。对于输入图像，我们首先使用\u003cstrong\u003e深度神经网络编码器提取多级特征\u003c/strong\u003e。接下来，我们通过\u003cstrong\u003e注入来自更高级别特征的语义信息\u003c/strong\u003e并通过 \u003cstrong\u003eHadamard 乘积\u003c/strong\u003e集成来自\u003cstrong\u003e较低级别特征的更精细的细节\u003c/strong\u003e来\u003cstrong\u003e增强每个级别的特征图\u003c/strong\u003e。我们新颖的\u003cstrong\u003e跳跃连接\u003c/strong\u003e赋予\u003cstrong\u003e所有级别的功能\u003c/strong\u003e以\u003cstrong\u003e丰富的语义特征和复杂的细节\u003c/strong\u003e。\u003cstrong\u003e改进后的特征随后被传输到解码器\u003c/strong\u003e以进行\u003cstrong\u003e进一步处理和分割\u003c/strong\u003e。我们的方法可以\u003cstrong\u003e无缝集成到任何编码器-解码器网络中\u003c/strong\u003e。我们在几个公共医学图像分割数据集上评估了我们的方法，用于皮肤病变分割和息肉分割，实验结果证明了我们的新方法相对于最先进的方法的分割准确性，同时\u003cstrong\u003e保留了内存和计算效率\u003c/strong\u003e。代码位于：https://github.com/yaoppeng/U-Net_v2。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e主要工作就在于中间的skip-connection\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e随着现代深度神经网络的进步，语义图像分割取得了重大进展。语义图像分割的典型范例涉及具有\u003cstrong\u003e跳跃连接的编码器-解码器网络[\u003cstrong\u003e1]。在此框架中，编码器从\u003c/strong\u003e输入图像中提取层次和抽象特征\u003c/strong\u003e，而解码器获取\u003cstrong\u003e编码器生成的特征图并重建像素级分割掩模或图\u003c/strong\u003e，\u003cstrong\u003e为输入图像中的每个像素分配类标签\u003c/strong\u003e。人们进行了一系列研究[2, 3]，\u003cstrong\u003e将全局信息纳入特征图\u003c/strong\u003e中并增强多尺度特征，从而大大提高了分割性能。\n在医学图像分析领域，\u003cstrong\u003e精确的图像分割\u003c/strong\u003e在计算机辅助诊断和分析中起着至关重要的作用。 U-Net [4] 最初是为了\u003cstrong\u003e医学图像分割\u003c/strong\u003e而引入的，利用\u003cstrong\u003e跳跃连接\u003c/strong\u003e来连接每个级别的\u003cstrong\u003e编码器和解码器阶段\u003c/strong\u003e。\u003cstrong\u003e跳跃连接\u003c/strong\u003e使解码器能够访问\u003cstrong\u003e早期编码器阶段\u003c/strong\u003e的特征，从而保留\u003cstrong\u003e高级语义信息\u003c/strong\u003e和\u003cstrong\u003e细粒度空间细节\u003c/strong\u003e。这种方法有助于\u003cstrong\u003e精确描绘对象边界\u003c/strong\u003e并提取\u003cstrong\u003e医学图像中的小结构\u003c/strong\u003e。此外，还应用了\u003cstrong\u003e密集连接机制\u003c/strong\u003e，通过\u003cstrong\u003e连接所有级别\u003c/strong\u003e和\u003cstrong\u003e所有阶段的特征\u003c/strong\u003e来减少\u003cstrong\u003e编码器和解码器中特征之间的差异\u003c/strong\u003e[5]。设计了一种机制来\u003cstrong\u003e通过连接较高和较低级别\u003c/strong\u003e的\u003cstrong\u003e不同尺度的特征\u003c/strong\u003e来\u003cstrong\u003e增强特征\u003c/strong\u003e[6]。\n然而，基于 U-Net 的模型中的这些连接在\u003cstrong\u003e集成低级和高级特征方面\u003c/strong\u003e可能\u003cstrong\u003e不够有效\u003c/strong\u003e。例如，在 ResNet [7] 中，深度神经网络是作为\u003cstrong\u003e多个浅层网络的集合\u003c/strong\u003e而形成的，并且\u003cstrong\u003e显式添加的残差连接\u003c/strong\u003e表明，即使在百万规模的训练中，网络也很难学习\u003cstrong\u003e恒等映射函数图像\u003c/strong\u003e数据集。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e对于编码器提取的特征，低级特征通常保留更多细节，但缺乏足够的语义信息，并且可能包含不需要的噪声\u003c/strong\u003e。相反，\u003cstrong\u003e高级特征包含更多语义信息，但由于分辨率显着降低而缺乏精确的细节（例如对象边界）\u003c/strong\u003e。通过\u003cstrong\u003e串联简单地融合特征\u003c/strong\u003e将在\u003cstrong\u003e很大程度上依赖于网络的学习能力\u003c/strong\u003e，这\u003cstrong\u003e通常与训练数据集的大小成正比\u003c/strong\u003e。这是一个具有挑战性的问题，特别是在医学成像领域，\u003cstrong\u003e通常受到有限数据的限制\u003c/strong\u003e。这种信息融合是\u003cstrong\u003e通过密集连接跨多个级别连接低级和高级特征\u003c/strong\u003e来实现的，可能会限制来自\u003cstrong\u003e不同级别的信息的贡献\u003c/strong\u003e并可能引入噪声。另一方面，尽管\u003cstrong\u003e引入的额外卷积并没有显着增加参数数量\u003c/strong\u003e，但 \u003cstrong\u003eGPU 内存消耗将会增加\u003c/strong\u003e，因为必须\u003cstrong\u003e存储所有中间特征图和相应的梯度\u003c/strong\u003e以进行前向传递和后向梯度计算。这会导致 \u003cstrong\u003eGPU 内存使用量\u003c/strong\u003e和\u003cstrong\u003e浮点运算 (FLOP) 增加\u003c/strong\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/mCdvu.png\" alt=\"image-20231211193109745\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003e(a) U-Net v2 模型的整体架构，由\u003cstrong\u003e编码器、SDI（语义和细节注入）模块和解码器\u003c/strong\u003e组成。 (b) SDI模块的架构。为简单起见，我们仅显示第三级特征的细化（l = 3）。 \u003cstrong\u003eSmoothConv 表示用于特征平滑的 3 × 3 卷积\u003c/strong\u003e。$\\bigotimes$ 表示哈达玛积。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在[8]中，利用\u003cstrong\u003e反向注意力\u003c/strong\u003e来明确地建立\u003cstrong\u003e多尺度特征之间\u003c/strong\u003e的联系。在[9]中，ReLU激活应用于\u003cstrong\u003e较高级别\u003c/strong\u003e的特征，并\u003cstrong\u003e将激活的特征与较低级别的特征相乘\u003c/strong\u003e。此外，在[10]中，作者提出分别从 \u003cstrong\u003eCNN 和 Transformer 模型\u003c/strong\u003e中提取特征，在多个级别上组合\u003cstrong\u003e来自 CNN 和 Transformer 分支\u003c/strong\u003e的特征来\u003cstrong\u003e增强特征图\u003c/strong\u003e。然而，这些方法\u003cstrong\u003e都很复杂\u003c/strong\u003e，而且它们的\u003cstrong\u003e性能仍然不是很令人满意\u003c/strong\u003e，因此需要进一步改进。\u003c/p\u003e\n\u003cp\u003e在本文中，我们提出了 U-Net v2，这是一种基于 U-Net 的新分割框架，具有\u003cstrong\u003e简单且高效的跳跃连接\u003c/strong\u003e。我们的模型首先\u003cstrong\u003e使用 CNN 或 Transformer 编码器\u003c/strong\u003e提取\u003cstrong\u003e多级特征图\u003c/strong\u003e。接下来，\u003cstrong\u003e对于第 i 层的特征图\u003c/strong\u003e，我们通过\u003cstrong\u003e简单的哈达玛乘积操作\u003c/strong\u003e显式地注入\u003cstrong\u003e高层特征（包含更多语义信息）\u003cstrong\u003e和\u003c/strong\u003e低层特征（捕获更精细的细节）\u003c/strong\u003e，从而\u003cstrong\u003e增强语义和细节第 i 级特征\u003c/strong\u003e。随后，\u003cstrong\u003e细化的特征\u003c/strong\u003e被传输到解码器进行\u003cstrong\u003e分辨率重建和分割\u003c/strong\u003e。我们的方法可以无缝集成到任何编码器-解码器网络中。\u003c/p\u003e\n\u003cp\u003e我们使用公开的数据集在两个医学图像分割任务（皮肤病变分割和息肉分割）上评估我们的新方法。实验结果表明，我们的 U-Net v2 在这些分割任务中始终优于最先进的方法，\u003cstrong\u003e同时保持 FLOP 和 GPU 内存效率\u003c/strong\u003e。\u003c/p\u003e","title":"U-NET V2: RETHINKING THE SKIP CONNECTIONS OF U-NET FOR MEDICAL IMAGE SEGMENTATION"},{"content":"Abstract 放射科医生拥有不同的培训和临床经验，导致肺结节的分割注释存在差异，从而导致分割的不确定性。传统方法通常选择单个注释作为学习目标或尝试学习包含多个注释的潜在空间。\n然而，这些方法无法利用多个注释之间的共识和分歧所固有的有价值的信息。在本文中，我们提出了一种不确定性感知注意机制（UAAM），它利用多个注释之间的共识和分歧来促进更好的分割。为此，我们引入了多置信度掩模（MCM），它结合了低置信度（LC）掩模和高置信度（HC）掩模。 LC 掩模表示分割置信度较低的区域，放射科医生可能有不同的分割选择。继UAAM之后，我们进一步设计了一个不确定性引导多置信分割网络（UGMCS-Net），它包含三个模块：一个捕获肺结节一般特征的特征提取模块，一个为肺结节产生三个特征的不确定性感知模块。注释的并集、交集和注释集，以及一个交集并集约束模块，该模块使用三个特征之间的距离来平衡最终分割和 MCM 的预测。为了全面展示我们方法的性能，我们提出了 LIDC-IDRI 上的复杂结节验证，它测试了 UGMCS-Net 对使用常规方法难以分割的肺结节的分割性能。实验结果表明，我们的方法可以显着提高传统方法难以分割的结节的分割性能。\nINTRODUCTION 肺结节分割在肺癌计算机辅助诊断 (CAD) 系统中至关重要 [1]，可提供结节大小、形状和其他重要医学特征等关键信息。然而，对于深度学习方法的一般训练和测试范例，每个结节图像数据只有一个由一名放射科医生描绘的注释掩模[2]-[6]。因此，网络每次只能提供结节区域的单个预测。\n然而，在临床实践中，不同的放射科医生由于其不同的培训和临床经验可能会为肺结节提供不同的分割注释[7]-[9]。\n因此，基于单一注释的传统方法无法反映临床经验的多样性，限制了深度学习方法的应用。\n解决放射科医生之间注释不同问题的一个直接解决方案是为每个肺结节图像合并多个注释。这导致了另一个问题：多个注释不可避免地会带来不确定性和冲突，因为放射科医生可能会对同一区域进行不同的注释。为了克服这个问题，Kohl 等人在 2018 年提出了一种概率 U-Net，它利用条件变分自动编码器将多个分割变体编码到低维潜在空间中 [8]、[10]。通过从该空间采样，网络可以影响相应的分割图。基于这项研究，Hu等人提出将真实不确定性与概率UNet相结合，这可以提高预测不确定性估计、样本准确性和样本多样性[7]。这些方法依赖于潜在空间和该空间中的随机样本。因此，这些方法只能通过多次预测来提供不确定区域。\n在本文中，我们提出了一个论点，即多个注释之间的不确定性遵循特定的模式。\n为了演示这种现象，我们引入了多重置信掩码 (MCM)，它结合了高置信度 (HC) 掩码和低置信度 (LC) 掩码，如图 1 所示。 A. 交叉掩码等于 HC mask，代表所有注释的交集。\n联合掩码是所有注释的联合。 LC掩模是交集掩模和并集掩模之间的差异。当在 LIDC-IDRI 数据集 [11] 上计算 HC 和 LC 的 Hounsfield 单位 (HU) 核估计时，如图 1.B 所示，我们可以观察到 LC 和 HC 掩模之间的 HU 分布存在明显区别。具体地，LC区域具有比HC区域更低的HU值。从像素分布来看，HU值越低，对应区域的密度越低。就CT图像特征而言，LC区域主要由结节边缘、毛刺和磨玻璃特征等边界相关特征组成，而HC区域主要分布在结节核心内。因此，我们提出了这样的假设：导致放射科医生之间差异的区域主要与低密度组织和边界相关特征有关。\n与其他方法不同，我们建议利用 MCM (多重置信掩码) ** 和注释集作为具有不同分割确定性的特征的学习指导**，有助于更好的分割性能。我们将这种训练称为UncertaintyAware Attention Mechanism，如图2所示。按照这种机制，我们进一步设计了用于肺结节分割的Uncertainty-Guide Multi-Confidence Segmentation Network（UGMCS-Net）。\nUGMCS-Net 包含三个模块：基于 U-Net 的特征提取模块、不确定性感知模块和交集并集约束模块。\n首先，标签分为交集标签( $L_\\cap$ )、并集标签 ( $L_\\cup$ )、原始标签 (L)。\n其次，HC (consensus) 表示 $L_\\cap$ 、 LC（disagreement）表示 $L_\\cup$ - $L_\\cap$。\nMCM (Multi Confidence Mask) 表示 HC 和 LC的统称\n首先，特征提取模块从输入的CT图像中提取通用特征图R。其次，不确定性感知模块在标签的交集、并集、和原标签的指导下，将通用特征图R转换为三个独立的特征图$R_{LC}$、$R_{HC}$和$R_{Uni}$。 $R_{LC}$、$R_{HC}$用于预测并集掩码和交集掩码，并将结果组合为MCM。$R_{Uni}$用于预测初步分割结果。我们稍后使用 $\\cup(X)$, $\\cap(X)$, $X_{Uni}$ 来表示预测的并集掩码、交集掩码和初步分割结果。第三，约束模块使用来自$R_{LC}$、$R_{HC}$ 和 $R_{Uni}$的特征感知注意块捕获首选特征，然后用特征距离约束最终预测$X_S$，确保分割结果以合理的方式受到约束。为了更好地利用多个注释，我们还引入了多注释融合损失来优化 $X_{Uni}$和 $X_S$，它计算预测和所有注释之间的平均 BCE 损失。\n该方法具有两个明显的优点： （1）与学习潜在空间的传统基于 VAE 的方法相比，该方法具有特定的学习目标，使其能够提供不确定结节区域的稳定预测。 （2）该方法利用所有注释来优化预测，以确保最终预测平衡不同条件，充分利用可用信息。\n我们在之前的出版物中报告了这项工作的初步版本[12]。本文的新贡献可概括如下: （1）一种称为不确定性感知注意机制（UAAM）的新颖机制：UAAM 最大限度地利用多个注释，并采用多重置信掩码（MCM）来指导低置信度和高置信度特征的学习。\n（2）升级后的Uncertainty-Guide Multi-Confidence Segmentation Network (UGMCS-Net)：基于该机制，我们将UGS-Net更新为UGMCSNet，其中包含特征提取模块、不确定性感知模块和新的交并集约束模块。为了充分利用多个注释，我们还引入了多注释融合损失。所提出的模块是即插即用的，可以应用于不同情况下的其他分割网络。\n（3）全面验证：我们提出了ComplexNodule Validation，测试UGMCS-Net对U-Net难以分割的肺结节的分割性能。实验表明，对于UNet上DSC分数低于60％的结节，我们网络的DSC分数可以提高11.03％，我们网络的IoU分数可以提高11.88％。我们还为不同的模块、主干和模型设置提供足够的消融研究。\nRelated Work 2.1 Lung Nodule Segmentation 肺结节分割对于肺结节计算机辅助检测 (CAD) 系统至关重要。其主要目标是准确地描绘目标结节的边界，以提供其直径、大小和语义特征等细节[13]-[16]。这项任务的主要挑战是肺结节具有各种形状、大小和微妙的特征。早年，研究人员提供了多种肺结节分割方法，例如基于形态学的方法和基于区域生长的方法[17]，[18]。近年来，深度学习已成为该领域最流行的方法。\n2017年，Wang等人提出了一种用于肺结节分割的多视图卷积网络。所提出的网络同时从 CT 图像的轴向、冠状和矢状视图中捕获了一组不同的结节敏感特征。使用多分支 CNN 网络对这些特征进行分析，平均 DSC 相似系数 (DSC) 为 77.67% [19]。此外，Wang 等人在 2017 年提出了一种具有中心池层的中心聚焦卷积神经网络，可以彻底分析 2D 和 3D 结节 [1]。 2020年，Cao等人设计了带有强度池层的双分支残差网络，增强了强度信息的学习，并将DSC提高到82.74％[20]。 2021年，Pezzano等人推出了一种CNN网络，可以通过生成两个代表CT中所有背景和次要重要元素的掩模来学习结节的背景，从而使网络可以更好地区分结节特征[15]。后来在2022年，Shariaty等人进一步提出了纹理特征提取和特征选择算法来改进分割，实现了84.75%的DSC[2]。\n根据上述研究的观察结果，显然现有方法主要优先考虑实现更精确的分割，而忽略了不同放射科医生对如何分割同一肺结节可能持有不同意见的事实。在这项研究中，我们认为注释之间的分歧也具有诊断价值。因此，我们的方法旨在生成一个分割，通过从注释集学习并识别具有不同分割确定性的区域来有效地平衡所有注释。\n2.2 Uncertainty in Lung Nodule Segmentation 许多医学图像视觉问题都存在模糊性。在临床情况下，仅通过 CT 扫描可能无法明确哪个特定区域是癌组织 [10]、[21]。因此，即使是经验丰富的医生和放射科医生也可能对相同的组织或肿瘤提供不同的分割。\n2018 年，Kohl 等人提出将此任务建模为学习肺结节多样化但合理的分割上的分布。基于 U-Net [5]，他们引入了概率 U-Net，它是 UNet 和条件 VAE 的组合，可以产生无限数量的合理分割。 2019年晚些时候，Kohl等人进一步提出了一种分层概率U-Net，它使用分层潜在空间分解来制定高保真度分割的采样和重建[8]。同样在 2019 年，Hu 等人分析了两种类型的不确定性：任意的和认知的 [7]。他们利用多个注释的可变性作为“ground truth”任意不确定性的来源，将这种不确定性与概率 UNet 结合起来，并尝试定量分析分割不确定性。 2021年，Long等人将[7]中的概念扩展到V-Net和3D肺结节CT图像。作为包含 1000 多个肺结节的多个注释的理想数据集，所有这些研究 [7]-[10] 都分析了 LIDC-IDRI。\n与基于VAE的网络不同，我们的工作更关注导致各种标注的原因，表现为分割分歧。我们引入了一种专门针对不确定性区域的替代方法，使我们能够对不确定的结节区域和整体肺结节分割做出稳定的预测。这种方法使我们能够深入了解分割差异的根本原因，并在不确定的肺结节区域中产生更可靠的结果。\n在医学图像处理中，\u0026ldquo;条件变分自编码器（Conditional Variational Autoencoder, CVAE）\u0026ldquo;是一种生成模型，它结合了变分自编码器（VAE）的特性和条件生成的能力。VAE是一种深度学习模型，能够学习输入数据的潜在表示，然后从这些表示中生成新的数据实例。CVAE在此基础上增加了条件变量，使得生成的过程可以依赖于某些条件或标签。\n对于医学图像，CVAE可以用于多种任务，如生成特定类型的医学图像（例如，根据特定疾病状态生成CT或MRI图像），数据增强（生成新的训练样本），以及特征提取和表示学习等。通过将条件信息（如疾病标签、图像类型或患者信息）融入到生成过程中，CVAE能够生成更符合特定条件的图像，从而在特定医学应用中发挥作用。\nMethod 3.1 Uncertainty-Guided Multi-Confidence Segmentation Network 在图 3 中，我们展示了 UncertaintyGuided Multi-Confidence Segmentation Network (UGMCSNet) 的架构。该网络以肺结节 CT 图像作为输入，并产生两个输出：预测的多置信度掩模（MCM）和最终的分割 $X_S$。 MCM 结合了预测的并集 $\\cup(X)$ 和交集 $\\cap(X)$。网络的学习目标是注释集 GT，以及它们的 Union Mask $\\cup(GT)$ 和 Intersection Mask $\\cap(GT)$。输入图像及其相应的掩模的尺寸为 50 × 50 像素，通过从带有官方注释的 LIDC-IDRI 数据集裁剪获得。在输入网络之前，输入图像和掩模的大小被调整为 3 × 64 × 64 像素的尺寸。\nUGMCS-Net 包含三个模块：(1) 特征提取模块，(2) 不确定性感知模块，(3) 交并并约束模块。特征提取模块可以使用任何基于UNet结构的分割网络，初步获得形状为32×64×64的特征图R。本文使用具有五个下采样和上采样层的Attention U-Net [4] 。每个上采样层由两个卷积层和一个注意力块组成。不确定性感知模块分析 R 并生成$R_{LC}$、$R_{HC}$和$R_{Uni}$。然后将这些特征图输入 MCM BCE Loss Block 和 Multiple Annotation Loss Block，生成初始的 $\\cup(X)$、$\\cap(X)$ 和合理的分割 $X_{Uni}$。计算并集以 $\\cup(X)$、$\\cap(X)$获得 MCM。 Intersection-Union Constraining Module 学习 $R_{LC}$、$R_{HC}$和$R_{Uni}$的不同特征，并将这三个特征融合到$R_{final}$ 中。然后该模块通过分析RF ianl提供更合理的最终分割$X_S$。\n3.2 Uncertainty-Aware Module 引入不确定性感知模块（UAM），通过学习$\\cup(GT)$、$\\cap(GT)$ 和 GT来充分合理地利用所有注释信息。该模块有两个任务：（1）从低置信度（LC）区域、高置信度（HC）区域和所有注释中捕获不同的特征； (2) 生成多重置信掩模 (MCM) 的初始预测和一般分割。\n如图3所示，UAM采用三分支CNN网络作为骨干。它以 R (32 × 64 × 64) 作为输入，并使用内核大小为 1×1 的三个不同卷积层提取 $R_{LC}$、$R_{HC}$和$R_{Uni}$。 $R_{LC}$、$R_{HC}$和$R_{Uni}$的大小相同，均为 32 × 64 × 64。 MCM BCE Loss Block 接收 $R_{LC}$、$R_{HC}$ ，用三个不同的卷积层生成 $\\cup(X)$ 和 $\\cap(X)$ ，内核大小为 3× 3. BCE损失计算$\\cup(X)$ 和 $\\cup(GT)$的损失以及$\\cap(X)$ 和 $\\cap(GT)$。 $\\cup(X)$和$\\cap(X)$通过归一化操作Normal($\\cup(X)$+$\\cap(X)$)组合为MCM\u0026rsquo;，反映了不同区域的不确定性程度。与我们之前的工作 [12] 不同，$R_{Uni}$ 的分支是通过多重注释损失块进行优化的，这将在稍后讨论。此外，具有相同形状的 1 × 64 × 64 的特征图 $R_{LC}$、$R_{HC}$和$R_{Uni}$ 将被输入到下一个模块中进行进一步分析。\n主要还是用来生成一个MCM\n3.3 Intersection-Union Constraining Module 如上所述，$\\cup(GT)$和$\\cap(GT)$ 是UAM的学习目标。具体地，$\\cup(GT)$表示所有可能是结节组织的区域，表示置$\\cap(GT)$信度最高的结节区域。为了在极端情况之间实现平衡，我们进一步开发了一个新模块，称为交集并集约束模块（IUCM）。\n该模块旨在捕获所有三个学习目标的特征，并产生更合理的分割预测，可以在极端情况之间取得平衡。\n如图 4 所示，IUCM 将 $R_{LC}$、$R_{HC}$和$R_{Uni}$作为输入，并将对应的 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$与特征感知注意块 (FAAB) 对应。 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$的尺寸相同，均为 32 × 32 × 32。FAAB 是基于自注意力块 [22] 和特征感知滤波器构建的。\n这些注意力块使用不同的特征感知滤波器处理$R_{LC}$、$R_{HC}$和$R_{Uni}$，使网络能够针对不同的学习目标制定不同的学习偏好，并获得更多有助于分割的图像特征[23]、[24]。更具体地说，假设输入$R_z$，FAAB的过程可以总结为：\n其中 z ∈ {Uni, LC, HC},A表示自注意力架构。 Г是一个特征感知滤波器，在本研究中，$R_{Uni}$和$R_{LC}$的Г是Gabor[25]，$R_{HC}$的Г是Otsu[26]。 Γ(A($R_z$)) 与$R_z$逐像素相加，以便网络可以从输入中保留更多信息。\n通过对数据集和Hounsfield Unit Kernel Estimations的观察，我们可以看到，$R_{HC}$主要是密度较高的实性结节，而$R_{LC}$则包括更多的低密度组织（如毛刺），主要分布在结节的边缘。 Otsu对密度特征敏感，可以帮助网络更准确地识别高密度组织。因此，我们应用 Otsu 从 $R_{HC}$ 中提取 $R_{HC}^{\\prime}$。同时，Gabor对图像边缘敏感，能够提供良好的方向选择和尺度选择特征，从而能够捕获图像局部区域多个方向的局部结构特征。因此，我们选择Gabor从$R_{LC}$和$R_{Uni}$中提取$R_{LC}^{\\prime}$和$R_{Uni}^{\\prime}$。关于过滤器选择的消融研究将在第四节中提供。\n得到 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$后，IUCM 得到 $S_z$ = d{$R_Z$,$R$},d是计算余弦相似度的运算。\nIUCM 的输出为 $R_{Aug}$ = Concat($S_{Uni}$× $R_{Uni}^{\\prime}$ ; $S_{LC}$× $R_{LC}^{\\prime}$; $S_{HC}$× $R_{HC}^{\\prime}$ )。$R_{Aug}$将与来自特征提取模块的 R 连接，输入到卷积层，并生成最终的分割预测 $X_S$。 R 和 $R_{Aug}$ 的串联保留了来自 CT 输入的更多信息。\n3.4 Loss Function 在图 3 中，UGMCS-Net 包含两个优化：MCM BCE 损失块和多注释损失块。\nMCM BCE Loss Block 计算$\\cup(X)$ 和 $\\cup(GT)$之间; $\\cap(X)$ 和$\\cap{(GT)}$之间的 BCE 损失可表示为：\n我们使用多注释融合损失来优化多注释损失块中的$X_{Uni}$和$X_S$，表示为Φ。在我们之前的工作中，只选择了一组注释来优化$X_{Uni}$和$X_S$，这导致其他注释中有价值的信息丢失。\n本研究引入了多重注释融合损失，它将预测与所有可能的注释进行比较。\n首先，$R_{Uni}$和$R_F$最终产生$X_{Uni}$和$X_S$。其次，如图 5 所示，多注释融合损失函数计算优化对象（$X_{Uni}$和$X_S$）与注释集之间的 BCE 损失，并合并这些损失的平均值。根据我们的设计，$X_{Uni}$应该倾向于整个注释集，XS应该从注释集中学习足够的信息并产生平衡所有注释的分割，因此我们选择使用多注释融合损失来优化$X_{Uni}$和$X_S$。我们有：\n网络的损失融合可以定义为：\n其中 α1、α2 和 α3 是预定义参数。根据经验，本文中α1设置为0.5，α2设置为0.5，α3设置为1。重量选择的消融研究将在第四节中显示。\nEXPERIMENT 4.1 Dataset and Experimental Settings 在本研究中，我们使用 LIDC-IDRI 数据集 [11] 评估所提出的网络，该数据集由 1018 个研究实例和超过 2600 个结节组成。在本研究中，我们选择了 1860 个直径为 3-30mm 且具有多个注释的肺结节。每个结节数据至少有两个注释，以便我们可以获得其CT图像、多个注释集GT、它们的并集$\\cup(GT)$和交集$\\cap(GT)$。输入图像及其掩模均为 50 × 50 像素，根据官方注释从 LIDC-IDRI 数据集中裁剪。每个像素反映 CT 图像的亨斯菲尔德单位。\n在训练之前，我们将 CT 图像的强度值裁剪到范围 [1000,1000] 并将所有强度值归一化到范围 [0, 1]。数据处理的代码在网站 https://github.com/qiuliwang/LIDCIDRI-Toolbox-python 上提供。\n我们在运行 Ubuntu 18.04、Tesla V100 GPU 的服务器上进行实验，使用 CUDA 11.2，GPU 内存约为 16G。该网络使用PyTorch-v1.0.1和Python3.7实现。我们使用五重验证来评估网络的有效性，确保数据分割的稳健性。我们使用热重启随机梯度下降（SGDR）作为优化器，初始学习率（LR）为0.00001，批量大小为32，动量为0.9，权重衰减率为0.0001。每个网络都训练 200 个 epoch，学习率每 50 个 epoch 更新一次。 UGMCS-Net的源代码、原始USGNet以及所有实验设置将上传到https://github.com/yanghan-yh/UGS-Net。\n4.2 Performance of Lung Nodule Segmentation 每个结节的相关注释Label1，它是注释集中的第一个。 Probabilistic U-Net是一种基于VAE的模糊分割方法，因此我们取其四个样本的平均值作为最终的分割结果。本研究使用三个指标来评估网络对病变区域的预测能力：平均 Dice 相似系数（DSC）、交集交集（IoU）和归一化表面 Dice（NSD）[35]。在表 I 中，所有方法均使用 Label1 进行评估。\n这实验数据太丰富了把！！！\n表I显示UGMCS-Net在DSC、IoU和NSD方面取得了最高分数，分别为87.65%（±0.56%）、78.78%（±0.83%）和95.62%（±0.59%）。与 U-Net 相比，UGMCS-Net 在三个指标上分别提高了 1.39%、1.99% 和 1.16%。同样，与 Attention U-Net 相比，UGMCS-Net 在各个指标上实现了 0.98%、1.45% 和 0.68% 的增强。这些结果凸显了 UGMCS-Net 卓越的分割性能，特别是 NSD 分数的大幅提高，表明其强大的边界特征分割能力。此外，与 UGSNet 相比，UGMCS-Net 在所有指标上都表现出了相当大的进步，DSC 分数提高了 0.49%，IoU 分数提高了 0.74%，NSD 分数提高了 0.34%。此外，从 UGMCS-Net 的五重交叉验证中获得的三个指标的方差始终小于 UGS-Net 的方差，表明通过集成多重注释融合损失和约束操作增强了网络稳定性。 nnU-Net 是用于分割任务的流行网络。然而，它在 DSC 中仅达到 84.60%，在 IoU 中仅达到 74.45%。这是因为nnU-Net的训练需要很大的数据集。然而，我们在此任务中只有 1860 个结节图像。\n图6显示了上述方法的部分分割结果。输入列中的红色框表示感兴趣的区域或结节容易出错的分割位置。结节(a)-(c)包含许多低密度区域，结节(d)-(f)在其边界处具有不规则形状，例如毛刺迹象，并且结节(g)-(h)具有空腔。 UGMCS-Net 对这些区域的分割明显比其他方法更符合病灶的实际形状。\n对比实验结果的套话 （可以学习一下）\n（U-Net、Attention U-Net、R2U-Net、Channel U-Net、Nested U-Net、UGS-Net 和 UGMCS-Net 的分割结果。输入列对应的红色框表示分割时应注意的特征或结节容易出错的位置。 UGMCS-Net 列中的红色框表示 UGMCS-Net 在这些位置的分割细节。绿色框表示次优分割结果的不足之处。最后一栏是相应结节的直径，单位为毫米）\n这个定性结果的对比方法，值得学习。\n图 6 和表 I 表明：（1）从注释集及其并集和交集中学习，为分割任务提供了更丰富的视觉信息。\n(2)从LC区域学习提高了网络识别低密度区域的能力。\n值得注意的是，图6（g）-（h）中，标注中没有空洞，但UGMCS-Net得到的分割结果反映了空洞特征。我们选择保留这些特征有两个原因：（1）常见结节组织的密度高于肺实质。但肺结节内的空腔密度极低，甚至是空的。\n我们可以将它们视为结节的一部分，也可以不视为结节的一部分。 (2) 分割图应提供更多有关结节特征的信息。空腔是重要的特征，因此保留这些空腔是更好的选择。如果需要，可以使用 cv2.findContours 等方法轻松去除预测中的空洞。\n表II，UGMCS-Net对于U-Net、Attention UNet、UGS-Net在Dice和IoU分数中的p值远小于0.05，并且t值的绝对值较大，表明UGMCS-Net在性能上明显优于U-Net、Attention U-Net和UGS-Net。\nP值 (P-value)：在统计分析中，P值用于量化结果发生的概率，假设零假设（通常是无效假设，如两组之间无差异）为真。在医学图像分析的研究中，如果涉及到统计检验（例如，比较两种分割方法的性能），P值可以用来表示这种比较的显著性水平。 V值：这个术语在医学图像分析中不常见，可能指的是特定研究或技术中使用的一个特定参数或度量。例如，它可能代表体积（Volume）的度量，特别是在分析器官大小或肿瘤体积时。在不同的上下文中，它可能有不同的含义。 上面的实验是基于注释集中的第一个注释Label1 。为了消除掩码选择的影响，我们还提供了 U-Net、Attention U-Net、UGS-Net 和 UGMCS-Net 在标注集中的第二个标签 Label2 上的性能。表III中列出的实验结果表明UGMCS-Net在Label2上保持了其优越的性能。这意味着所提出的方法可以在不同的掩模选择上保持稳定的性能。在传统的训练方法中，每个结节都被分配一个单一的掩模，无法为具有复杂结构特征的结节提供足够的信息。\n在我们的网络中，每个结节与 2-4 个掩模相关联。通过整合多重注释融合损失，我们将更全面的信息注入到学习过程中。它对于分割具有复杂结构和低密度纹理的结节特别有益。因此，多重注释融合损失显着提高了性能，特别是对于具有复杂结构的结节。\n4.3 Uncertain Region Prediction 除了能够分割结节之外，UGMCS-Net 还可以预测更有可能是结节组织的区域和可能性较低的区域。图 7 说明了预测结果 $\\cup(X)$和$\\cap(X)$、最终分割结果 $X_S$ 以及生成的 MCM’。在MCM’和MCM+UGSNet中，红色表示高置信度区域，蓝色表示低置信度区域，绿色对应于最终分割$X_S$。在理想情况下，$X_S$应有效地在高置信度和低置信度区域之间取得平衡。\n（预测交集$\\cap(X)$、预测并集$\\cup(X)$、最终分割$X_S$ 和 MCM 由 UGMCS-Net 生成。 MCM 中的颜色用于更好的可视化，红色表示$\\cap(X)$，蓝色表示$\\cup(X)$。此外，最终的分割在 MCM 中表示并标记为绿色以方便比较。红色框表示不易区分的结节区域或特征。最后一栏是相应结节的直径（以毫米为单位）)\n根据图 7，我们的最终分割结果位于高置信度区域和低置信度区域这两种极端情况之间。这些中间结果表明（1）我们的预测认识到所有潜在的注释，并且（2）预测被限制在$\\cap(X)$和$\\cup(X)$之间。\n具体来说，在肺结节特征分割中使用MCM具有以下几个优点，可以更好地显示结节的语义特征：（1）MCM可以更好地突出结节的显着空腔特征（图7.（g）（h））。与其他方法相比，UGMCS-Net 的预测掩模上的结节腔特征更加明显。这是由于UGMCS-Net能够在交叉掩模$\\cap(GT)$和联合掩模$\\cup(GT)$的指导下捕获结节组织的密度差异，这有助于保留结节腔的更多特征。\n(2)MCM可以更好地分割毛刺征象，这是诊断良恶性肺结节的重要特征(图7.(a)-(f))。针状结构是由结节侵入周围组织引起的星状变形，通常是低密度的并分布在结节边缘周围。这一特征是传统深度学习方法难以分割的。 UGMCS-Net在union mask $\\cup(GT)$的指导下可以更加关注结节边界特征，从而对分布在结节边界的毛刺有更好的分割性能。\n（3）MCM可以更好地分割结节的低密度组织（图7.（i）-（l）），该区域常见于磨玻璃结节，是造成专家标记差异的主要区域。 UGMCSNet通过对注释集GT和union mask $\\cup(GT)$的研究，可以最大程度地识别低密度组织，这对于磨玻璃结节的诊断很有帮助。\n由于LC掩模尺寸较小，传统的定量评估指标如DSC和IoU不足以衡量MCM的预测质量。为了解决这个问题，我们将预测的 HC 和 LC 掩模的 HU 分布与实际的 HC 和 LC 掩模进行比较。我们假设UGMCS-Net能够合理地预测不同区域的不确定性程度。因此，预测的 HC 和 LC 掩模的 HU 值分布应与实际分布相似。图8显示预测曲线与实际曲线吻合较好，表明我们预测的区域不确定性水平在统计上是可靠的。\n4.4 Ablation Study 我真的要被这个工作量给吓鼠了，太能做实验了把\n模块的消融研究：如果不指出实用性，我们会在每个部分进行五倍验证。为了更好地利用多个注释的信息并增强 $R_{LC}$ 、 $R_{HC}$ 和 $R{final}$ 之间的关系，我们用多注释融合损失和交叉联合约束模块更新了 UGMCS-Net。为了进一步证明这两个模块的贡献，我们基于 UGMCS-Net 构建了 UGMCS-$Φ_a$、UGMCS-$Φ_b$、UGMCS-$Φ_a$+$Φ_b$ 和 UGMCS-IUCM 进行消融实验。在UGMCS-$Φ_a$中，多注释融合损失仅应用于$\\cup(X)$;在UGMCS-$Φ_b$中，Multiple Annotation Fusion Loss仅应用于网络XS的最终输出；在UGMCS-$Φ_a$+$Φ_b$ 中，多重注释融合损失应用于$\\cup(X)$和$X_S$；在UGMCS-IUCM中，我们使用USG-Net中的Intersection-Union约束模块，但没有多重注释融合损失。\n我们的 UGMCS-Net 及其四种变体的性能列于表 IV，“-”表示 Attention U-Net。 V1是指没有IUCM和多重注释融合损失的UGS-Net，它作为其他变体的基础网络。\n结果表明：（1）UGMCS-$Φ_a$、UGMCS-$Φ_b$网络相对于UGMCS-Net的性能改进表明，所有标注信息的融合可以使网络更准确地捕获结节区域并获得更好的分割性能。 (2) UGMCS-$Φ_a$+$Φ_b$ 网络的 DSC、IoU 和 NSD 高于 UGMCS-$Φ_a$、UGMCS-$Φ_b$网络，表明同时对 UAM 和最终输出使用多重注释融合损失优于单独使用其中之一。 (3) 我们的 UGMCS-Net 优于UGMCS-$Φ_a$+$Φ_b$和 UGMCS-IUCM，证明了交集并集约束模块的有效性。尽管仅添加交集-并集约束模块（UGMCS-IUCM）时网络的定量性能略有下降，但观察到了显着的定性改进，这将在后面讨论。此外，UGMCS-Net的优越性能表明，多重注释融合损失和交叉联合约束模块可以相互增强，约束不确定性并促进更好的分割性能。\n为了进一步验证多注释融合损失和交叉联合约束模块的有效性，我们在图 9 中使用 Grad-CAM [36]、[37] 演示了特征图可视化。每种情况下的结果代表了网络的最终预测。 M3、M2和M1分别表示不同网络配置下倒数第三个、第二个和第一个卷积层的视觉特征图。基于图9，我们观察到：（1）当将多重注释融合损失应用于$\\cup(X)$或$X_S$时，网络对低密度组织的识别能力显着提高（UGMCS-$Φ_a$和UGMCS-$Φ_b$，结节A-D)； （2）$\\cup(X)$或$X_S$中同时使用Multiple Annotation Fusion Loss，可以在提高对低密度组织的敏感性的基础上，使网络勾勒出结节边界更加清晰（UGMCS-$Φ_a$+$Φ_b$网络，结节A-D）。 (3) IntersectionUnion Constraining Module使网络能够学习更多的边界特征，例如spiculation（毛刺）（UGMCS-IUCM网络，Nodule A-C）。 （4）当同时使用Multiple Annotation Fusion Loss和Intersection-Union Constraining Module时，网络注意力转移到结节边界，勾画出更加合理完整的结节区域（UGMCS-Net，Nodule A-E）。\n如图9所示，在IUCM的帮助下，网络可以针对复杂结节获得更语义化、更合理的分割结果。然而，如表IV所示，与UGMCS-$Φ_a$+$Φ_b$相比，UGMCS-Net在DSC、IoU和NSD上的性能增益较弱。我们认为造成这种现象的原因有3个：（1）IUCM专注于提高复杂结节的分割性能，与测量上的改进相比，IUCM使得模型对分割结果的性能提升更加显着（进一步验证在第 IV-E 节）。\n（2）复杂结节仅占结节总数的一小部分，因此IUCM无法显着提高模型在各项指标上的得分。 (3)数据集中仍然存在一些失败案例。如图10所示，在这些情况下，UGMCS-IUCM和UGMCS-Net获得的分割掩模包含更多的结节组织并且是更准确的病变区域，但它们的DSC分数较低。\n由于不同医生领域的知识偏差会影响groundtruth，因此对于分割任务来说，获得更准确、更合理的分割掩模通常比更高的度量分数更有意义。\n表V显示了加入UAM和IUCM后模型复杂度的增加。显然，UAM 和 IUCM 可以使模型以很少的计算成本获得更好的性能。\nBackbone 的消融研究：我们测试 U-Net 和 R2U-Net 作为三个模块的骨干。如表六所示，以U-Net为骨干的模型在DSC、IoU和NSD上分别获得了87.04%、78.07%和94.50%的分数。\n以R2U-Net为骨干的模型在DSC、IoU和NSD上分别获得了86.20%、76.82%和93.75%的结果。实验结果表明，所提出的特征提取模块、不确定性感知模块和交集并集约束模块是即插即用的。\n我们之前的工作评估了 nnU-Net [22] 作为骨干网络。\n这个网络在这个任务中有两个缺点：（1）它占用了太多的计算资源，（2）我们只有 1860 个结节，这可能会导致过度拟合。为了平衡计算资源和性能增益，我们在本工作中选择 Attention U-Net。\n交叉点联合约束模块中滤波器的消融研究：在第 III-C 节中，我们讨论了 Otsu 对密度特征的敏感性，使模型能够准确识别高密度组织。因此，我们利用 Otsu 的方法来提取 $R_{HC}$ 的特征。相反，Gabor对图像边缘敏感并提供有效的方向和尺度选择特征，被选择用于 $R_{LC}$的特征提取。在这些部分中，我们进行了涉及 Fold1 上 IntersectionUnion 约束模块内的五个过滤器设置的实验。表 VII 概述了这些设置。如表 VII 所示，我们的最终配置产生了最高的 DSC 分数。\n我们在图 11 中提供了 $R_{LC}^{\\prime}$ 和 $R_{HC}^{\\prime}$ 的特征可视化。可以看出， $R_{LC}^{\\prime}$ 的可视化更多地集中在结节的边缘，其中包含更多的低置信度区域。相比之下， $R_{HC}^{\\prime}$ 的可视化更关注结节的核心，具有更高的分割置信度。这些可视化结果重新证明了我们过滤器设计的有效性。\n参数消融研究：在方程 4 中，存在三个手动设置的参数：α1 指定为 0.5，α2 指定为 0.5，α3 指定为 1。在本节中，我们进行五重验证并说明这些参数选择背后的基本原理。\n如表VIII所示，当α1设置为0.5，α2设置为0.5，α3设置为1时，所提出的方法达到其峰值性能。值得注意的是，α3代表最终分割的权重，这意味着XS在训练过程中发挥着重要作用。\n此外，我们对三个分支的概率图加权平均融合进行了实验，希望网络能够为IUCM中的每个分支选择合适的权重。然而，我们观察到与 $R_{Uni}$ 对应的分支可以为结节分割提供更通用的特征，并且比其他两个分支显得“更强”。因此，其他两个分支的权重往往会减少到 0。根据这些观察结果，我们选择直接融合概率图。\n4.5 Complex-Nodule Validation UGMCS-Net 从可能导致分割不确定性的区域学习特征。因此，它可以更好地分割具有大的低密度区域或复杂结构的结节。为了更好地证明其对 U-Net 难以分割的结节的改进，我们设计了一个 Complex Nodule Validation。基于五重验证U-Net，我们进一步选择了DSC分数低于60％、70％和80％的三组结节。然后使用相同的数据设置训练 Attention U-Net、UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net，以再次测试这些结节并比较 DSC 和 IoU 分数。\nUGMCS-Net 在复杂结节上的性能提升是显而易见的。与Attention U-Net相比，对于U-Net上DSC分数低于60%的结节，UGMCS-$Φ_a$+$Φ_b$导致平均DSC分数提高1.58%，UGMCS-IUCM使平均DSC分数提高5.12%，UGMCS-平均 DSC 得分净产量增加了 5.45%。对于 U-Net 上 DSC 得分在 60% 到 70% 之间的结节，UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 的平均 DSC 得分分别提高了 0.69%、3.11% 和 5.07%。同样，对于 U-Net 上 DSC 分数在 70% 至 80% 之间的结节，UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 的平均 DSC 分数相应提高了 0.14%、1.52% 和 1.64%。从上述差异可以看出，UGMCS-IUCM对于复杂结节有很大程度的性能提升。根据表一，我们的网络在整个数据集上仅将 DSC 提高了 0.89%，但相对于 U-Net 上 DSC 分数低于 60% 的结节而言，相对于 Attention U-Net 具有很大的性能提升。这是因为具有复杂结构的结节仅占所有数据的一小部分。\n图12显示了一些复杂结节的分割结果。红色下标是UGMCS-Net的分段DSC，黑色下标是UNet的DSC。可以看出，U-Net分割DSC得分低于60%的结节是一些低密度或毛玻璃组织。 UGMCS-Net在这些结节中的显着改进表明UGMCS-Net可以更好地学习结节低密度组织的特征并更准确地分割低密度结节病变区域。当U-Net分割DSC得分低于70%时，可以观察到除了一些低密度结节外，一些结节还存在不规则空洞、毛刺、组织内突然出现亮点或肺壁过亮。 UGMCS-Net在这些结节上令人信服的分割性能反映了UGMCS-Net对边界特征、密度差异的学习能力以及良好的抗噪声能力。当U-Net分割DSC得分低于80%时，我们观察到许多新结节具有更多的实体组织。在这种情况下，UGMCS-Net可以准确地确定结节区域。此外，对于大多数结节，UGMCS-Net的分割结果反映了更多的语义特征，具有更强的可解释性。\n复杂结节验证中的分割性能分析。 UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 平均 DSC 和 IoU 后面是与 Attention U-Net 相应度量的差值（绿色数字）。所有指标均以百分比表示。\n（复杂结节验证：该验证测试了 UGMCS-Net 对 U-Net 难以分割的肺结节进行三个级别的分割性能。每张CT图像的最后两个掩模分别是UGMCS-Net和U-Net的分割结果。U-Net的分割结果以黑色显示，UGMCS-Net以红色显示。所有指标均以百分比表示。）\nDISCUSSIONS 长期以来，肺结节分割的任务一直致力于实现高精度，其中 DSC 或 IoU 是主要目标。然而，考虑到肺结节尺寸小且结构复杂，如果分割结果以不同置信度突出显示区域，可能对放射科医生更有帮助。高置信度区域提供了结节或肿瘤组织的主要部分，而低置信度区域则包含重要的低密度特征，例如毛玻璃状和毛刺征，放射科医生也应注意这些特征。\n所提出的方法旨在提供对临床诊断更有用的信息，而不是简单地改进 DSC。它并不寻求取代医生的临床作用，而是通过允许他们利用人工智能方法的优势来补充医生的临床作用。我们相信，这是将人工智能融入临床实践的更好方法。\n5.1 Data Requirement 我们的方法并不需要每个放射科医生都对所有结节进行注释。如 [11] 所示，项目期间共有 12 名放射科医生参与了所有五个站点的图像注释程序。鉴于大多数结节由 1-4 名放射科医生进行注释，可以想象结节可能由不同的放射科医生进行注释。\n尽管如此，不同放射科医生参与注释结节不会妨碍我们方法的适用性。尽管不同放射科医生之间的注释风格可能存在差异，但值得注意的是，训练有素的放射科医生遵循既定的结节注释标准，例如[38]。这些标准确保不同组的放射科医生提供多样化但基本一致的注释。\n5.2 Limitation 我们研究的一个局限性是我们仅在 LIDC-IDRI 数据集上测试我们提出的方法，该数据集是目前唯一公开可用的肺结节完整注释数据集。尽管已有十多年的历史，该数据集仍然提供了许多研究机会。然而，对多个注释的需求可能会限制我们的方法在现实临床环境中的实用性，其中获得多个注释可能并不总是可行。为了解决这个限制，我们计划探索能够基于单个注释自动识别高置信度和低置信度区域的技术，从而提高我们方法的可行性和适用性。\nCONCLUSIONS 多个注释之间的协议，以改进分割并识别分割置信度较低的区域。UAAM 从多置信度模板 (MCM) 中捕获特征，多置信度模板是低置信度 (LC) 模板和高置信度 (HC) 模板的组合。基于UAAM，我们进一步设计了不确定性引导分割网络（UGMCS-Net），其中包含特征提取模块、不确定性感知模块和交集并集约束模块。这些模块共同从多个注释之间的共识或分歧中学习有价值的信息，提供具有高和低分割置信度的区域，以及可以平衡所有可能性的分割结果。除了传统的验证方法之外，我们还提出了 LIDC-IDRI 上的复杂结节验证，测试 UGMCS-Net 对 U-Net 难以分割的肺结节的分割性能。\n实验结果表明，我们的方法可以显着提高 U-Net 分割效果不佳的结节的分割性能。\n总结：牛逼牛逼牛逼！！！ 确实牛逼！ 不愧是一区顶刊\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-uncertainty-aware-attentionmechanism/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e放射科医生拥有不同的培训和临床经验，导致\u003cstrong\u003e肺结节的分割注释\u003c/strong\u003e存在\u003cstrong\u003e差异\u003c/strong\u003e，从而导\u003cstrong\u003e致分割的不确定性\u003c/strong\u003e。传统方法通常选择\u003cstrong\u003e单个注释\u003c/strong\u003e作为学习目标或尝试学习包含\u003cstrong\u003e多个注释\u003c/strong\u003e的\u003cstrong\u003e潜在空间\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e然而，这些方法无法\u003cstrong\u003e利用多个注释之间的共识和分歧所固有的有价值的信息\u003c/strong\u003e。在本文中，我们提出了一种\u003cstrong\u003e不确定性感知注意机制\u003c/strong\u003e（UAAM），它利用多个\u003cstrong\u003e注释之间的共识\u003c/strong\u003e和分歧来促进更好的分割。为此，我们引入了\u003cstrong\u003e多置信度掩模\u003c/strong\u003e（MCM），它结合了\u003cstrong\u003e低置信度（LC）掩模\u003c/strong\u003e和高置信度（HC）掩模。 \u003cstrong\u003eLC 掩模\u003c/strong\u003e表示\u003cstrong\u003e分割置信度较低的区域\u003c/strong\u003e，\u003cstrong\u003e放射科医生可能有不同的分割选择\u003c/strong\u003e。继\u003cstrong\u003eUAAM\u003c/strong\u003e之后，我们进一步设计了一个\u003cstrong\u003e不确定性引导多置信分割网络\u003c/strong\u003e（UGMCS-Net），它包含三个模块：\u003cstrong\u003e一个捕获肺结节一般特征的特征提取模块\u003c/strong\u003e，\u003cstrong\u003e一个为肺结节产生三个特征的不确定性感知模块\u003c/strong\u003e。\u003cstrong\u003e注释的并集、交集和注释集，以及一个交集并集约束模块\u003c/strong\u003e，该模块使用\u003cstrong\u003e三个特征之间的距离来平衡最终分割和 MCM 的预测\u003c/strong\u003e。为了全面展示我们方法的性能，我们提出了 LIDC-IDRI 上的复杂结节验证，它测试了 UGMCS-Net 对使用常规方法难以分割的肺结节的分割性能。实验结果表明，我们的方法可以显着提高传统方法难以分割的结节的分割性能。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"introduction\"\u003eINTRODUCTION\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e肺结节分割\u003c/strong\u003e在\u003cstrong\u003e肺癌计算机辅助诊断 (CAD)\u003c/strong\u003e 系统中至关重要 [1]，可提供\u003cstrong\u003e结节大小、形状和其他重要医学特征\u003c/strong\u003e等关键信息。然而，对于深度学习方法的\u003cstrong\u003e一般训练和测试范例\u003c/strong\u003e，每个结节图像数据只有一个由\u003cstrong\u003e一名放射科医生\u003c/strong\u003e描绘的注释掩模[2]-[6]。因此，\u003cstrong\u003e网络每次只能提供结节区域\u003c/strong\u003e的单个预测。\u003c/p\u003e\n\u003cp\u003e然而，在临床实践中，不同的放射科医生\u003cstrong\u003e由于其不同的培训和临床经验\u003c/strong\u003e可能会为肺结节提供\u003cstrong\u003e不同的分割注释\u003c/strong\u003e[7]-[9]。\u003c/p\u003e\n\u003cp\u003e因此，基于\u003cstrong\u003e单一注释的传统方法\u003c/strong\u003e无法反映\u003cstrong\u003e临床经验的多样性\u003c/strong\u003e，限制了深度学习方法的应用。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解决放射科医生之间注释不同问题\u003c/strong\u003e的一个直接解决方案是为\u003cstrong\u003e每个肺结节图像合并多个注释\u003c/strong\u003e。这导致了另一个问题：\u003cstrong\u003e多个注释不可避免地会带来不确定性和冲突\u003c/strong\u003e，因为放射科医生\u003cstrong\u003e可能会对同一区域进行不同的注释\u003c/strong\u003e。为了克服这个问题，Kohl 等人在 2018 年提出了一种概率 U-Net，它\u003cstrong\u003e利用条件变分自动编码器\u003c/strong\u003e将\u003cstrong\u003e多个分割变体编码\u003c/strong\u003e到\u003cstrong\u003e低维潜在空间\u003c/strong\u003e中 [8]、[10]。通过从该空间采样，网络可以影响相应的分割图。基于这项研究，Hu等人提出将\u003cstrong\u003e真实不确定性\u003c/strong\u003e与\u003cstrong\u003e概率UNet\u003c/strong\u003e相结合，这可以\u003cstrong\u003e提高预测不确定性估计\u003c/strong\u003e、\u003cstrong\u003e样本准确性和样本多样性\u003c/strong\u003e[7]。这些方法依赖于\u003cstrong\u003e潜在空间和该空间中的随机样本\u003c/strong\u003e。因此，这些方法只能通过多次预测来提供不确定区域。\u003c/p\u003e\n\u003cp\u003e在本文中，我们提出了一个论点，即\u003cstrong\u003e多个注释之间的不确定性遵循特定的模式\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e为了演示这种现象，我们引入了\u003cstrong\u003e多重置信掩码\u003c/strong\u003e (MCM)，它结合了\u003cstrong\u003e高置信度 (HC) 掩码\u003c/strong\u003e和低置信度 (LC) 掩码，如图 1 所示。 A. 交叉掩码等于 \u003cstrong\u003eHC mask\u003c/strong\u003e，代表\u003cstrong\u003e所有注释的交集\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e联合掩码是所有注释的联合\u003c/strong\u003e。 \u003cstrong\u003eLC掩模是交集掩模和并集掩模之间的差异\u003c/strong\u003e。当在 LIDC-IDRI 数据集 [11] 上计算 HC 和 LC 的 Hounsfield 单位 (HU) 核估计时，\u003cstrong\u003e如图 1.B 所示，我们可以观察到 LC 和 HC 掩模之间的 HU 分布存在明显区别\u003c/strong\u003e。具体地，LC区域具有比HC区域更低的HU值。从像素分布来看，\u003cstrong\u003eHU值越低，对应区域的密度越低\u003c/strong\u003e。就CT图像特征而言，LC区域\u003cstrong\u003e主要由结节边缘、毛刺和磨玻璃特征等边界相关特征组成\u003c/strong\u003e，而\u003cstrong\u003eHC区域主要分布在结节核心内\u003c/strong\u003e。因此，我们提出了这样的假设：导致放射科医生之间差异的区域主要与\u003cstrong\u003e低密度组织和边界相关特征\u003c/strong\u003e有关。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/327b2.png\" alt=\"image-20231130203343980\"  /\u003e\r\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e与其他方法不同，我们建议利用 \u003cstrong\u003eMCM (多重置信掩码) ** 和注释集作为具有\u003c/strong\u003e不同分割确定性的特征的学习指导**，有助于更好的分割性能。我们将这种训练称为\u003cstrong\u003eUncertaintyAware Attention Mechanism\u003c/strong\u003e，如图2所示。按照这种机制，我们进一步设计了用于肺结节分割的\u003cstrong\u003eUncertainty-Guide Multi-Confidence Segmentation Network\u003c/strong\u003e（UGMCS-Net）。\u003c/p\u003e","title":"Uncertainty-Aware Attention Mechanism:利用不确定性感知注意机制进行肺结节分割和不确定区域预测"},{"content":"程序设计作业接口文档 统一返回格式\n{ code: ...,\t# 状态码 msg: ...,\t# 描述信息 data: { # 数据 ... } } code = { 200 == 成功, 500 == 失败, } msg = { success == 成功 fail == 失败 ... } data = { key : value } 前端 虚拟换衣功能 @请求格式 （请求后端） # 前后端需统一样例图片id { userId: ...\u0026lt;int\u0026gt;,\t# 标识哪个用户的请求 isUploadCloth: ...\u0026lt;bool\u0026gt;, # 若上传衣服图片使用base64，否则用id isUploadPerson: ...\u0026lt;bool\u0026gt;, # 若上传人物图片使用base64，否则用id clothData: ...\u0026lt;base64||null\u0026gt;, # 衣服图片base64编码 personData: ...\u0026lt;base64||null\u0026gt;,\t# 人物图片base64编码 exampleClothId: ...\u0026lt;int\u0026gt;,\t# 衣服样例图片id examplePersonId: ...\u0026lt;int\u0026gt;\t# 任务样例图片id } 动漫头像功能 @请求格式 （请求后端） { userId: ...\u0026lt;int\u0026gt;, # 标识哪个用户的请求 imgData: ...\u0026lt;base64\u0026gt; # 需要动漫化的图片 } 后端 虚拟换衣功能 前端请求API: https://talented-civet-separately.ngrok-free.app/tryon/ @返回格式 (返回前端) { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { tryon_result : ...\u0026lt;url\u0026gt;, # 处理后的图片url } } 动漫头像功能 前端请求API: https://talented-civet-separately.ngrok-free.app/anime/ @返回格式 (返回前端) { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg : ...\u0026lt;string\u0026gt;, # 消息 (success / fail) data: { anime_result : ...\u0026lt;url\u0026gt;, # 处理后的图片url } } 模型端 虚拟换衣功能 后端请求API: https://certain-ideally-foal.ngrok-free.app/tryon/predict/ @请求格式\t(后端发出请求) { userid : ...\u0026lt;int\u0026gt;, # 用户id cloth : ...\u0026lt;url\u0026gt;, # 衣服图片url链接 person : ...\u0026lt;url\u0026gt;\t# 人物图片url链接 } @返回格式\t（返回后端） { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { image_value : ...\u0026lt;base64\u0026gt;, # 处理后的图片base64编码 } } 动漫头像功能 后端请求API: https://certain-ideally-foal.ngrok-free.app/anime/predict/ @请求格式 { userid : ...\u0026lt;int\u0026gt;, # 用户id origin_image : ...\u0026lt;url\u0026gt;, # 原图片url链接 } @返回格式 { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { image_value : ...\u0026lt;base64\u0026gt;, # 处理后的图片base64编码 } } ","permalink":"https://swimmingliu.cn/posts/diary/2023-%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%BD%9C%E4%B8%9A%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3/","summary":"\u003ch1 id=\"程序设计作业接口文档\"\u003e程序设计作业接口文档\u003c/h1\u003e\n\u003cp\u003e统一返回格式\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecode:\u003c/span\u003e \u003cspan class=\"err\"\u003e...,\u003c/span\u003e\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e状态码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003emsg:\u003c/span\u003e \u003cspan class=\"err\"\u003e...,\u003c/span\u003e\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e描述信息\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003edata:\u003c/span\u003e \u003cspan class=\"err\"\u003e{\u003c/span\u003e     \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \t\u003cspan class=\"err\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003ecode\u003c/span\u003e \u003cspan class=\"err\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003e200\u003c/span\u003e \u003cspan class=\"err\"\u003e==\u003c/span\u003e \u003cspan class=\"err\"\u003e成功,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003e500\u003c/span\u003e \u003cspan class=\"err\"\u003e==\u003c/span\u003e \u003cspan class=\"err\"\u003e失败,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003emsg\u003c/span\u003e \u003cspan class=\"err\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003esuccess\u003c/span\u003e \u003cspan class=\"err\"\u003e==\u003c/span\u003e \u003cspan class=\"err\"\u003e成功\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003efail\u003c/span\u003e    \u003cspan class=\"err\"\u003e==\u003c/span\u003e \u003cspan class=\"err\"\u003e失败\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003edata\u003c/span\u003e \u003cspan class=\"err\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ekey\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003evalue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"前端\"\u003e前端\u003c/h2\u003e\n\u003ch3 id=\"虚拟换衣功能\"\u003e虚拟换衣功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e@请求格式\u003c/span\u003e  \u003cspan class=\"err\"\u003e（请求后端）\u003c/span\u003e  \t \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e前后端需统一样例图片id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003euserId:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e\t\t\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e标识哪个用户的请求\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eisUploadCloth:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;bool\u0026gt;,\u003c/span\u003e           \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e若上传衣服图片使用base64，否则用id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eisUploadPerson:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;bool\u0026gt;,\u003c/span\u003e          \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e若上传人物图片使用base64，否则用id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eclothData:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;base64||null\u0026gt;,\u003c/span\u003e \t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e衣服图片base64编码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003epersonData:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;base64||null\u0026gt;,\u003c/span\u003e\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e人物图片base64编码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eexampleClothId:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e\t\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e衣服样例图片id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eexamplePersonId:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;\u003c/span\u003e\t\t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e任务样例图片id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"动漫头像功能\"\u003e动漫头像功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e@请求格式\u003c/span\u003e \u003cspan class=\"err\"\u003e（请求后端）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003euserId:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e    \t        \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e标识哪个用户的请求\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eimgData:\u003c/span\u003e  \u003cspan class=\"err\"\u003e...\u0026lt;base64\u0026gt;\u003c/span\u003e \t        \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e需要动漫化的图片\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"后端\"\u003e后端\u003c/h2\u003e\n\u003ch3 id=\"虚拟换衣功能-1\"\u003e虚拟换衣功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e前端请求API:\u003c/span\u003e \u003cspan class=\"err\"\u003ehttps:\u003c/span\u003e\u003cspan class=\"c1\"\u003e//talented-civet-separately.ngrok-free.app/tryon/\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"err\"\u003e@返回格式\u003c/span\u003e \u003cspan class=\"err\"\u003e(返回前端)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecode:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e \t                \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e状态码\u003c/span\u003e \u003cspan class=\"err\"\u003e(200表示成功,\u003c/span\u003e \u003cspan class=\"err\"\u003e500表示失败)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003emsg:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;string\u0026gt;,\u003c/span\u003e\t                \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e消息\u003c/span\u003e \u003cspan class=\"err\"\u003e(success\u003c/span\u003e \u003cspan class=\"err\"\u003e/\u003c/span\u003e \u003cspan class=\"err\"\u003efail)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003edata:\u003c/span\u003e \u003cspan class=\"err\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"err\"\u003etryon_result\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;url\u0026gt;,\u003c/span\u003e  \t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e处理后的图片url\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"动漫头像功能-1\"\u003e动漫头像功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e前端请求API:\u003c/span\u003e \u003cspan class=\"err\"\u003ehttps:\u003c/span\u003e\u003cspan class=\"c1\"\u003e//talented-civet-separately.ngrok-free.app/anime/\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"err\"\u003e@返回格式\u003c/span\u003e \u003cspan class=\"err\"\u003e(返回前端)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecode:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e  \t                \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e状态码\u003c/span\u003e \u003cspan class=\"err\"\u003e(200表示成功,\u003c/span\u003e \u003cspan class=\"err\"\u003e500表示失败)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003emsg\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;string\u0026gt;,\u003c/span\u003e \t                \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e消息\u003c/span\u003e \u003cspan class=\"err\"\u003e(success\u003c/span\u003e \u003cspan class=\"err\"\u003e/\u003c/span\u003e \u003cspan class=\"err\"\u003efail)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003edata:\u003c/span\u003e \u003cspan class=\"err\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \t\u003cspan class=\"err\"\u003eanime_result\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e  \u003cspan class=\"err\"\u003e...\u0026lt;url\u0026gt;,\u003c/span\u003e \t\u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e处理后的图片url\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"模型端\"\u003e模型端\u003c/h2\u003e\n\u003ch3 id=\"虚拟换衣功能-2\"\u003e虚拟换衣功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e后端请求API:\u003c/span\u003e \u003cspan class=\"err\"\u003ehttps:\u003c/span\u003e\u003cspan class=\"c1\"\u003e//certain-ideally-foal.ngrok-free.app/tryon/predict/\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"err\"\u003e@请求格式\u003c/span\u003e\t\u003cspan class=\"err\"\u003e(后端发出请求)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003euserid\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e              \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e用户id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecloth\u003c/span\u003e  \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;url\u0026gt;,\u003c/span\u003e \t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e衣服图片url链接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eperson\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;url\u0026gt;\u003c/span\u003e\t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e人物图片url链接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e@返回格式\u003c/span\u003e\t\u003cspan class=\"err\"\u003e（返回后端）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecode:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e \t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e状态码\u003c/span\u003e \u003cspan class=\"err\"\u003e(200表示成功,\u003c/span\u003e \u003cspan class=\"err\"\u003e500表示失败)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003emsg:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;string\u0026gt;,\u003c/span\u003e\t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e消息\u003c/span\u003e \u003cspan class=\"err\"\u003e(success\u003c/span\u003e \u003cspan class=\"err\"\u003e/\u003c/span\u003e \u003cspan class=\"err\"\u003efail)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003edata:\u003c/span\u003e \u003cspan class=\"err\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"err\"\u003eimage_value\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;base64\u0026gt;,\u003c/span\u003e  \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e处理后的图片base64编码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"动漫头像功能-2\"\u003e动漫头像功能\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e后端请求API:\u003c/span\u003e \u003cspan class=\"err\"\u003ehttps:\u003c/span\u003e\u003cspan class=\"c1\"\u003e//certain-ideally-foal.ngrok-free.app/anime/predict/\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"err\"\u003e@请求格式\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003euserid\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e  \t   \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e用户id\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003eorigin_image\u003c/span\u003e  \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;url\u0026gt;,\u003c/span\u003e \t   \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e原图片url链接\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e@返回格式\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003ecode:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;int\u0026gt;,\u003c/span\u003e \t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e状态码\u003c/span\u003e \u003cspan class=\"err\"\u003e(200表示成功,\u003c/span\u003e \u003cspan class=\"err\"\u003e500表示失败)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003emsg:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;string\u0026gt;,\u003c/span\u003e\t            \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e消息\u003c/span\u003e \u003cspan class=\"err\"\u003e(success\u003c/span\u003e \u003cspan class=\"err\"\u003e/\u003c/span\u003e \u003cspan class=\"err\"\u003efail)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"err\"\u003edata:\u003c/span\u003e \u003cspan class=\"err\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"err\"\u003eimage_value\u003c/span\u003e \u003cspan class=\"err\"\u003e:\u003c/span\u003e \u003cspan class=\"err\"\u003e...\u0026lt;base64\u0026gt;,\u003c/span\u003e  \u003cspan class=\"err\"\u003e#\u003c/span\u003e \u003cspan class=\"err\"\u003e处理后的图片base64编码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"程序设计作业接口文档"},{"content":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。\n2）级联网络在医学分割中的应用：级联网络已广泛应用于正常组织和病变的分割以及不同类型病变的分割，包括肝脏病变、脑肿瘤、硬化病变和前列腺癌的分割[4] ，[15][5]，[16]。例如，Awad 等人[17]提出了一个名为 CU-Net 的级联框架，用于在 CT 扫描中对肝脏和病变进行自动分割。他们还提供了可以指导临床治疗的有用信息和解释。 Xi等人[18]提出了一种级联U-ResNets，它遵循一种新颖的垂直级联策略，并在他们的工作中评估了不同类型的损失函数。除了肝脏病灶分割之外，级联策略在 BraTS 挑战中也很流行。例如，BraTS 2019挑战赛的Top-2解决方案[6]、[7]都是具有不同级联策略的级联网络。\n3）神经网络中的注意力：注意力机制受到人类感知和视觉认知的启发，并已普遍应用于计算机视觉任务中[19]，[20][11]，[21]。计算机视觉任务中的注意力机制是在神经网络提取的特征表示上生成空间或通道权重图。例如，Woo等人[20]开发了一个卷积块注意力模块（CBAM）来引入一种融合注意力机制，其中包括通道注意力和空间注意力。这种注意力模块可以插入到常用的分类或分割网络中。Oktay等人[11]在Attention U-Net中提出了一种新颖的注意力门，用于细化网络编码器提取的特征表示，以促进网络专注于ROI。最近，首先在自然语言处理任务中提出的变压器[22]已被引入到医学分割任务中。例如，Wang 等人 [23] 提出了一种 TransBTS，用于从多模态脑 MRI 中执行脑肿瘤分割。\n综上所述，注意力机制已被广泛用于突出ROI并抑制不相关信息，但目前基于注意力的方法的研究并没有对注意力如何产生以及网络为何关注某些区域提供清晰的解释，这使得限制了注意力机制的可解释性。\nMethod 在本节中，我们将详细介绍所提出的先验注意网络架构。在第一部分中，我们将概述所提议的网络。然后，我们相应地提供有关具有中间监督、参数化跳跃连接和具有深度监督的多类解码器的所提出的注意引导解码器的详细信息。\n3.1 . Overview of Network Architecture 基本上，我们提出的网络是基于 U-Net [3] 架构进行修改的，该架构具有 U 形拓扑以及编码器和解码器之间的跳跃连接。在提出的先验注意网络中，一种新颖的注意引导解码器模块被集成到网络的跳跃连接中，以通过空间注意来细化特征表示。网络中还引入了一种新颖的参数化跳跃连接，以指导网络学习普通特征图和精炼特征图之间的比率。注意力引导解码器从编码器获取丰富的语义特征并生成空间注意力图来指导接下来的多类分割。为了产生与投资回报率相关的注意力，框架中使用了中间监督策略。然后将细化的特征图发送到多类解码器以进行最终的密集预测。\n多类解码器采用深度监督策略以获得更好的收敛性并提高分割性能。这种网络拓扑通过注意力引导解码器生成的注意力图在单个网络中实现了传统级联网络的两个步骤。组网方案如图2所示。\n3.2 Attention Guiding Decoder 在典型的级联网络中，分割的第一步是执行粗分割并找到输入图像中的 ROI。在提出的先验注意网络中，我们提出了一个注意引导解码器来执行该过程。所提出的注意力引导解码器被集成到网络中以生成与 ROI 相关的注意力图，然后利用这些图来细化特征表示并提高多类分割性能。\n1）模块拓扑：所提出的注意力引导解码器的基本拓扑基于FCN [2]中提出的特征融合。从网络解码器最深三层提取的特征表示被馈送到该模块。由于特征图的空间大小不同，因此首先执行线性插值以对特征图进行上采样。然后对特征进行压缩，抑制通道维度中的不相关信息，降低计算成本。然后将压缩后的特征分别在通道维度上连接起来以进行特征融合。(torch.cat) 最后，融合三个特征图以获得最终的预测。\n为了简单起见，我们使用 2D 分割来说明注意力图的计算。我们使用 Xi ∈ R Ci × Hi × Wi,i ∈ (3, 4, 5) 表示从网络编码器提取的特征图，其中 X5 表示最深的特征。特征压缩和融合计算如下：\n其中Z5 ∈ R C4 × H4 × W4表示X5和X4的融合特征，Z4 ∈ R C3 × H3 × W3表示X4和X3的融合特征，Wc5 ∈RC5×C4和Wc4 ∈RC4×C3表示相应的压缩卷积， W4 ∈ RC4×C4 表示融合 X5 和 X4 的融合卷积，⊕ 表示特征串联。\n注意力引导解码器的输出计算如下：\n其中 W3 ∈ R C3×C3 表示融合 X4 和 X3 的融合卷积，Wout ∈ R C3 × 1 表示输出卷积，σ 分别表示 Sigmoid 激活。\n2）中间监督：计算机视觉中的传统注意力机制自动生成注意力图，但注意力生成的过程通常是人类无法解释的，并且网络关注的区域可能与人类关注的区域不同。\n这种差距会限制注意力机制的性能和可解释性，有时还会导致网络容量的恶化。为了解决这些问题，我们在网络中引入了中间监督策略。在遵循从粗到细的方式的多病灶分割任务中，我们首先生成一个二元Ground Truth，其中前景表示所有类型的病灶，背景表示其他一切。在具有 C 种病变的多病变分割任务中，我们使用 Gi,i ∈ (1,\u0026hellip;,C) 表示第 i 类病变的二元基本事实，其中前景表示特定病变背景代表其他一切。二进制真实值 Gb 计算如下：\n然后利用二元损失函数来计算二元真实值 yb 和注意力引导解码器生成的注意力图 Y 之间的二元损失 l：\n其中 Lb 表示二元损失函数。然后利用计算出的损失 l 来监督注意力引导解码器的参数更新。\n这个地方的中间监督主要是 要让中间的注意力机制起作用，不能随便生成。\n通过引入中间监督，生成的注意力由输入图像的二元真实值进行监督。这样，网络被迫学习多病灶分割任务的分解，即首先提取病灶区域，然后对病灶区域进行细粒度分类。这种分解降低了多病灶分割的难度，并且与当前在“黑匣子”中生成注意力的基于注意力的医学分割方法相比，具有更好的可解释性。\n3.3 Parameterized Skip Connections 跳跃连接已广泛应用于流行的卷积网络中，包括U-Net [3]、ResNet [24]等。受[11]的启发，我们建议将注意力图集成到连接网络编码器和多网络的跳跃连接中，形成多级解码器。在跳跃连接中，我们还引入了额外的残差路径来恢复普通特征图并进一步提高分割性能。与传统的残差路径相比，残差路径的幅值因子αi,i ∈ (1, 2,…, 5)被设置为网络的可学习参数，并在反向传播过程中更新。我们相信这样的设置可以为网络增加额外的非线性能力并增强跳过连接的有效性。\n我们使用 Fi,i ∈ (1, 2,\u0026hellip;, 5) 表示来自网络编码器的普通特征图，Y 表示注意力图，精炼后的特征图 Fri,i ∈ (1, 2,. .., 5) 多类解码器接收的信息计算如下：\n然后将细化的特征图发送到多类解码器以进行最终的多类预测。\n3.4 Multi-Class Decoder With Deep Supervision U形分割网络中的解码器用于接收编码器发送的特征图，随着解码器中的特征通道数量的减少和空间分辨率的增加，分割性能逐步细化。然而，随着网络变深，最深的解码器块变得难以训练，这可能会限制最终的分割性能。深度监督策略已经被提出来训练深度卷积网络[25]、[26]。在所提出的先验注意网络中，辅助预测是从不同级别的解码器块中提取的，并使用相同的基本事实进行监督。我们使用 Pi,i ∈ (1, 2, 3) 表示来自多类解码器的辅助预测，Pm 表示最终的多类预测，g 表示真实值，Lm 表示多类损失函数。最终的多类损失计算如下：\n所提出的多类解码器的解码器块也与当前网络设置具有共同的设置，即卷积层、归一化层和非线性激活单元的堆栈。\n总结一下：\n① 注意力机制 + 中间监督：最后三层特征融合 + 这一部分做深度监督（原来这样也叫创新）\n② 跳跃连接的部分加了一个α因子 （感觉像权重一样的东西 ）\n③ 多阶段的深度监督 （这个就算一个trick吧，大家都在用）， 不过这里变成了多类别\nExperiments 4.1 Strong Baselines and Evaluation Metrics 为了研究网络架构的性能差异，我们将所提出的先验注意网络与医学分割中最流行的方法进行了比较，包括 U-Net [3]、Attention U-Net [11] 和级联 U-Net，在两个 2D 中和 3D 分割任务。值得注意的是，与他们论文中提出的原始版本相比，基线方法根据网络拓扑方面的某些任务进行了修改和优化，以获得性能提升。我们将残差连接[24]、批量归一化[30]和来自 ImageNet 的预训练编码器引入到 2D COVIDlesion 分割任务的基线方法中，并且我们还将残差连接、实例归一化和 PReLU 激活引入到 3D 脑肿瘤分割任务中。除了网络拓扑之外，基线方法与所提出的先验注意网络共享相同的数据增强和训练配置。\n对于 COVID-19 病变的 2D 分割，我们使用 Dice 指数、精度分数和召回分数来评估所提出的网络的性能。 Dice指数是一种用来衡量两个样本相似度的统计量，已广泛用于分割算法的评估。精确率衡量的是实际正确的阳性识别的比例，召回率衡量的是算法对阳性样本的敏感度。对于 BraTS 2020 挑战赛的 3D 分割，在在线门户上进行评估，并根据 Dice 指数和 95% Hausdorff 距离（HD）对算法进行排名。\n我们使用 G 表示ground truth，P 表示密集预测，TP 表示正确预测的正样本，FP 表示错误预测的正样本，TN 表示正确预测的副样本，FN 表示错误预测的副样本。这些指标的计算方式如下：\n这个HD（Hausdorff ）也是一种评估分割结果的方式 （alright 又多了一种指标）\n4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割 1）数据：由于可用的开源COVID-19 CT分割数据集通常很小，因此利用两个独立的公开可用数据集，即COVID-19 CT分割数据集[27]和CC-CCII数据集[28]来验证所提出的方法二维分割任务中的方法。第一个数据集包含来自 40 多名患者的 100 个轴向 CT 切片，这些切片已重新缩放至 512 × 512 像素并进行灰度化。所有切片均由放射科医生用不同的标签进行分割，以识别不同类型的肺部感染。第二个数据集由 150 名 COVID-19 患者的 750 张 CT 切片组成，这些切片被手动分割为背景、肺野、毛玻璃混浊和实变。由于并非所有 750 个切片都包含病变，我们最终使用了 150 名患者的 549 个带注释的切片。对于这两个数据集，利用 5 倍交叉验证来评估所提出模型的性能。\n折叠之间的数据根据患者进行分割，以避免潜在的数据泄漏。最终的标签和分割图包含 3 个类别，包括背景、毛玻璃不透明度 (GGO) 和合并 (CON.)。\n2）实现细节：a）模型设置和损失函数：对于预训练网络编码器，我们采用来自ImageNet的预训练ResNeXt-50（32 × 4d）[31]作为基线方法和所提出的先验注意网络的编码器。对于解码器中的上采样，采用双线性插值，比例因子为2。对于中间监督和级联U-Net第一阶段的二元损失函数，我们采用Dice Loss [13]和Focal的线性组合损失[32]作为损失函数。对于最终输出的多类损失函数，我们采用Focal Tversky Loss [33]作为损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 16.04 服务器上使用 PyTorch 1.7.1 框架实现的。我们使用 NVIDIA RTX 2080 Ti GPU 来加速我们的训练过程。在我们的训练过程中使用Albumentations [34] 进行数据增强，以减少过度拟合并提高泛化能力。首先，将所有输入图像重新缩放为 560 × 560，然后进行随机亮度和对比度偏移以及随机仿射变换。然后将图像随机裁剪为 512 × 512，然后进行随机弹性变换，最后输入网络。该模型由 Adam 优化器优化，β1 = 0.9、β2 = 0.999、γ = 1e − 8。L2 正则化也用于减少过度拟合。我们将模型权重衰减设置为 1e − 5。初始学习率设置为 1e −4 并降低，然后采用余弦退火策略。批量大小设置为 4，模型训练 40 轮。该模型使用 5 倍交叉验证进行评估。\n3）定量结果：我们在两个数据集上的实验中不同模型的详细比较分别如表一和表二所示。如图所示，我们提出的网络在毛玻璃不透明度和固结的 Dice 分数方面优于 U-Net、Attention U-Net。所提出的 PANet 以更少的参数和计算成本实现了与级联 U-Net 竞争的结果。由于这些模型在模型主干和训练策略上是相同的，很明显，所提出的注意力引导解码器、中间监督和深度监督的组合对分割性能有很大贡献。注意力引导解码器的利用有助于模型更准确地检测感染组织并生成与感染相关的注意力图，从而有利于解码器中的多类分割。\n此外，中间监督和深度监督的引入促进了网络的收敛，这也有助于提高性能。\n好好好，这哥们儿，睁着眼睛说瞎话是吧（这Unet明明比你低啊，精度也没差多少啊）\n4）定性结果：不同模型在 2D COVID-19 切片上的视觉比较如图 4 所示。由于模型在 Dice 分数方面非常接近，因此乍一看这些模型的表现相似。但与 U-Net 和 Attention U-Net 相比，所提出的 PANet 在实变和微小病变的分割上表现更好。\n与 U-Net 和 Attention U-Net 相比，PANet 产生更准确的分割掩模，并且与 Cascaded U-Net 相比，所提出的网络以更少的计算成本实现了有竞争力的结果。\n额 只要定量结果上去了，好像定性结果都是挑好的说吧？\n) 消融实验：进行了几次消融实验来评估我们模型中组件的性能，如表 III 所示。 a）具有深度监督的多类解码器的有效性：为了探索深度监督策略的贡献，我们建立了两个实验：No.1（U-Net）和No.2（U-Net + DS）。表三的结果表明，深度监督在一定程度上对绩效有所贡献。\nb）注意力引导解码器的有效性：我们通过构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中所提出的注意力引导解码器的有效性。如表III所示，与实验1相比，注意力引导解码器的引入提供了显着的性能提升。这表明注意力引导解码器在所提出的网络中提供了有效的注意力图，从而指导解码器中的多类分割。\nc）参数化跳跃连接的有效性：为了探索所提出的参数化跳跃连接的有效性，我们建立了两个实验4（U-Net + AGD*）和5（U-Net + AGD）。引入参数化跳跃连接后，分割性能得到了提高，几乎没有额外的参数或计算成本。\nd）中间监督的有效性：为了研究所提出的 PANet 中中间监督策略的有效性，我们比较了第 3 号（U-Net + AGD w/o IS）和第 5 号（U-Net + AGD）。如表 III 所示，与第 3 种相比，具有中间监督的网络获得了额外的改进。此外，在比较第 6 种（U-Net + DS + AGD w/o IS）和第 7 种（PANet）时也可以观察到改进。 ）尽管改进相对较小。可以看出，深度监管的引入也对绩效产生了提升，因此中级监管对绩效的提升并不像以前那么显着。\n4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs 1）数据：我们使用来自 BraTS 2020 挑战赛的开源多模态 MRI 数据集 [29]、[36] [37]。训练集由 369 个多对比 MRI 扫描组成，其中每个扫描包含四种模式，即原生 T1 加权、对比后 T1 加权 (T1Gd)、T2 加权 (T2) 和 T2 流体衰减反转恢复 (FLAIR) ）。\n每次扫描都有相应的 4 类标签：背景（标签 0）、GD 增强肿瘤（ET，标签 4）、瘤周水肿（ED，标签 2）以及坏死和非增强肿瘤核心（NET/ NCR，标签 1)。验证集由 125 个多重对比 MRI 扫描组成，其模式与训练集和隐藏的基本事实相同。所有 MRI 扫描均去除颅骨，与相同的大脑模板 (SRI24) 对齐，并插值至 1mm3 分辨率。验证阶段通过在线门户进行，算法根据 3 个重叠肿瘤区域的性能进行排名，即增强肿瘤 (ET)、肿瘤核心 (ET + NET/NCR) 和整个肿瘤 (ET) + NET/NCR + ED）\n2）实现细节：a）模型设置和损失函数：对于3D分割，由于缺乏开源预训练编码器，所有模型都是从头开始训练的。下采样通过跨步 3 × 3 × 3 填充卷积执行，上采样通过三线性插值实现。为了进一步提高在BraTS数据集上的性能，我们采用基于区域的训练策略（直接在重叠区域而不是独立标签上优化）并增强肿瘤抑制（如果增强肿瘤的预测体积为，则用坏死替换预测的增强肿瘤）小于某个阈值）在训练过程中。对于损失函数，我们采用 Dice Loss [13] 和 Cross Entropy Loss 的线性组合作为网络中二分类和多分类阶段的损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 服务器上使用 PyTorch 1.7.1 框架实现的。由于3D分割的训练，尤其是级联U-Net对显存的要求较高，因此我们使用NVIDIA RTX 2080 Ti GPU和NVIDIA RTX 3090 GPU分别训练单个模型和级联模型。\n由于训练过程的显存占用大于11Gb，我们采用PyTorch框架提供的原生混合精度训练程序来节省显存使用并加速训练过程。人工智能医学开放网络（MONAI）项目[38]和TorchIO[39]分别用于训练和推理阶段的数据加载过程。数据增强是在训练过程中通过 MONAI 项目进行的。\n首先，分别使用 z 分数标准化对所有模态进行标准化。然后通过随机翻转、随机强度偏移、随机强度缩放和弹性变换来增强图像。最后，我们将图像块随机裁剪为 128 × 128 × 128 并将其输入网络。对于推理，我们还采用基于补丁的推理管道来生成 BraTS 2020 验证集的预测。面片大小设置为 128 × 128 × 128，面片之间的重叠设置为 75%。重叠区域中的预测是重叠块的平均值。这种重叠配置可以被视为自集成并产生更好的分割性能。对于模型评估，我们进行了 2 个单独的评估程序来比较分割模型的性能。首先，我们对 BraTS 2020 训练集进行 5 倍交叉验证，以比较离散区域（增强肿瘤、瘤周水肿和非增强肿瘤核心）上的分割性能。然后，我们对 BraTS 2020 验证集进行评估，以比较重叠区域（增强肿瘤、肿瘤核心和整个肿瘤）的分割性能，其中我们可以将所提出的方法与最先进的方法进行比较BraTS 挑战。\n3）定量结果：BraTS 2020 训练集的交叉验证性能已在表 IV 中报告。\n所提出的 PANet 在 Dice 分数和 Hausdorff 距离方面优于所有其他网络。此外，我们还在 BraTS 2020 验证集上对训练后的模型进行了验证。通过这种方式，我们还将所提出的网络与模态配对学习（BraTS 2020 中的 Top-2 解决方案）[35] 和研究中的 Transformer TransBTS [23] 进行了比较，除了基于 U 的常用网络之外-网。\n性能列于表五中。我们提出的 PANet 在 BraTS 2020 验证集上优于 U-Net、Attention U-Net、级联 U-Net 和 TransBTS。此外，所提出的 PANet 在 BraTS 2020 上实现了与 Top2 解决方案类似的 Dice 分数，并且具有更好的 Hausdorff 距离。另外，值得注意的是，与 U-Net 相比，所提出的 PANet 仅增加了 3.9% 的额外 GFlops，并且以更少的计算成本实现了比级联 U-Net 更好的性能。这些结果表明，所提出的先验注意网络在 BraTS 2020 数据集上的分割性能和计算效率之间取得了复杂的平衡。\n4）定性结果：我们可视化具有不同分割难度的 3 幅 MRI 图像，以展示不同模型的性能。对于最简单的情况（BraTS20_Validation_077），所有模型都能够分割病变，而所提出的 PANet 获得最高的分割 Dice 分数。对于有一定难度的病例（BraTS20_Validation_028），Attention U-Net和级联U-Net在水肿分割方面都产生了严重的误报，导致整个肿瘤的Dice评分较低。对于最难的情况（BraTS20_Validation_076），由于增强肿瘤的假阳性分割，U-Net、Attention U-Net 和级联 U-Net 的性能并不乐观，而所提出的 PANet 产生了最好的分割性能。所提出的 PANet 的成功归功于具有中间监督的注意力引导解码器，特征图通过空间注意力图进行细化，这可以防止潜在的误报预测。\n关于医学影像中的轴位面（横断面）、冠状面、矢状面的解释\n1.冠状面 （Coronal），又称额状面。即从左右方向，沿人体的长轴将人体纵切为前、后两部分的切面。这种提法只是为了在临床中将器官位置描述的更具体，英文名称是：Coronal section；\n2.矢状面 (Sagittal)就是把人体分成左右两面的解剖面，于这个面平行的也是矢状面。出于这个位置的叫矢状位。矢状位的英文名称是：Median sagittal section；\n3.水平位 (Axial)又称横断位，即左右、前后构成的面为水平位，英文名称是:Transverse section。\n5）消融分析：在 BraTS 2020 验证数据集上也进行了与 2D 分割类似的消融实验，以评估我们模型中呈现的组件的有效性，如表六所示。\na）具有深度监督的多类解码器的有效性：我们通过向网络解码器引入深度监督来构建实验2（U-Net + DS）。与基线（No.1）相比，实验No.2在增强肿瘤、肿瘤核心和整个肿瘤的分割性能方面提供了一定程度的性能提升。此外，当引入注意力引导解码器时，可以观察到性能的提高（第3和第6）。\nb) 注意力引导解码器的有效性：我们构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中注意力引导解码器的有效性。与基线相比，注意力引导解码器在增强肿瘤和肿瘤核心的分割性能方面产生了显着的改善。这表明注意力引导解码器对从编码器提取的特征图产生有效的注意力，从而更容易区分病变。\nc）参数化跳跃连接的有效性：我们建立了两个实验No.4（U-Net + AGD*）和No.5（UNet + AGD）来探索所提出的参数化跳跃连接的有效性。引入参数化跳跃连接后，在增强肿瘤和肿瘤核心方面分割性能有所提高，但整个肿瘤的分割性能略有下降。\nd）中间监督的有效性：我们通过比较表六中的第3号和第5号来调查中间监督策略的有效性。在No.5（U-Net + AGD）中，通过引入中间监督，增强肿瘤和肿瘤核心的Dice得分显着提高。但是当引入深度监督时，中间监督的性能提升并不像以前那么显着（第6和第7），这与2D分割情况类似。\nDISCUSSION AND CONCLUSION 多病灶分割在临床场景中具有重要意义，因为某种疾病可能同时发生多种类型的感染，不同感染阶段的患者可能会出现不同类型的病灶。例如，磨玻璃样混浊（GGO）和实变（CON.）是COVID-19患者典型的肺部病变，前者通常发生在早期患者，而后者的增加可能表明病情恶化。胶质瘤可分为低级别胶质瘤（LGG）和胶质母细胞瘤，即高级别胶质瘤（GBM/HGG），并且在发生HGG的患者中更有可能发现强化肿瘤。因此，多病灶分割在患者的筛查和预后方面具有巨大的潜力。\n多病灶分割问题可以分解为粗分割和细分割，粗分割是对病灶进行粗略分割，而细分割则基于前一分割，以产生最终的分割图。级联网络广泛用于多病变分割任务，因为这些算法背后的逻辑非常自然。然而，级联网络在潜在的临床部署中受到限制，因为它们缺乏灵活性并且对计算资源的要求很高。与现有的级联网络相比，我们开发了一种先验注意网络，它将粗分割和细分割集成到一个网络中。我们提出的网络架构的优点是分割性能和计算效率的平衡。通过将分割的两个步骤结合在一个网络中，所提出的先验注意网络能够实现多病灶分割的端到端训练，在训练和推理方面都具有更大的灵活性，并且在临床部署中具有更大的潜力。此外，我们设法保持所提出的先验注意网络的性能，在分割性能和运行效率之间实现复杂的平衡。\n与级联 U-Net 相比，我们提出的先验注意力网络在 2D 和 3D 任务中都实现了更好的性能和效率，如图 6 所示。\n总之，我们提出了一种新颖的分割网络，即先验注意网络，用于医学图像中的多病灶分割。受流行的从粗到细策略的启发，我们通过空间注意机制将级联网络的两个步骤聚合成一个网络。此外，我们引入了一种新颖的中间监督机制来指导与病变相关的注意图的生成，这可以指导解码器中的后续多类分割。所提出的网络在 2D 和 3D 医学图像（包括 CT 扫描和多模态 MRI）上进行评估。对于 2D 分割，与级联 U-Net 相比，所提出的先验注意力网络以更少的计算成本获得了有竞争力的结果。对于 3D 脑肿瘤分割，所提出的先验注意网络在 BraTS 2020 验证数据集上产生了最先进的性能，并且优于基于 U-Net 的其他基线方法。实验结果表明，该方法在医学影像中的许多多病灶分割任务中具有巨大的应用潜力，与二元分割相比可以提供更多信息，并有助于医生未来的临床诊断。\n","permalink":"https://swimmingliu.cn/posts/papernotes/2022-priorattentionnetwork/","summary":"\u003ch1 id=\"prior-attention-network-用于医学图像中多病灶分割的预先注意网络\"\u003ePrior Attention Network: 用于医学图像中多病灶分割的预先注意网络\u003c/h1\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从\u003cstrong\u003e粗到精策略的卷积神经网络（CNN）\u003cstrong\u003e已广泛应用于该领域。然而，由于\u003c/strong\u003e组织的大小、对比度和高类间相似性的不确定性\u003c/strong\u003e，多病灶分割仍然具有挑战性。此外，\u003cstrong\u003e普遍采用的级联策略\u003c/strong\u003e对\u003cstrong\u003e硬件要求较高\u003c/strong\u003e，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它\u003cstrong\u003e遵循从粗到细的策略\u003c/strong\u003e来在医学图像中执行多病灶分割。所提出的网络通过在网络中\u003cstrong\u003e插入与病变相关的空间注意机制\u003c/strong\u003e，在单个网络中实现了\u003cstrong\u003e两个步骤的分割\u003c/strong\u003e。此外，我们还提出了\u003cstrong\u003e中间监督策略\u003c/strong\u003e，用于\u003cstrong\u003e生成与病变相关的注意力\u003c/strong\u003e来获取\u003cstrong\u003e感兴趣区域（ROI）\u003c/strong\u003e，这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：\u003cstrong\u003e肺部 CT 切片\u003c/strong\u003e中多发性\u003cstrong\u003e肺部感染的 2D 分割\u003c/strong\u003e和\u003cstrong\u003e脑 MRI 中多发性病变的 3D 分割\u003c/strong\u003e。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 \u003ca href=\"https://github.com/hsiangyuzhao/PANet\"\u003ehttps://github.com/hsiangyuzhao/PANet\u003c/a\u003e 获取\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e问题导向：\u003c/p\u003e\n\u003cp\u003e①\u003cstrong\u003e组织的大小、对比度和高类间相似性的不确定性\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e②多类别病灶分割\u003c/p\u003e\n\u003cp\u003e③\u003cstrong\u003e普遍采用的级联策略\u003c/strong\u003e对\u003cstrong\u003e硬件要求较高\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]\u003c/strong\u003e。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有\u003cstrong\u003e深度编码器\u003c/strong\u003e，可从输入图像中自动提取特征，并通过以下操作\u003cstrong\u003e生成密集预测\u003c/strong\u003e。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e这一段都可以当成经典医学图像分割的背景引入\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e然而，尽管医学分割取得了这些突破，但目前的\u003cstrong\u003e医学分割方法主要集中在病灶的二元分割上\u003c/strong\u003e，即\u003cstrong\u003e区分病灶（前景）和其他一切（背景）\u003c/strong\u003e。尽管二元分割确实有助于\u003cstrong\u003e隔离某些感兴趣区域\u003c/strong\u003e并允许对\u003cstrong\u003e医学图像进行精确分析\u003c/strong\u003e，但在某些需要\u003cstrong\u003e对病变进行多类分割的场景中，二元分割还不够\u003c/strong\u003e。与二元分割相比，由于\u003cstrong\u003e组织的类间相似性，这种情况要困难得多\u003c/strong\u003e，因为\u003cstrong\u003e不同类型的病变在纹理、大小和形状上可能相似\u003c/strong\u003e。具有从粗到细策略的级联网络已广泛应用于此类场景，例如\u003cstrong\u003e肝脏和病变的分割、脑肿瘤分割\u003c/strong\u003e[4]、[5][6]、[7]。\u003c/p\u003e\n\u003cp\u003e此类网络通常由两个独立的网络组成，其中第一个\u003cstrong\u003e网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割\u003c/strong\u003e。然而，尽管\u003cstrong\u003e级联网络\u003c/strong\u003e已广泛应用于医学图像的\u003cstrong\u003e多病灶分割\u003c/strong\u003e，但级联策略也有其缺点。由于\u003cstrong\u003e级联网络由两个独立的网络组成\u003c/strong\u003e，\u003cstrong\u003e参数量和显存占用通常是单个网络的两倍\u003c/strong\u003e，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此\u003cstrong\u003e级联网络的训练过程有时比单个网络更困难\u003c/strong\u003e，这可能导致欠拟合。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e级联网络：参数量大、容易欠拟合。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行\u003cstrong\u003e多病灶分割\u003c/strong\u003e。所提出的网络由\u003cstrong\u003e一个\u003c/strong\u003e用于\u003cstrong\u003e特征提取的编码器\u003c/strong\u003e和\u003cstrong\u003e两个分别生成病变区域注意力和最终预测的解码器组成\u003c/strong\u003e。该网络与\u003cstrong\u003e注意力机制\u003c/strong\u003e结合在一起。为了\u003cstrong\u003e减少参数大小和硬件\u003c/strong\u003e占用，我们使用\u003cstrong\u003e网络编码器的深层、语义丰富的特征\u003c/strong\u003e来\u003cstrong\u003e生成病变区域的空间注意力\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e然后，\u003cstrong\u003e编码器生成的特征\u003c/strong\u003e表示通过\u003cstrong\u003e空间注意力\u003c/strong\u003e进行细化，并将其发送到解码器以进行\u003cstrong\u003e最终的多类预测\u003c/strong\u003e。为了\u003cstrong\u003e提高分割性能并加速收敛\u003c/strong\u003e，我们还在网络结构中引入了\u003cstrong\u003e中间监督和深度监督\u003c/strong\u003e。通过这些改进，与传统的级联网络相比，所提出的网络以\u003cstrong\u003e显着降低的参数大小和计算成本\u003c/strong\u003e实现了有竞争力的结果。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\u003c/p\u003e\n\u003cp\u003e中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这项工作的贡献体现在三个方面。\u003cstrong\u003e首先\u003c/strong\u003e，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在\u003cstrong\u003e单个网络\u003c/strong\u003e中，遵循 2D 和 3D 医学图像中多病灶分割的\u003cstrong\u003e从粗到细的策略\u003c/strong\u003e。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。\u003cstrong\u003e其次\u003c/strong\u003e，我们提出了一种\u003cstrong\u003e监督空间注意力机制\u003c/strong\u003e，将\u003cstrong\u003e病变区域的注意力与网络提取的特征相结合\u003c/strong\u003e，将多病变分割分解为\u003cstrong\u003e两个更容易的阶段\u003c/strong\u003e，并且与当前\u003cstrong\u003e基于注意力的方法相比具有更好的可解释性\u003c/strong\u003e。\u003cstrong\u003e第三\u003c/strong\u003e，所提出的网络已在两个实际应用中得到验证，\u003cstrong\u003e包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割\u003c/strong\u003e。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\u003c/p\u003e\n\u003ch2 id=\"related-work\"\u003eRelated Work\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。\u003c/p\u003e","title":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络"},{"content":"地大服务器使用教程 1. 服务器环境介绍 NVIDIA RTX 3090 (24GB) NVIDIA RTX 2080 Ti (11GB) 2. 配置实验环境 2.1 Conda环境安装 每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\nConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\n大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\n2.2 Conda环境配置 （以YOLOv8为例） # 创建conda环境 名为yolov8_lyj python版本为3.9 conda create -n yolov8_lyj python=3.9 # 激活环境 conda activate yolov8_lyj # 选择合适的路径，克隆github项目代码 git clone https://github.com/ultralytics/ultralytics # 进入到项目路径下 cd ultralytics/ # 安装相关依赖包 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 2.3 准备数据集 下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\n按照算法指定的数据集格式，对数据集格式进行调整。\n​\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\n​\thttps://zhuanlan.zhihu.com/p/461488682\n2.4 开始实验 在算法中指定数据集的存放路径 （相对/绝对路径均可）\n初始化算法的参数\nbatch-size 批处理大小：每一次处理图片的个数，根据显卡内存进行调整\repochs\t迭代次数：算法总共需要训练的轮次\rworkers 载入数据进程数：每一次调用多少个进程来载入数据\rdevice 选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡 开始训练 # 运行训练代码 python mian.py (注：使用向日葵的同学，可以直接在Pycharm当中运行)\n3. 注意事项 1. 查看显卡使用情况 两种办法：\n# 第一种 使用nivida驱动直接查看\rnvidia-smi\r# 第二种 使用第三方库 gpustat动态查看\r# 先安装第三方库\rpip install gpustat -i https://pypi.tuna.tsinghua.edu.cn/simple\r# 每两秒刷新一次 动态查看显存使用情况\rwatch -n2 gpustat 2. Magic Network 使用向日葵的同学，可以使用Magic Network进行github仓库克隆、google网盘数据集下载等\n使用方法：\nexport http_proxy=http://127.0.0.1:7890\rexport https_proxy=http://127.0.0.1:7890 预祝大家科研顺利，硕果累累，offer拿到手软！！！\n博客地址： SwimmingLiu.cn\n","permalink":"https://swimmingliu.cn/posts/diary/2023-%E5%9C%B0%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"\u003ch1 id=\"地大服务器使用教程\"\u003e地大服务器使用教程\u003c/h1\u003e\n\u003ch2 id=\"1-服务器环境介绍\"\u003e1. 服务器环境介绍\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eNVIDIA RTX \u003cspan class=\"m\"\u003e3090\u003c/span\u003e \u003cspan class=\"o\"\u003e(\u003c/span\u003e24GB\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eNVIDIA RTX \u003cspan class=\"m\"\u003e2080\u003c/span\u003e Ti \u003cspan class=\"o\"\u003e(\u003c/span\u003e11GB\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://oss.swimmingliu.cn/HoBnO.png\" alt=\"image-20231120113601423\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"2-配置实验环境\"\u003e2. 配置实验环境\u003c/h2\u003e\n\u003ch3 id=\"21-conda环境安装\"\u003e2.1 Conda环境安装\u003c/h3\u003e\n\u003cp\u003e每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\u003c/p\u003e\n\u003cp\u003eConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\u003c/p\u003e\n\u003cp\u003e大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\u003c/p\u003e\n\u003ch3 id=\"22-conda环境配置-以yolov8为例\"\u003e2.2 Conda环境配置 （以YOLOv8为例）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 创建conda环境 名为yolov8_lyj python版本为3.9\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda create -n yolov8_lyj \u003cspan class=\"nv\"\u003epython\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e3.9\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 激活环境\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003econda activate yolov8_lyj\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 选择合适的路径，克隆github项目代码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit clone https://github.com/ultralytics/ultralytics\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 进入到项目路径下\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e ultralytics/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 安装相关依赖包\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"23-准备数据集\"\u003e2.3 准备数据集\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e按照算法指定的数据集格式，对数据集格式进行调整。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e​\t\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\u003c/p\u003e\n\u003cp\u003e​\t\thttps://zhuanlan.zhihu.com/p/461488682\u003c/p\u003e\n\u003ch3 id=\"24-开始实验\"\u003e2.4 开始实验\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e在算法中指定数据集的存放路径 （相对/绝对路径均可）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e初始化算法的参数\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ebatch-size  批处理大小：每一次处理图片的个数，根据显卡内存进行调整\r\nepochs\t   迭代次数：算法总共需要训练的轮次\r\nworkers     载入数据进程数：每一次调用多少个进程来载入数据\r\ndevice      选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"3\"\u003e\n\u003cli\u003e开始训练\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 运行训练代码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epython mian.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e(注：使用向日葵的同学，可以直接在Pycharm当中运行)\u003c/p\u003e","title":"地大服务器使用教程"},{"content":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023) 1. Abstract 由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。 作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。 ACC-UNet结合了卷积神经网络（ConvNets）的内在归纳偏差和Transformer的设计决策 卷积神经网络（ConvNets）的内在归纳偏差：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。 Transformer的设计决策：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。 ACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\n2.Introduction 语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\n学习一下这里的背景描述\n原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。\n从 UNet出发，然后逐步介绍他的变体（UNet++等）。随后介绍Transformer和UNet的各种结合体，为后续对比实验做铺垫。最后结合一张发展图，简明扼要描述UNet的创新历程。\n最近，鉴于 Transformer 带来的进步，研究开始重新发现 CNN 的潜力。这方面的开创性工作是“A ConvNet for the 2020s”[16]，它探讨了 Transformer 引入的各种想法及其在卷积网络中的适用性。通过逐渐融合训练协议和微观-宏观设计选择的思想，这项工作使 ResNet 模型的性能优于 Swin Transformer 模型。\n在本文中，我们在 UNet 模型的背景下提出了同样的问题。我们研究仅基于卷积的 UNet 模型是否可以与基于 Transformer 的 UNet 竞争。在此过程中，我们从 Transformer 架构中获得动力并开发了纯卷积 UNet 模型。我们提出了一种基于补丁的上下文聚合，与基于窗口的自注意力相反。此外，我们通过融合来自多个级别编码器的特征图来创新跳跃连接。对 5 个基准数据集的广泛实验表明，我们提出的修改有可能改进 UNet 模型。\n通过介绍CONvNet的重要性，引入本文重点 （纯卷积模块的UNet）。\n3. Method 3.1 A high-level view of transformers in UNet Transformers 显然在两个不同方面改进了 UNet 模型:\n1.利用自注意力的远程依赖性: Transformer 可以通过使用（窗口式）自注意力，从更大的上下文视图中计算特征。此外，他们还通过采用反向瓶颈（即增加 MLP 层中的神经元）来提高表达能力。此外，它们包含快捷连接，这有助于学习。 2.通过通道注意力的自适应多级特征组合: 基于 Transformer 的 UNet 使用通道注意力自适应地融合来自多个编码器级别的特征图。与受当前级别信息限制的简单跳跃连接相比，由于来自不同级别的各种感兴趣区域的组合，这会生成丰富的特征。\n主要就两个点：\n自主注意力在UNet的编码和解码阶段都能够快速、准确地根据上下文信息提取特征。\n自注意力机制可以用来连接encoder阶段不同stage的特征图。\n猜想：把新出的自注意力机制加在ACC-Unet是不是也会有提升呢？\n3.2 Hierarchical Aggregation of Neighborhood Context (HANC) 我们首先探索引入远程依赖性并提高卷积块的表达能力的可能性。我们仅使用逐点和深度卷积来降低计算复杂度。为了增加表达能力，我们建议在卷积块中包含反向瓶颈[16]，这可以通过使用逐点卷积将通道数从 cin 增加到 cinv = cin * inv_f ctr 来实现。由于这些额外的通道会增加模型复杂度，因此我们使用3×3深度卷积来补偿。输入特征图 xin ∈ R cin,n,m 被转换为 x1 ∈ R cinv,n,m （图 2b）\n接下来，我们希望在卷积块中模拟自注意力，其核心是将一个像素与其邻域中的其他像素进行比较[15]。通过将像素值与其邻域的平均值和最大值进行比较可以简化这种比较。因此，我们可以通过附加相邻像素特征的平均值和最大值来提供邻域比较的近似概念。因此，连续逐点卷积可以考虑这些并捕获对比视图。\n由于分层分析对图像有益[23]，我们不是在单个大窗口中计算这种聚合，而是在多个级别中分层计算，例如 2 × 2, 2 ^2 × 2^ 2 , · · · , 2^(k− 1) × 2^(k−1) 个补丁。当 k = 1 时，这将是普通的卷积运算，但是当我们增加 k 的值时，将提供更多的上下文信息，从而绕过对更大卷积核的需要。因此，我们提出的分层邻域上下文聚合通过上下文信息丰富了特征图 x1 ∈ R cinv,n,m 作为 x2 ∈ R cinv*(2k−1),n,m （图 2b），其中 ||对应于沿通道维度的串联。\n下面与Transfomer类似，我们在卷积块中包含一个快捷连接，以实现更好的梯度传播。因此，我们执行另一个逐点卷积以减少 cin 的通道数并与输入特征图相加。因此，x2 ∈ R cinv*(2k−1),n,m 变为 x3 ∈ R cin,n,m （图 2b）\n最后，我们使用逐点卷积将滤波器的数量更改为cout作为输出\n\u0026ldquo;inverted bottlenecks\u0026rdquo;（反向瓶颈） ：将Bottleneck放在卷积块的开始，而不是结束。它先使用1x1卷积来减少通道数，然后才是较大的卷积层。\n3.3 Multi Level Feature Compilation (MLFC) 接下来，我们研究多级特征组合的可行性，这是使用基于 Transformer 的 UNet 的另一个优点。\n基于 Transformer 的跳跃连接已经证明了所有编码器级别的有效特征融合以及各个解码器从编译的特征图中进行的适当过滤[24,22,27]。这是通过连接不同级别的投影令牌来执行的[22]。按照这种方法，我们调整从不同编码器级别获得的卷积特征图的大小，以使它们均衡并连接它们。这为我们提供了跨不同语义级别的特征图的概述。我们应用逐点卷积运算来总结这种表示并与相应的编码器特征图合并。整体信息和个体信息的融合通过另一个卷积传递，我们假设它用来自其他级别特征的信息丰富了当前级别特征。\n对于来自 4 个不同级别的特征 x1、x2、x3、x4，特征图可以通过多级信息来丰富（图 2d）\n这里， resizei(xj ) 是将 xj 的大小调整为 xi 的大小且 ctot = c1 + c2 + c3 + c4 的操作。此操作针对所有不同级别单独完成。因此，我们提出了另一个名为多级特征编译（MLFC）的新颖块，它聚合来自多个编码器级别的信息并丰富各个编码器特征图。该块如图 2d 所示。\n总结： 把各stage的特征联合起来进行逐点卷积，然后各stage得到新的特征信息\n3.4 ACC-UNet 因此，我们提出全卷积ACC-UNet（图2a）。我们从普通的 UNet 模型开始，并将滤波器的数量减少了一半。然后，我们用我们提出的 HANC 块替换了编码器和解码器中的卷积块。我们考虑 inv_f ctr = 3，而不是第 3 级的最后一个解码器块 (inv_f ctr = 34)，以模拟 Swin Transformer 第 3 阶段的扩展。 k = 3，最多考虑 4 × 4 个补丁，被选择用于除瓶颈级别 (k = 1) 及其旁边的级别 (k = 2) 之外的所有级别。接下来，我们通过使用残差块（图 2c）来修改跳跃连接以减少语义间隙 [11] 并堆叠 3 个 MLFC 块。所有卷积层均经过批量归一化 [12]，由 Leaky-RELU [17] 激活，并通过挤压和激励 [10] 重新校准。\n总而言之，在 UNet 模型中，我们用我们提出的 HANC 块替换了经典的卷积块，该 HANC 块执行近似版本的自注意力，并修改了与 MLFC 块的跳跃连接，MLFC 块考虑来自不同编码器级别的特征图。所提出的模型有 16.77 M 个参数，比普通 UNet 模型大约增加了 2M。\n4.Experiments 4.1 Datasets 为了评估 ACC-UNet，我们在 5 个跨不同任务和模式的公共数据集上进行了实验。我们使用 ISIC-2018 [6,21]（皮肤镜检查，2594 张图像）、BUSI [3]（乳腺超声，使用类似于 [13] 的 437 张良性图像和 210 张恶性图像）、CVC-ClinicDB [4]（结肠镜检查，612 张图像） ）、COVID [1]（肺炎病灶分割，100 张图像）和 GlaS [20]（腺体分割，85 张训练图像和 80 张测试图像）。所有图像和掩模的大小都调整为224×224。对于GlaS数据集，我们将原始测试分割作为测试数据，对于其他数据集，我们随机选择20％的图像作为测试数据。\n剩余的 60% 和 20% 图像用于训练和验证，并使用不同的随机改组重复实验 3 次。\nISIC-2018、BUSI、 CVC-ClinicDB、COVID、GlaS 五个数据集\n4.2 Comparison Experiments 4.3 Ablation Study 4.4 Qualitative Results ","permalink":"https://swimmingliu.cn/posts/papernotes/2023-acc-unet/","summary":"\u003ch1 id=\"acc-unet-a-completely-convolutional-unet-model-for-the-2020s-miccai2023\"\u003eACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023)\u003c/h1\u003e\n\u003ch2 id=\"1-abstract\"\u003e1. Abstract\u003c/h2\u003e\n\u003cp\u003e由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。\n作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。\nACC-UNet结合了\u003cstrong\u003e卷积神经网络（ConvNets）的内在归纳偏差\u003c/strong\u003e和\u003cstrong\u003eTransformer的设计决策\u003c/strong\u003e\n\u003cstrong\u003e卷积神经网络（ConvNets）的内在归纳偏差\u003c/strong\u003e：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。\n\u003cstrong\u003eTransformer的设计决策\u003c/strong\u003e：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。\nACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\u003c/p\u003e\n\u003ch2 id=\"2introduction\"\u003e2.Introduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e学习一下这里的背景描述\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。\u003c/p\u003e","title":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023)"},{"content":"(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络 Abstract 准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用逐元素加法或串联在解码器中逐步融合不同级别的特征。然而，这两种操作都容易产生大量冗余信息，从而削弱不同级别特征之间的互补性，导致病灶定位不准确和边缘模糊。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本减法单元（SU）来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器提供像素级和结构级差异信息。\n然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\n没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\n两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\nIntroduction 作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\n先说医学分割在医学领域重要\u0026hellip;(balabala) 然后当前领域存在xxx挑战\u0026hellip;(balabala)\n这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\n一般来说，编码器中不同级别的特征有不同的特征。高级别具有更多的语义信息，有助于定位对象，而低级别具有更详细的信息，可以捕捉对象的微妙边界。解码器利用特定级别和跨级别特征来生成最终的高分辨率预测。然而，上述方法直接使用逐元素加法或串联来融合来自编码器的任意两级特征并将它们传输到解码器。这些简单的操作并没有更多地关注不同层次之间的差异信息。这一缺点不仅会产生冗余信息来稀释真正有用的特征，还会削弱特定于级别的特征的特性，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于感受野有限，单尺度卷积核很难捕获大小变化物体的上下文信息。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的语义上下文和纹理细节。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取层内多尺度信息。然而，类似ASPP的多尺度卷积模块会产生许多额外的参数和计算。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\n在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于每对相邻的级别特征。 SU突出了特征之间有用的差异信息，并消除了冗余部分的干扰。其次，我们借助所提出的多尺度减法模块收集极端多尺度信息。\n对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们聚合特定于级别的特征和多路径跨级别差分特征，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\n多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\n（也就是说可以用这种办法替换注意力机制）\nRELATED WORK Medical Image Segmentation Network 根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了长连接和短连接，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，注意力门嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不同形状和大小的目标结构。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\n医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。\n我们可以看到，医学通用方法通常针对通用挑战（即丰富的特征表示、多尺度信息提取和跨级别特征聚合）。并且，医学特异性方法根据当前器官或病变的特征提出有针对性的解决方案，例如设计一系列注意力机制、边缘增强模块、不确定性估计等。然而，通用医学模型和医学特异性模型都依赖于通过大量的加法或串联操作来实现特征融合，削弱了互补特征之间的特殊性部分。我们提出的多尺度减法模块自然专注于提取差异信息，从而为解码器提供有效的目标特征。\n主要是说大部分特征融合都是用加法/乘法/串联实现的，但是减法可以削弱互补特征之间的特殊性部分。所以多尺度减法模块提取差异信息，然后再用加法进行特征融合。\nMulti-scale Feature Extraction 尺度线索在捕捉对象的上下文信息中发挥着重要作用。受到被广泛验证为有效且理论上合理的框架的尺度空间理论的启发，越来越多的多尺度方法被提出。与单尺度特征相比，多尺度特征有利于解决自然发生的尺度变化。这一特性可以帮助医学分割模型感知不同尺度的病变。根据形式，当前基于多尺度的方法可以大致分为两类，即层间多尺度结构和层内多尺度结构。前者基于特征编码器提取的不同尺度的特征，并在解码器中逐步聚合它们，例如U形[1]、[2]、[4]、[9]-[11]、[37] ，[38]架构。后者通常配备多尺度可插拔模块，如ASPP [16]、DenseASPP [17]、FoldASPP [6]和PAFEM [12]，构建具有不同扩张率的并行多分支卷积层，以获得丰富的组合感受野。与它们不同的是，我们通过同时引入层间和层内多尺度，提出了具有极端多尺度信息的多尺度减法模块中的多尺度。并且，层内多尺度减法单元专注于挖掘从像素到像素到区域到区域的特征对的自差分性质。与单尺度操作相比，整个过程非常高效，不需要额外的参数。\n多尺度减法模块可以超越其他卷积类办法的多尺度特征信息提取办法\nLoss Method 图像分割中的大多数损失函数都是基于交叉熵或重合度量。传统的交叉熵损失对类别信息一视同仁。 Long等人[24]提出了每个类别的加权交叉熵损失（WCE），以抵消数据中的类别不平衡。 Lin等人[39]引入了困难样本和简单样本的权重来提出焦点损失。 Dice loss[40]被提出作为V-Net中重合测量的损失函数，可以有效抑制类别不平衡带来的问题。 Tversky 损失[41]是 Dice 损失的正则化版本，用于控制准确率和召回率对损失函数的贡献。 Wong等人[42]通过Dice损失和WCE损失的加权求和提出指数对数损失（EL Loss）来提高小结构物体的分割精度。\nTaghanaki等人[43]发现单独使用基于重叠的损失函数存在风险，并提出comomoloss将Dice损失作为正则化项与WCE损失相结合来处理输入输出不平衡的问题。\n虽然这些各种各样的损失函数在不同层次上有不同的效果，但手动设计这些复杂的函数确实费时费力。为此，我们提出了自动且全面的分割损失结构，称为LossNet。\nLossNet权重就0.1 （ 感觉这个是为了创新而创新）\nMETHOD Encoder: Res2Net + Connection: MMSB + Decoder: Plus\nMulti-scale in Multi-scale Subtraction Module 我们使用 FA 和 FB 来表示相邻级别的特征图。\n它们都已被 ReLU 操作激活。我们定义一个基本减法单位（SU）：\n其中是逐元素减法运算，然后计算绝对值，Conv(·) 表示卷积层。直接对元素位置特征进行单尺度减法只是为了建立孤立像素级别上的差异关系，没有考虑病灶可能具有区域聚类的特征。与带有单尺度减法单元的MSNet MICCAI版本[27]相比，我们设计了一个强大的层内多尺度减法单元（MSU），并将MSNet改进为M2SNet。如图3所示，我们利用大小为1×1、3×3和5×5的固定全一权重的多尺度卷积滤波器根据像素-像素和区域区域模式计算细节和结构差异值。使用具有固定参数的多尺度滤波器不仅可以直接捕获匹配空间位置处的初始特征对之间的多尺度差异线索，而且可以在不引入额外参数负担的情况下实现高效训练。因此，M2SNet可以保持与MSNet相同的低计算量，并获得更高精度的性能。整个多尺度减法过程可以表述为：\n其中 Filter(·) n×n 表示大小为 n × n 的完整滤波器（卷积）。 MSU可以捕获FA和FB的互补信息，并突出它们从纹理到结构的差异，从而为解码器提供更丰富的信息。\n为了获得跨多个特征级别的高阶互补信息，我们水平和垂直连接多个MSU来计算一系列具有不同阶数和感受野的差分特征。多尺度减法模块中多尺度的细节可以在图2中找到。我们聚合了相应级别和任意级别之间的特定尺度特征（MSi 1 ）和跨尺度差分特征（MSi n6=1）。其他级别生成互补增强特征（CEi）。这个过程可以表述如下：\n最后，所有CEi参与解码，然后对息肉区域进行分割。\n这里就是介绍一下MSU\nLossNet 在所提出的模型中，总训练损失可以写为：\n其中L w IoU和L w BCE表示加权IoU损失和二元交叉熵（BCE）损失，它们已在分割任务中广泛采用。我们使用与[4]、[44]、[45]中相同的定义，它们的有效性已在这些工作中得到验证。与它们不同的是，我们额外使用LossNet来进一步优化从细节到结构的分割。\n具体来说，我们使用 ImageNet 预训练分类网络，例如 VGG-16，分别提取预测和地面实况的多尺度特征。然后，它们的特征差异计算为损失 Lf ：\n令 F i P 和 F i G 分别表示从预测和地面实况中提取的第 i 层特征图。 l i f 计算为其欧几里德距离（L2-Loss），该距离在像素级别进行监督：\n从图4中可以看出，低层特征图包含丰富的边界信息，高层特征图描述位置信息。因此，LossNet可以在特征层面产生全面的监督。\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-m2snet/","summary":"\u003ch1 id=\"2023-m2snet-新颖多尺度模块--智能损失函数--通用图像分割sota网络\"\u003e(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络\u003c/h1\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用\u003cstrong\u003e逐元素加法或串联在解码器中逐步融合不同级别的特征\u003c/strong\u003e。然而，这两种操作都\u003cstrong\u003e容易产生大量冗余信息\u003c/strong\u003e，从而\u003cstrong\u003e削弱不同级别特征之间的互补性\u003c/strong\u003e，导致\u003cstrong\u003e病灶定位不准确和边缘模糊\u003c/strong\u003e。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本\u003cstrong\u003e减法单元（SU）\u003cstrong\u003e来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器\u003c/strong\u003e提供像素级和结构级差异信息\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\u003c/p\u003e\n\u003cp\u003e没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e先说医学分割在医学领域重要\u0026hellip;(balabala)  然后当前领域存在xxx挑战\u0026hellip;(balabala)\u003c/p\u003e\n\u003cp\u003e这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e一般来说，编码器中不同级别的特征有不同的特征。\u003cstrong\u003e高级别具有更多的语义信息\u003c/strong\u003e，有助于定位对象，而低级别具有\u003cstrong\u003e更详细的信息\u003c/strong\u003e，可以捕捉对象的\u003cstrong\u003e微妙边界\u003c/strong\u003e。解码器利用特定级别和跨级别特征来生成最终的\u003cstrong\u003e高分辨率预测\u003c/strong\u003e。然而，上述方法\u003cstrong\u003e直接使用逐元素加法或串联来融合来自编码器的任意两级特征\u003c/strong\u003e并将它们传输到解码器。这些简单的操作并没有更多地关注\u003cstrong\u003e不同层次之间的差异信息。\u003cstrong\u003e这一缺点不仅会产生\u003c/strong\u003e冗余信息来稀释真正有用的特征\u003c/strong\u003e，还会\u003cstrong\u003e削弱特定于级别的特征的特性\u003c/strong\u003e，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于\u003cstrong\u003e感受野有限\u003c/strong\u003e，单尺度卷积核很难捕获大小变化物体的\u003cstrong\u003e上下文信息\u003c/strong\u003e。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的\u003cstrong\u003e语义上下文和纹理细节\u003c/strong\u003e。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取\u003cstrong\u003e层内多尺度信息\u003c/strong\u003e。然而，类似ASPP的多尺度卷积模块会产生\u003cstrong\u003e许多额外的参数和计算\u003c/strong\u003e。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\u003c/p\u003e\n\u003cp\u003e在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于\u003cstrong\u003e每对相邻的级别特征\u003c/strong\u003e。 SU突出了\u003cstrong\u003e特征之间有用的差异信息，并消除了冗余部分的干扰\u003c/strong\u003e。其次，我们借助所提出的\u003cstrong\u003e多尺度减法模块\u003c/strong\u003e收集\u003cstrong\u003e极端多尺度信息\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们\u003cstrong\u003e聚合特定于级别的特征\u003c/strong\u003e和\u003cstrong\u003e多路径跨级别差分特征\u003c/strong\u003e，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元\u003cstrong\u003e改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数\u003c/strong\u003e。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\u003c/p\u003e\n\u003cp\u003e（也就是说可以用这种办法替换注意力机制）\u003c/p\u003e\n\u003ch2 id=\"related-work\"\u003eRELATED WORK\u003c/h2\u003e\n\u003ch3 id=\"medical-image-segmentation-network\"\u003eMedical Image Segmentation Network\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了\u003cstrong\u003e长连接和短连接\u003c/strong\u003e，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，\u003cstrong\u003e注意力门\u003c/strong\u003e嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不\u003cstrong\u003e同形状和大小的目标结构\u003c/strong\u003e。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\u003c/p\u003e\n\u003cp\u003e医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。\u003c/p\u003e","title":"M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation"},{"content":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation 1. Abstract 目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\n作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\n创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\n1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\n2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\n2. Introduction 背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\n相同方式可引入脑瘤、肺癌。\n为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\nAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\n先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。\n提出问题：医疗环境中计算资源的限制，复杂模块难以落地 \u0026mdash;\u0026gt; 解决办法：轻量化模型\n当前轻量化发展历程 \u0026mdash;\u0026gt; 轻量化的模型分割效果不好 \u0026mdash;\u0026gt; EGE-Unet 轻量+分割能力强\n具体来说，EGE-UNet 利用两个关键模块：群组多轴 Hadamard 产品注意力模块（GHPA）和群组聚合桥模块（GAB）。\n一方面，由于多头自注意力机制（MHSA），最近基于 ViT [7] 的模型已经显示出前景。 MHSA将输入划分为多个head，并在每个head中计算self-attention，这使得模型能够从不同的角度获取信息，整合不同的知识，提高性能。尽管如此，MHSA 的二次复杂度极大地增加了模型的大小。因此，我们提出了具有线性复杂度的哈达玛产品注意力机制（HPA）。HPA 采用可学习的权重，并使用输入执行哈达玛乘积运算以获得输出。随后，受到 MHSA 中多头模式的启发，我们提出了 GHPA，它将输入分为不同的组，并在每个组中执行 HPA。然而，值得注意的是，我们在不同组的不同轴上进行HPA，这有助于进一步从不同的角度获取信息。\n另一方面，对于GAB，由于医学图像中分割目标的大小和形状不一致，因此获得多尺度信息至关重要[19]。因此，GAB基于组聚合融合不同大小的高层和低层特征，并额外引入掩模信息来辅助特征融合。通过将上述两个模块与UNet相结合，我们提出了EGE-UNet，它以极低的参数和计算量实现了出色的分割性能。与以前仅注重提高性能的方法不同，我们的模型还优先考虑现实环境中的可用性。图 1 显示了 EGEUNet 与其他网络的清晰比较。\n具体介绍为什么引入两个创新模块（GHPA、GAB）、以及模块是基于什么论文。（模块背景+创新方法）\n(1)提出了GHPA和GAB，前者有效地获取和集成多视角信息，后者接受不同尺度的特征，以及用于高效多尺度特征融合的辅助掩模。\n(2)我们提出了EGEUNet，这是一种专为皮肤病变分割而设计的极其轻量级的模型。\n(3) 我们进行了广泛的实验，证明了我们的方法在以显着降低的资源需求实现最先进性能方面的有效性。\n主要贡献：（1）写模块作用 （2）写整体网络优势 （3）实验效果\n3. Method 3.1EGE-Unet网络结构 EGE-UNet由对称编码器-解码器部分组成的 U 形架构之上。\n编码器由六级组成，每级通道数为{8,16,24,32,48,64}。解码器同理\n前三个阶段采用内核大小为 3 的普通卷积，后三个阶段利用提出的 GHPA 从不同的角度提取表示信息。\n与 UNet 中的简单跳跃连接相比，EGE-UNet 在编码器和解码器之间的每个阶段都采用了 GAB。\n利用深度监督生成不同规模的掩模预测，这些预测用于损失函数并作为 GAB 的输入之一。\n通过集成这些高级模块，EGE-UNet 显着减少了参数和计算负载，同时与之前的方法相比增强了分割性能。\n3.2 GHPA (Group multi-axis Hadamard Product Attention module) 为了克服 MHSA 带来的二次复杂度问题，我们提出了具有线性复杂度的 HPA。给定输入 x 和随机初始化的可学习张量 p，首先使用双线性插值来调整 p 的大小以匹配 x 的大小。然后，我们在 p 上采用深度可分离卷积（DW）[10][20]，然后在 x 和 p 之间进行哈达玛乘积运算以获得输出。然而，仅利用简单的HPA不足以从多个角度提取信息，导致结果不理想。受 MHSA 中多头模式的启发，我们引入了基于 HPA 的 GHPA，如算法 1 所示。我们将输入沿通道维度平均分为四组，并在高度-宽度、通道-高度和通道上执行 HPA - 分别为前三组的宽度轴。对于最后一组，我们只在特征图上使用DW。最后，我们沿着通道维度连接四组，并应用另一个数据仓库来整合不同角度的信息。请注意，DW 中使用的所有内核大小均为 3。\n首先对输入的特征分为四组进行处理：高度-宽度、通道-高度、通道-宽度、深度可分离卷积\n然后连接4组特征，进行可分离卷积融合特征。\n具体过程：\n第一步，按通道数将输入张量分为四组。（x1, x2, x3, x4）\n设置初始化三个全一张量，分别为高度-宽度、通道-高度、通道-宽度（Pxy, Pzx, Pzy）。\n第二步，将 x1, x2, x3 的对应切片分别使用双线插值法（bilinear）在Pxy, Pzx, Pzy中进行插值。\n第三步，对插值后的Pxy, Pzx, Pzy，进行深度可分离卷积，然后分别和x1, x2, x3进行哈达玛乘积\n第四步，连接4组特征信息，然后经过深度可分离卷积融合特征。\n3.3 GAB (Group Aggregation Bridge module) 多尺度信息的获取被认为对于密集预测任务（例如医学图像分割）至关重要。因此，如图 3 所示，我们引入了 GAB，它接受三个输入：低级特征、高级特征和掩码。首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征的大小，以匹配低层特征的大小。其次，我们沿通道维度将两个特征映射分为四组，并将一组低级特征与一组高级特征连接起来，以获得四组融合特征。对于每组融合特征，掩码被连接起来。接下来，将内核大小为3和不同扩张率{1,2,5,7}的扩张卷积[25]应用于不同的组，以提取不同尺度的信息。最后，将四组沿通道维度连接起来，然后应用内核大小为 1 的普通卷积，以实现不同尺度的特征之间的交互。\nGAB模块作用： 将高级特征、低级特征、低级特征的预测掩码进行特征融合，作为新的输入特征进行解码。\n具体过程： 高级特征、低级特征、低级特征的预测掩码 (xh、xl 、Mask)\n首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征 (xh) 的大小，以匹配低层特征 (xl) 的大小。\n其次，沿通道维度将两个特征映射分为四组。（对应不同空洞卷积的扩张率：d1 = 1, d2 = 2, d3 = 5, d4 = 7）\n并将每一组的低级特、高级特征和掩码连接起来，总共四组融合特征。\n最后，将四组特征进行连接，并进行1x1卷积得到输出。\n3.4 Loss Function 在本研究中，由于不同的GAB需要不同尺度的掩模信息，因此采用深度监督来计算不同阶段的损失函数，以生成更准确的掩模信息。我们的损失函数可以表示为方程（1）和（2）。其中 Bce 和 Dice 表示二元交叉熵和dice损失。 λi是不同阶段的权重。在本文中，我们默认将i=0到i=5之间的λi设置为1、0.5、0.4、0.3、0.2、0.1。\n分为6个阶段，逐一计算每个阶段的损失。然后按照权重对损失进行求和。\n4.Experiments 4.1 Datasets and Implementation details 为了评估我们模型的有效性，我们选择了两个公共皮肤病变分割数据集，即 ISIC2017 [1][3] 和 ISIC2018 [2][6]，分别包含 2150 个和 2694 个皮肤镜图像。与之前的研究[19]一致，我们以 7:3 的比例将数据集随机划分为训练集和测试集。\nEGE-UNet是由Pytorch[17]框架开发的。所有实验均在单个 NVIDIA RTX A6000 GPU 上执行。图像被归一化并调整大小为 256×256。我们应用各种数据增强，包括水平翻转、垂直翻转和随机旋转。 AdamW [13] 用作优化器，以 0.001 的学习率初始化，CosineAnnealingLR [12] 用作调度器，最大迭代次数为 50，最小学习率为 1e-5。总共训练了 300 个 epoch，批量大小为 8。为了评估我们的方法，我们采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为指标，并进行 5 次训练\n​\t在公共皮肤病变分割数据集（ISIC2017 和 ISIC2018 ）进行对比实验，在ISIC2018进行消融实验\n采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为评估指标\n4.2 Comparison Experiments 4.3 Ablation Experiments 4.4 Qualitative Comparisons 5. ConClusions 在本文中，我们提出了两个高级模块。我们的 GHPA 使用一种新颖的 HPA 机制将自注意力的二次复杂度简化为线性复杂度。它还利用分组来充分捕获来自不同角度的信息。我们的 GAB 融合了低级和高级特征，并引入了一个掩模来集成多尺度信息。基于这些模块，我们提出了用于皮肤病变分割任务的 EGE-UNet。实验结果证明了我们的方法在显着降低资源需求的情况下实现最先进的性能的有效性。我们希望我们的工作能够激发医学图像界对轻量级模型的进一步研究。\n作者提出的EGE-UNet实现了轻量、准确的皮肤病变分割任务\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-ege-unet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","summary":"\u003ch1 id=\"ege-unet-an-efficient-group-enhanced-unet-for-skin-lesion-segmentation\"\u003eEGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation\u003c/h1\u003e\n\u003ch2 id=\"1-abstract\"\u003e1. Abstract\u003c/h2\u003e\n\u003cp\u003e目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\u003c/p\u003e\n\u003cp\u003e作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\u003c/p\u003e\n\u003cp\u003e创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\u003c/p\u003e\n\u003cp\u003e1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\u003c/p\u003e\n\u003cp\u003e2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\u003c/p\u003e\n\u003ch2 id=\"2-introduction\"\u003e2. Introduction\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e相同方式可引入脑瘤、肺癌。\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。\u003c/p\u003e","title":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"}]