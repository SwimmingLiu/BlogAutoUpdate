[{"content":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。\n2）级联网络在医学分割中的应用：级联网络已广泛应用于正常组织和病变的分割以及不同类型病变的分割，包括肝脏病变、脑肿瘤、硬化病变和前列腺癌的分割[4] ，[15][5]，[16]。例如，Awad 等人[17]提出了一个名为 CU-Net 的级联框架，用于在 CT 扫描中对肝脏和病变进行自动分割。他们还提供了可以指导临床治疗的有用信息和解释。 Xi等人[18]提出了一种级联U-ResNets，它遵循一种新颖的垂直级联策略，并在他们的工作中评估了不同类型的损失函数。除了肝脏病灶分割之外，级联策略在 BraTS 挑战中也很流行。例如，BraTS 2019挑战赛的Top-2解决方案[6]、[7]都是具有不同级联策略的级联网络。\n3）神经网络中的注意力：注意力机制受到人类感知和视觉认知的启发，并已普遍应用于计算机视觉任务中[19]，[20][11]，[21]。计算机视觉任务中的注意力机制是在神经网络提取的特征表示上生成空间或通道权重图。例如，Woo等人[20]开发了一个卷积块注意力模块（CBAM）来引入一种融合注意力机制，其中包括通道注意力和空间注意力。这种注意力模块可以插入到常用的分类或分割网络中。Oktay等人[11]在Attention U-Net中提出了一种新颖的注意力门，用于细化网络编码器提取的特征表示，以促进网络专注于ROI。最近，首先在自然语言处理任务中提出的变压器[22]已被引入到医学分割任务中。例如，Wang 等人 [23] 提出了一种 TransBTS，用于从多模态脑 MRI 中执行脑肿瘤分割。\n综上所述，注意力机制已被广泛用于突出ROI并抑制不相关信息，但目前基于注意力的方法的研究并没有对注意力如何产生以及网络为何关注某些区域提供清晰的解释，这使得限制了注意力机制的可解释性。\nMethod 在本节中，我们将详细介绍所提出的先验注意网络架构。在第一部分中，我们将概述所提议的网络。然后，我们相应地提供有关具有中间监督、参数化跳跃连接和具有深度监督的多类解码器的所提出的注意引导解码器的详细信息。\n3.1 . Overview of Network Architecture 基本上，我们提出的网络是基于 U-Net [3] 架构进行修改的，该架构具有 U 形拓扑以及编码器和解码器之间的跳跃连接。在提出的先验注意网络中，一种新颖的注意引导解码器模块被集成到网络的跳跃连接中，以通过空间注意来细化特征表示。网络中还引入了一种新颖的参数化跳跃连接，以指导网络学习普通特征图和精炼特征图之间的比率。注意力引导解码器从编码器获取丰富的语义特征并生成空间注意力图来指导接下来的多类分割。为了产生与投资回报率相关的注意力，框架中使用了中间监督策略。然后将细化的特征图发送到多类解码器以进行最终的密集预测。\n多类解码器采用深度监督策略以获得更好的收敛性并提高分割性能。这种网络拓扑通过注意力引导解码器生成的注意力图在单个网络中实现了传统级联网络的两个步骤。组网方案如图2所示。\n3.2 Attention Guiding Decoder 在典型的级联网络中，分割的第一步是执行粗分割并找到输入图像中的 ROI。在提出的先验注意网络中，我们提出了一个注意引导解码器来执行该过程。所提出的注意力引导解码器被集成到网络中以生成与 ROI 相关的注意力图，然后利用这些图来细化特征表示并提高多类分割性能。\n1）模块拓扑：所提出的注意力引导解码器的基本拓扑基于FCN [2]中提出的特征融合。从网络解码器最深三层提取的特征表示被馈送到该模块。由于特征图的空间大小不同，因此首先执行线性插值以对特征图进行上采样。然后对特征进行压缩，抑制通道维度中的不相关信息，降低计算成本。然后将压缩后的特征分别在通道维度上连接起来以进行特征融合。(torch.cat) 最后，融合三个特征图以获得最终的预测。\n为了简单起见，我们使用 2D 分割来说明注意力图的计算。我们使用 Xi ∈ R Ci × Hi × Wi,i ∈ (3, 4, 5) 表示从网络编码器提取的特征图，其中 X5 表示最深的特征。特征压缩和融合计算如下：\n其中Z5 ∈ R C4 × H4 × W4表示X5和X4的融合特征，Z4 ∈ R C3 × H3 × W3表示X4和X3的融合特征，Wc5 ∈RC5×C4和Wc4 ∈RC4×C3表示相应的压缩卷积， W4 ∈ RC4×C4 表示融合 X5 和 X4 的融合卷积，⊕ 表示特征串联。\n注意力引导解码器的输出计算如下：\n其中 W3 ∈ R C3×C3 表示融合 X4 和 X3 的融合卷积，Wout ∈ R C3 × 1 表示输出卷积，σ 分别表示 Sigmoid 激活。\n2）中间监督：计算机视觉中的传统注意力机制自动生成注意力图，但注意力生成的过程通常是人类无法解释的，并且网络关注的区域可能与人类关注的区域不同。\n这种差距会限制注意力机制的性能和可解释性，有时还会导致网络容量的恶化。为了解决这些问题，我们在网络中引入了中间监督策略。在遵循从粗到细的方式的多病灶分割任务中，我们首先生成一个二元Ground Truth，其中前景表示所有类型的病灶，背景表示其他一切。在具有 C 种病变的多病变分割任务中，我们使用 Gi,i ∈ (1,\u0026hellip;,C) 表示第 i 类病变的二元基本事实，其中前景表示特定病变背景代表其他一切。二进制真实值 Gb 计算如下：\n然后利用二元损失函数来计算二元真实值 yb 和注意力引导解码器生成的注意力图 Y 之间的二元损失 l：\n其中 Lb 表示二元损失函数。然后利用计算出的损失 l 来监督注意力引导解码器的参数更新。\n这个地方的中间监督主要是 要让中间的注意力机制起作用，不能随便生成。\n通过引入中间监督，生成的注意力由输入图像的二元真实值进行监督。这样，网络被迫学习多病灶分割任务的分解，即首先提取病灶区域，然后对病灶区域进行细粒度分类。这种分解降低了多病灶分割的难度，并且与当前在“黑匣子”中生成注意力的基于注意力的医学分割方法相比，具有更好的可解释性。\n3.3 Parameterized Skip Connections 跳跃连接已广泛应用于流行的卷积网络中，包括U-Net [3]、ResNet [24]等。受[11]的启发，我们建议将注意力图集成到连接网络编码器和多网络的跳跃连接中，形成多级解码器。在跳跃连接中，我们还引入了额外的残差路径来恢复普通特征图并进一步提高分割性能。与传统的残差路径相比，残差路径的幅值因子αi,i ∈ (1, 2,…, 5)被设置为网络的可学习参数，并在反向传播过程中更新。我们相信这样的设置可以为网络增加额外的非线性能力并增强跳过连接的有效性。\n我们使用 Fi,i ∈ (1, 2,\u0026hellip;, 5) 表示来自网络编码器的普通特征图，Y 表示注意力图，精炼后的特征图 Fri,i ∈ (1, 2,. .., 5) 多类解码器接收的信息计算如下：\n然后将细化的特征图发送到多类解码器以进行最终的多类预测。\n3.4 Multi-Class Decoder With Deep Supervision U形分割网络中的解码器用于接收编码器发送的特征图，随着解码器中的特征通道数量的减少和空间分辨率的增加，分割性能逐步细化。然而，随着网络变深，最深的解码器块变得难以训练，这可能会限制最终的分割性能。深度监督策略已经被提出来训练深度卷积网络[25]、[26]。在所提出的先验注意网络中，辅助预测是从不同级别的解码器块中提取的，并使用相同的基本事实进行监督。我们使用 Pi,i ∈ (1, 2, 3) 表示来自多类解码器的辅助预测，Pm 表示最终的多类预测，g 表示真实值，Lm 表示多类损失函数。最终的多类损失计算如下：\n所提出的多类解码器的解码器块也与当前网络设置具有共同的设置，即卷积层、归一化层和非线性激活单元的堆栈。\n总结一下：\n① 注意力机制 + 中间监督：最后三层特征融合 + 这一部分做深度监督（原来这样也叫创新）\n② 跳跃连接的部分加了一个α因子 （感觉像权重一样的东西 ）\n③ 多阶段的深度监督 （这个就算一个trick吧，大家都在用）， 不过这里变成了多类别\nExperiments 4.1 Strong Baselines and Evaluation Metrics 为了研究网络架构的性能差异，我们将所提出的先验注意网络与医学分割中最流行的方法进行了比较，包括 U-Net [3]、Attention U-Net [11] 和级联 U-Net，在两个 2D 中和 3D 分割任务。值得注意的是，与他们论文中提出的原始版本相比，基线方法根据网络拓扑方面的某些任务进行了修改和优化，以获得性能提升。我们将残差连接[24]、批量归一化[30]和来自 ImageNet 的预训练编码器引入到 2D COVIDlesion 分割任务的基线方法中，并且我们还将残差连接、实例归一化和 PReLU 激活引入到 3D 脑肿瘤分割任务中。除了网络拓扑之外，基线方法与所提出的先验注意网络共享相同的数据增强和训练配置。\n对于 COVID-19 病变的 2D 分割，我们使用 Dice 指数、精度分数和召回分数来评估所提出的网络的性能。 Dice指数是一种用来衡量两个样本相似度的统计量，已广泛用于分割算法的评估。精确率衡量的是实际正确的阳性识别的比例，召回率衡量的是算法对阳性样本的敏感度。对于 BraTS 2020 挑战赛的 3D 分割，在在线门户上进行评估，并根据 Dice 指数和 95% Hausdorff 距离（HD）对算法进行排名。\n我们使用 G 表示ground truth，P 表示密集预测，TP 表示正确预测的正样本，FP 表示错误预测的正样本，TN 表示正确预测的副样本，FN 表示错误预测的副样本。这些指标的计算方式如下：\n这个HD（Hausdorff ）也是一种评估分割结果的方式 （alright 又多了一种指标）\n4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割 1）数据：由于可用的开源COVID-19 CT分割数据集通常很小，因此利用两个独立的公开可用数据集，即COVID-19 CT分割数据集[27]和CC-CCII数据集[28]来验证所提出的方法二维分割任务中的方法。第一个数据集包含来自 40 多名患者的 100 个轴向 CT 切片，这些切片已重新缩放至 512 × 512 像素并进行灰度化。所有切片均由放射科医生用不同的标签进行分割，以识别不同类型的肺部感染。第二个数据集由 150 名 COVID-19 患者的 750 张 CT 切片组成，这些切片被手动分割为背景、肺野、毛玻璃混浊和实变。由于并非所有 750 个切片都包含病变，我们最终使用了 150 名患者的 549 个带注释的切片。对于这两个数据集，利用 5 倍交叉验证来评估所提出模型的性能。\n折叠之间的数据根据患者进行分割，以避免潜在的数据泄漏。最终的标签和分割图包含 3 个类别，包括背景、毛玻璃不透明度 (GGO) 和合并 (CON.)。\n2）实现细节：a）模型设置和损失函数：对于预训练网络编码器，我们采用来自ImageNet的预训练ResNeXt-50（32 × 4d）[31]作为基线方法和所提出的先验注意网络的编码器。对于解码器中的上采样，采用双线性插值，比例因子为2。对于中间监督和级联U-Net第一阶段的二元损失函数，我们采用Dice Loss [13]和Focal的线性组合损失[32]作为损失函数。对于最终输出的多类损失函数，我们采用Focal Tversky Loss [33]作为损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 16.04 服务器上使用 PyTorch 1.7.1 框架实现的。我们使用 NVIDIA RTX 2080 Ti GPU 来加速我们的训练过程。在我们的训练过程中使用Albumentations [34] 进行数据增强，以减少过度拟合并提高泛化能力。首先，将所有输入图像重新缩放为 560 × 560，然后进行随机亮度和对比度偏移以及随机仿射变换。然后将图像随机裁剪为 512 × 512，然后进行随机弹性变换，最后输入网络。该模型由 Adam 优化器优化，β1 = 0.9、β2 = 0.999、γ = 1e − 8。L2 正则化也用于减少过度拟合。我们将模型权重衰减设置为 1e − 5。初始学习率设置为 1e −4 并降低，然后采用余弦退火策略。批量大小设置为 4，模型训练 40 轮。该模型使用 5 倍交叉验证进行评估。\n3）定量结果：我们在两个数据集上的实验中不同模型的详细比较分别如表一和表二所示。如图所示，我们提出的网络在毛玻璃不透明度和固结的 Dice 分数方面优于 U-Net、Attention U-Net。所提出的 PANet 以更少的参数和计算成本实现了与级联 U-Net 竞争的结果。由于这些模型在模型主干和训练策略上是相同的，很明显，所提出的注意力引导解码器、中间监督和深度监督的组合对分割性能有很大贡献。注意力引导解码器的利用有助于模型更准确地检测感染组织并生成与感染相关的注意力图，从而有利于解码器中的多类分割。\n此外，中间监督和深度监督的引入促进了网络的收敛，这也有助于提高性能。\n好好好，这哥们儿，睁着眼睛说瞎话是吧（这Unet明明比你低啊，精度也没差多少啊）\n4）定性结果：不同模型在 2D COVID-19 切片上的视觉比较如图 4 所示。由于模型在 Dice 分数方面非常接近，因此乍一看这些模型的表现相似。但与 U-Net 和 Attention U-Net 相比，所提出的 PANet 在实变和微小病变的分割上表现更好。\n与 U-Net 和 Attention U-Net 相比，PANet 产生更准确的分割掩模，并且与 Cascaded U-Net 相比，所提出的网络以更少的计算成本实现了有竞争力的结果。\n额 只要定量结果上去了，好像定性结果都是挑好的说吧？\n) 消融实验：进行了几次消融实验来评估我们模型中组件的性能，如表 III 所示。 a）具有深度监督的多类解码器的有效性：为了探索深度监督策略的贡献，我们建立了两个实验：No.1（U-Net）和No.2（U-Net + DS）。表三的结果表明，深度监督在一定程度上对绩效有所贡献。\nb）注意力引导解码器的有效性：我们通过构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中所提出的注意力引导解码器的有效性。如表III所示，与实验1相比，注意力引导解码器的引入提供了显着的性能提升。这表明注意力引导解码器在所提出的网络中提供了有效的注意力图，从而指导解码器中的多类分割。\nc）参数化跳跃连接的有效性：为了探索所提出的参数化跳跃连接的有效性，我们建立了两个实验4（U-Net + AGD*）和5（U-Net + AGD）。引入参数化跳跃连接后，分割性能得到了提高，几乎没有额外的参数或计算成本。\nd）中间监督的有效性：为了研究所提出的 PANet 中中间监督策略的有效性，我们比较了第 3 号（U-Net + AGD w/o IS）和第 5 号（U-Net + AGD）。如表 III 所示，与第 3 种相比，具有中间监督的网络获得了额外的改进。此外，在比较第 6 种（U-Net + DS + AGD w/o IS）和第 7 种（PANet）时也可以观察到改进。 ）尽管改进相对较小。可以看出，深度监管的引入也对绩效产生了提升，因此中级监管对绩效的提升并不像以前那么显着。\n4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs 1）数据：我们使用来自 BraTS 2020 挑战赛的开源多模态 MRI 数据集 [29]、[36] [37]。训练集由 369 个多对比 MRI 扫描组成，其中每个扫描包含四种模式，即原生 T1 加权、对比后 T1 加权 (T1Gd)、T2 加权 (T2) 和 T2 流体衰减反转恢复 (FLAIR) ）。\n每次扫描都有相应的 4 类标签：背景（标签 0）、GD 增强肿瘤（ET，标签 4）、瘤周水肿（ED，标签 2）以及坏死和非增强肿瘤核心（NET/ NCR，标签 1)。验证集由 125 个多重对比 MRI 扫描组成，其模式与训练集和隐藏的基本事实相同。所有 MRI 扫描均去除颅骨，与相同的大脑模板 (SRI24) 对齐，并插值至 1mm3 分辨率。验证阶段通过在线门户进行，算法根据 3 个重叠肿瘤区域的性能进行排名，即增强肿瘤 (ET)、肿瘤核心 (ET + NET/NCR) 和整个肿瘤 (ET) + NET/NCR + ED）\n2）实现细节：a）模型设置和损失函数：对于3D分割，由于缺乏开源预训练编码器，所有模型都是从头开始训练的。下采样通过跨步 3 × 3 × 3 填充卷积执行，上采样通过三线性插值实现。为了进一步提高在BraTS数据集上的性能，我们采用基于区域的训练策略（直接在重叠区域而不是独立标签上优化）并增强肿瘤抑制（如果增强肿瘤的预测体积为，则用坏死替换预测的增强肿瘤）小于某个阈值）在训练过程中。对于损失函数，我们采用 Dice Loss [13] 和 Cross Entropy Loss 的线性组合作为网络中二分类和多分类阶段的损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 服务器上使用 PyTorch 1.7.1 框架实现的。由于3D分割的训练，尤其是级联U-Net对显存的要求较高，因此我们使用NVIDIA RTX 2080 Ti GPU和NVIDIA RTX 3090 GPU分别训练单个模型和级联模型。\n由于训练过程的显存占用大于11Gb，我们采用PyTorch框架提供的原生混合精度训练程序来节省显存使用并加速训练过程。人工智能医学开放网络（MONAI）项目[38]和TorchIO[39]分别用于训练和推理阶段的数据加载过程。数据增强是在训练过程中通过 MONAI 项目进行的。\n首先，分别使用 z 分数标准化对所有模态进行标准化。然后通过随机翻转、随机强度偏移、随机强度缩放和弹性变换来增强图像。最后，我们将图像块随机裁剪为 128 × 128 × 128 并将其输入网络。对于推理，我们还采用基于补丁的推理管道来生成 BraTS 2020 验证集的预测。面片大小设置为 128 × 128 × 128，面片之间的重叠设置为 75%。重叠区域中的预测是重叠块的平均值。这种重叠配置可以被视为自集成并产生更好的分割性能。对于模型评估，我们进行了 2 个单独的评估程序来比较分割模型的性能。首先，我们对 BraTS 2020 训练集进行 5 倍交叉验证，以比较离散区域（增强肿瘤、瘤周水肿和非增强肿瘤核心）上的分割性能。然后，我们对 BraTS 2020 验证集进行评估，以比较重叠区域（增强肿瘤、肿瘤核心和整个肿瘤）的分割性能，其中我们可以将所提出的方法与最先进的方法进行比较BraTS 挑战。\n3）定量结果：BraTS 2020 训练集的交叉验证性能已在表 IV 中报告。\n所提出的 PANet 在 Dice 分数和 Hausdorff 距离方面优于所有其他网络。此外，我们还在 BraTS 2020 验证集上对训练后的模型进行了验证。通过这种方式，我们还将所提出的网络与模态配对学习（BraTS 2020 中的 Top-2 解决方案）[35] 和研究中的 Transformer TransBTS [23] 进行了比较，除了基于 U 的常用网络之外-网。\n性能列于表五中。我们提出的 PANet 在 BraTS 2020 验证集上优于 U-Net、Attention U-Net、级联 U-Net 和 TransBTS。此外，所提出的 PANet 在 BraTS 2020 上实现了与 Top2 解决方案类似的 Dice 分数，并且具有更好的 Hausdorff 距离。另外，值得注意的是，与 U-Net 相比，所提出的 PANet 仅增加了 3.9% 的额外 GFlops，并且以更少的计算成本实现了比级联 U-Net 更好的性能。这些结果表明，所提出的先验注意网络在 BraTS 2020 数据集上的分割性能和计算效率之间取得了复杂的平衡。\n4）定性结果：我们可视化具有不同分割难度的 3 幅 MRI 图像，以展示不同模型的性能。对于最简单的情况（BraTS20_Validation_077），所有模型都能够分割病变，而所提出的 PANet 获得最高的分割 Dice 分数。对于有一定难度的病例（BraTS20_Validation_028），Attention U-Net和级联U-Net在水肿分割方面都产生了严重的误报，导致整个肿瘤的Dice评分较低。对于最难的情况（BraTS20_Validation_076），由于增强肿瘤的假阳性分割，U-Net、Attention U-Net 和级联 U-Net 的性能并不乐观，而所提出的 PANet 产生了最好的分割性能。所提出的 PANet 的成功归功于具有中间监督的注意力引导解码器，特征图通过空间注意力图进行细化，这可以防止潜在的误报预测。\n关于医学影像中的轴位面（横断面）、冠状面、矢状面的解释\n1.冠状面 （Coronal），又称额状面。即从左右方向，沿人体的长轴将人体纵切为前、后两部分的切面。这种提法只是为了在临床中将器官位置描述的更具体，英文名称是：Coronal section；\n2.矢状面 (Sagittal)就是把人体分成左右两面的解剖面，于这个面平行的也是矢状面。出于这个位置的叫矢状位。矢状位的英文名称是：Median sagittal section；\n3.水平位 (Axial)又称横断位，即左右、前后构成的面为水平位，英文名称是:Transverse section。\n5）消融分析：在 BraTS 2020 验证数据集上也进行了与 2D 分割类似的消融实验，以评估我们模型中呈现的组件的有效性，如表六所示。\na）具有深度监督的多类解码器的有效性：我们通过向网络解码器引入深度监督来构建实验2（U-Net + DS）。与基线（No.1）相比，实验No.2在增强肿瘤、肿瘤核心和整个肿瘤的分割性能方面提供了一定程度的性能提升。此外，当引入注意力引导解码器时，可以观察到性能的提高（第3和第6）。\nb) 注意力引导解码器的有效性：我们构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中注意力引导解码器的有效性。与基线相比，注意力引导解码器在增强肿瘤和肿瘤核心的分割性能方面产生了显着的改善。这表明注意力引导解码器对从编码器提取的特征图产生有效的注意力，从而更容易区分病变。\nc）参数化跳跃连接的有效性：我们建立了两个实验No.4（U-Net + AGD*）和No.5（UNet + AGD）来探索所提出的参数化跳跃连接的有效性。引入参数化跳跃连接后，在增强肿瘤和肿瘤核心方面分割性能有所提高，但整个肿瘤的分割性能略有下降。\nd）中间监督的有效性：我们通过比较表六中的第3号和第5号来调查中间监督策略的有效性。在No.5（U-Net + AGD）中，通过引入中间监督，增强肿瘤和肿瘤核心的Dice得分显着提高。但是当引入深度监督时，中间监督的性能提升并不像以前那么显着（第6和第7），这与2D分割情况类似。\nDISCUSSION AND CONCLUSION 多病灶分割在临床场景中具有重要意义，因为某种疾病可能同时发生多种类型的感染，不同感染阶段的患者可能会出现不同类型的病灶。例如，磨玻璃样混浊（GGO）和实变（CON.）是COVID-19患者典型的肺部病变，前者通常发生在早期患者，而后者的增加可能表明病情恶化。胶质瘤可分为低级别胶质瘤（LGG）和胶质母细胞瘤，即高级别胶质瘤（GBM/HGG），并且在发生HGG的患者中更有可能发现强化肿瘤。因此，多病灶分割在患者的筛查和预后方面具有巨大的潜力。\n多病灶分割问题可以分解为粗分割和细分割，粗分割是对病灶进行粗略分割，而细分割则基于前一分割，以产生最终的分割图。级联网络广泛用于多病变分割任务，因为这些算法背后的逻辑非常自然。然而，级联网络在潜在的临床部署中受到限制，因为它们缺乏灵活性并且对计算资源的要求很高。与现有的级联网络相比，我们开发了一种先验注意网络，它将粗分割和细分割集成到一个网络中。我们提出的网络架构的优点是分割性能和计算效率的平衡。通过将分割的两个步骤结合在一个网络中，所提出的先验注意网络能够实现多病灶分割的端到端训练，在训练和推理方面都具有更大的灵活性，并且在临床部署中具有更大的潜力。此外，我们设法保持所提出的先验注意网络的性能，在分割性能和运行效率之间实现复杂的平衡。\n与级联 U-Net 相比，我们提出的先验注意力网络在 2D 和 3D 任务中都实现了更好的性能和效率，如图 6 所示。\n总之，我们提出了一种新颖的分割网络，即先验注意网络，用于医学图像中的多病灶分割。受流行的从粗到细策略的启发，我们通过空间注意机制将级联网络的两个步骤聚合成一个网络。此外，我们引入了一种新颖的中间监督机制来指导与病变相关的注意图的生成，这可以指导解码器中的后续多类分割。所提出的网络在 2D 和 3D 医学图像（包括 CT 扫描和多模态 MRI）上进行评估。对于 2D 分割，与级联 U-Net 相比，所提出的先验注意力网络以更少的计算成本获得了有竞争力的结果。对于 3D 脑肿瘤分割，所提出的先验注意网络在 BraTS 2020 验证数据集上产生了最先进的性能，并且优于基于 U-Net 的其他基线方法。实验结果表明，该方法在医学影像中的许多多病灶分割任务中具有巨大的应用潜力，与二元分割相比可以提供更多信息，并有助于医生未来的临床诊断。\n","permalink":"https://swimmingliu.github.io/posts/papernotes/2022-priorattentionnetwork/","summary":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。","title":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络"},{"content":"地大服务器使用教程 1. 服务器环境介绍 NVIDIA RTX 3090 (24GB) NVIDIA RTX 2080 Ti (11GB) 2. 配置实验环境 2.1 Conda环境安装 每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\nConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\n大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\n2.2 Conda环境配置 （以YOLOv8为例） # 创建conda环境 名为yolov8_lyj python版本为3.9 conda create -n yolov8_lyj python=3.9 # 激活环境 conda activate yolov8_lyj # 选择合适的路径，克隆github项目代码 git clone https://github.com/ultralytics/ultralytics # 进入到项目路径下 cd ultralytics/ # 安装相关依赖包 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 2.3 准备数据集 下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\n按照算法指定的数据集格式，对数据集格式进行调整。\n​\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\n​\thttps://zhuanlan.zhihu.com/p/461488682\n2.4 开始实验 在算法中指定数据集的存放路径 （相对/绝对路径均可）\n初始化算法的参数\nbatch-size 批处理大小：每一次处理图片的个数，根据显卡内存进行调整 epochs\t迭代次数：算法总共需要训练的轮次 workers 载入数据进程数：每一次调用多少个进程来载入数据 device 选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡 开始训练 # 运行训练代码 python mian.py (注：使用向日葵的同学，可以直接在Pycharm当中运行)\n3. 注意事项 1. 查看显卡使用情况 两种办法：\n# 第一种 使用nivida驱动直接查看 nvidia-smi # 第二种 使用第三方库 gpustat动态查看 # 先安装第三方库 pip install gpustat -i https://pypi.tuna.tsinghua.edu.cn/simple # 每两秒刷新一次 动态查看显存使用情况 watch -n2 gpustat 2. 科学上网 使用向日葵的同学，可以使用科学上网进行github仓库克隆、google网盘数据集下载等\n使用方法：\nexport http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 预祝大家科研顺利，硕果累累，offer拿到手软！！！\n博客地址： SwimmingLiu.cn\n","permalink":"https://swimmingliu.github.io/posts/diary/2023-%E5%9C%B0%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"地大服务器使用教程 1. 服务器环境介绍 NVIDIA RTX 3090 (24GB) NVIDIA RTX 2080 Ti (11GB) 2. 配置实验环境 2.1 Conda环境安装 每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\nConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\n大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\n2.2 Conda环境配置 （以YOLOv8为例） # 创建conda环境 名为yolov8_lyj python版本为3.9 conda create -n yolov8_lyj python=3.9 # 激活环境 conda activate yolov8_lyj # 选择合适的路径，克隆github项目代码 git clone https://github.com/ultralytics/ultralytics # 进入到项目路径下 cd ultralytics/ # 安装相关依赖包 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 2.3 准备数据集 下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\n按照算法指定的数据集格式，对数据集格式进行调整。\n​\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\n​\thttps://zhuanlan.zhihu.com/p/461488682\n2.4 开始实验 在算法中指定数据集的存放路径 （相对/绝对路径均可）\n初始化算法的参数\nbatch-size 批处理大小：每一次处理图片的个数，根据显卡内存进行调整 epochs\t迭代次数：算法总共需要训练的轮次 workers 载入数据进程数：每一次调用多少个进程来载入数据 device 选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡 开始训练 # 运行训练代码 python mian.","title":"地大服务器使用教程"},{"content":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023) 1. Abstract 由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。 作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。 ACC-UNet结合了卷积神经网络（ConvNets）的内在归纳偏差和Transformer的设计决策 卷积神经网络（ConvNets）的内在归纳偏差：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。 Transformer的设计决策：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。 ACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\n2.Introduction 语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\n学习一下这里的背景描述\n原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。\n从 UNet出发，然后逐步介绍他的变体（UNet++等）。随后介绍Transformer和UNet的各种结合体，为后续对比实验做铺垫。最后结合一张发展图，简明扼要描述UNet的创新历程。\n最近，鉴于 Transformer 带来的进步，研究开始重新发现 CNN 的潜力。这方面的开创性工作是“A ConvNet for the 2020s”[16]，它探讨了 Transformer 引入的各种想法及其在卷积网络中的适用性。通过逐渐融合训练协议和微观-宏观设计选择的思想，这项工作使 ResNet 模型的性能优于 Swin Transformer 模型。\n在本文中，我们在 UNet 模型的背景下提出了同样的问题。我们研究仅基于卷积的 UNet 模型是否可以与基于 Transformer 的 UNet 竞争。在此过程中，我们从 Transformer 架构中获得动力并开发了纯卷积 UNet 模型。我们提出了一种基于补丁的上下文聚合，与基于窗口的自注意力相反。此外，我们通过融合来自多个级别编码器的特征图来创新跳跃连接。对 5 个基准数据集的广泛实验表明，我们提出的修改有可能改进 UNet 模型。\n通过介绍CONvNet的重要性，引入本文重点 （纯卷积模块的UNet）。\n3. Method 3.1 A high-level view of transformers in UNet Transformers 显然在两个不同方面改进了 UNet 模型:\n1.利用自注意力的远程依赖性: Transformer 可以通过使用（窗口式）自注意力，从更大的上下文视图中计算特征。此外，他们还通过采用反向瓶颈（即增加 MLP 层中的神经元）来提高表达能力。此外，它们包含快捷连接，这有助于学习。 2.通过通道注意力的自适应多级特征组合: 基于 Transformer 的 UNet 使用通道注意力自适应地融合来自多个编码器级别的特征图。与受当前级别信息限制的简单跳跃连接相比，由于来自不同级别的各种感兴趣区域的组合，这会生成丰富的特征。\n主要就两个点：\n自主注意力在UNet的编码和解码阶段都能够快速、准确地根据上下文信息提取特征。\n自注意力机制可以用来连接encoder阶段不同stage的特征图。\n猜想：把新出的自注意力机制加在ACC-Unet是不是也会有提升呢？\n3.2 Hierarchical Aggregation of Neighborhood Context (HANC) 我们首先探索引入远程依赖性并提高卷积块的表达能力的可能性。我们仅使用逐点和深度卷积来降低计算复杂度。为了增加表达能力，我们建议在卷积块中包含反向瓶颈[16]，这可以通过使用逐点卷积将通道数从 cin 增加到 cinv = cin * inv_f ctr 来实现。由于这些额外的通道会增加模型复杂度，因此我们使用3×3深度卷积来补偿。输入特征图 xin ∈ R cin,n,m 被转换为 x1 ∈ R cinv,n,m （图 2b）\n接下来，我们希望在卷积块中模拟自注意力，其核心是将一个像素与其邻域中的其他像素进行比较[15]。通过将像素值与其邻域的平均值和最大值进行比较可以简化这种比较。因此，我们可以通过附加相邻像素特征的平均值和最大值来提供邻域比较的近似概念。因此，连续逐点卷积可以考虑这些并捕获对比视图。\n由于分层分析对图像有益[23]，我们不是在单个大窗口中计算这种聚合，而是在多个级别中分层计算，例如 2 × 2, 2 ^2 × 2^ 2 , · · · , 2^(k− 1) × 2^(k−1) 个补丁。当 k = 1 时，这将是普通的卷积运算，但是当我们增加 k 的值时，将提供更多的上下文信息，从而绕过对更大卷积核的需要。因此，我们提出的分层邻域上下文聚合通过上下文信息丰富了特征图 x1 ∈ R cinv,n,m 作为 x2 ∈ R cinv*(2k−1),n,m （图 2b），其中 ||对应于沿通道维度的串联。\n下面与Transfomer类似，我们在卷积块中包含一个快捷连接，以实现更好的梯度传播。因此，我们执行另一个逐点卷积以减少 cin 的通道数并与输入特征图相加。因此，x2 ∈ R cinv*(2k−1),n,m 变为 x3 ∈ R cin,n,m （图 2b）\n最后，我们使用逐点卷积将滤波器的数量更改为cout作为输出\n\u0026ldquo;inverted bottlenecks\u0026rdquo;（反向瓶颈） ：将Bottleneck放在卷积块的开始，而不是结束。它先使用1x1卷积来减少通道数，然后才是较大的卷积层。\n3.3 Multi Level Feature Compilation (MLFC) 接下来，我们研究多级特征组合的可行性，这是使用基于 Transformer 的 UNet 的另一个优点。\n基于 Transformer 的跳跃连接已经证明了所有编码器级别的有效特征融合以及各个解码器从编译的特征图中进行的适当过滤[24,22,27]。这是通过连接不同级别的投影令牌来执行的[22]。按照这种方法，我们调整从不同编码器级别获得的卷积特征图的大小，以使它们均衡并连接它们。这为我们提供了跨不同语义级别的特征图的概述。我们应用逐点卷积运算来总结这种表示并与相应的编码器特征图合并。整体信息和个体信息的融合通过另一个卷积传递，我们假设它用来自其他级别特征的信息丰富了当前级别特征。\n对于来自 4 个不同级别的特征 x1、x2、x3、x4，特征图可以通过多级信息来丰富（图 2d）\n这里， resizei(xj ) 是将 xj 的大小调整为 xi 的大小且 ctot = c1 + c2 + c3 + c4 的操作。此操作针对所有不同级别单独完成。因此，我们提出了另一个名为多级特征编译（MLFC）的新颖块，它聚合来自多个编码器级别的信息并丰富各个编码器特征图。该块如图 2d 所示。\n总结： 把各stage的特征联合起来进行逐点卷积，然后各stage得到新的特征信息\n3.4 ACC-UNet 因此，我们提出全卷积ACC-UNet（图2a）。我们从普通的 UNet 模型开始，并将滤波器的数量减少了一半。然后，我们用我们提出的 HANC 块替换了编码器和解码器中的卷积块。我们考虑 inv_f ctr = 3，而不是第 3 级的最后一个解码器块 (inv_f ctr = 34)，以模拟 Swin Transformer 第 3 阶段的扩展。 k = 3，最多考虑 4 × 4 个补丁，被选择用于除瓶颈级别 (k = 1) 及其旁边的级别 (k = 2) 之外的所有级别。接下来，我们通过使用残差块（图 2c）来修改跳跃连接以减少语义间隙 [11] 并堆叠 3 个 MLFC 块。所有卷积层均经过批量归一化 [12]，由 Leaky-RELU [17] 激活，并通过挤压和激励 [10] 重新校准。\n总而言之，在 UNet 模型中，我们用我们提出的 HANC 块替换了经典的卷积块，该 HANC 块执行近似版本的自注意力，并修改了与 MLFC 块的跳跃连接，MLFC 块考虑来自不同编码器级别的特征图。所提出的模型有 16.77 M 个参数，比普通 UNet 模型大约增加了 2M。\n4.Experiments 4.1 Datasets 为了评估 ACC-UNet，我们在 5 个跨不同任务和模式的公共数据集上进行了实验。我们使用 ISIC-2018 [6,21]（皮肤镜检查，2594 张图像）、BUSI [3]（乳腺超声，使用类似于 [13] 的 437 张良性图像和 210 张恶性图像）、CVC-ClinicDB [4]（结肠镜检查，612 张图像） ）、COVID [1]（肺炎病灶分割，100 张图像）和 GlaS [20]（腺体分割，85 张训练图像和 80 张测试图像）。所有图像和掩模的大小都调整为224×224。对于GlaS数据集，我们将原始测试分割作为测试数据，对于其他数据集，我们随机选择20％的图像作为测试数据。\n剩余的 60% 和 20% 图像用于训练和验证，并使用不同的随机改组重复实验 3 次。\nISIC-2018、BUSI、 CVC-ClinicDB、COVID、GlaS 五个数据集\n4.2 Comparison Experiments 4.3 Ablation Study 4.4 Qualitative Results ","permalink":"https://swimmingliu.github.io/posts/papernotes/2023-acc-unet/","summary":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023) 1. Abstract 由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。 作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。 ACC-UNet结合了卷积神经网络（ConvNets）的内在归纳偏差和Transformer的设计决策 卷积神经网络（ConvNets）的内在归纳偏差：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。 Transformer的设计决策：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。 ACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\n2.Introduction 语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\n学习一下这里的背景描述\n原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。","title":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023)"},{"content":"(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络 Abstract 准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用逐元素加法或串联在解码器中逐步融合不同级别的特征。然而，这两种操作都容易产生大量冗余信息，从而削弱不同级别特征之间的互补性，导致病灶定位不准确和边缘模糊。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本减法单元（SU）来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器提供像素级和结构级差异信息。\n然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\n没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\n两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\nIntroduction 作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\n先说医学分割在医学领域重要\u0026hellip;(balabala) 然后当前领域存在xxx挑战\u0026hellip;(balabala)\n这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\n一般来说，编码器中不同级别的特征有不同的特征。高级别具有更多的语义信息，有助于定位对象，而低级别具有更详细的信息，可以捕捉对象的微妙边界。解码器利用特定级别和跨级别特征来生成最终的高分辨率预测。然而，上述方法直接使用逐元素加法或串联来融合来自编码器的任意两级特征并将它们传输到解码器。这些简单的操作并没有更多地关注不同层次之间的差异信息。这一缺点不仅会产生冗余信息来稀释真正有用的特征，还会削弱特定于级别的特征的特性，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于感受野有限，单尺度卷积核很难捕获大小变化物体的上下文信息。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的语义上下文和纹理细节。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取层内多尺度信息。然而，类似ASPP的多尺度卷积模块会产生许多额外的参数和计算。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\n在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于每对相邻的级别特征。 SU突出了特征之间有用的差异信息，并消除了冗余部分的干扰。其次，我们借助所提出的多尺度减法模块收集极端多尺度信息。\n对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们聚合特定于级别的特征和多路径跨级别差分特征，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\n多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\n（也就是说可以用这种办法替换注意力机制）\nRELATED WORK Medical Image Segmentation Network 根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了长连接和短连接，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，注意力门嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不同形状和大小的目标结构。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\n医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。\n我们可以看到，医学通用方法通常针对通用挑战（即丰富的特征表示、多尺度信息提取和跨级别特征聚合）。并且，医学特异性方法根据当前器官或病变的特征提出有针对性的解决方案，例如设计一系列注意力机制、边缘增强模块、不确定性估计等。然而，通用医学模型和医学特异性模型都依赖于通过大量的加法或串联操作来实现特征融合，削弱了互补特征之间的特殊性部分。我们提出的多尺度减法模块自然专注于提取差异信息，从而为解码器提供有效的目标特征。\n主要是说大部分特征融合都是用加法/乘法/串联实现的，但是减法可以削弱互补特征之间的特殊性部分。所以多尺度减法模块提取差异信息，然后再用加法进行特征融合。\nMulti-scale Feature Extraction 尺度线索在捕捉对象的上下文信息中发挥着重要作用。受到被广泛验证为有效且理论上合理的框架的尺度空间理论的启发，越来越多的多尺度方法被提出。与单尺度特征相比，多尺度特征有利于解决自然发生的尺度变化。这一特性可以帮助医学分割模型感知不同尺度的病变。根据形式，当前基于多尺度的方法可以大致分为两类，即层间多尺度结构和层内多尺度结构。前者基于特征编码器提取的不同尺度的特征，并在解码器中逐步聚合它们，例如U形[1]、[2]、[4]、[9]-[11]、[37] ，[38]架构。后者通常配备多尺度可插拔模块，如ASPP [16]、DenseASPP [17]、FoldASPP [6]和PAFEM [12]，构建具有不同扩张率的并行多分支卷积层，以获得丰富的组合感受野。与它们不同的是，我们通过同时引入层间和层内多尺度，提出了具有极端多尺度信息的多尺度减法模块中的多尺度。并且，层内多尺度减法单元专注于挖掘从像素到像素到区域到区域的特征对的自差分性质。与单尺度操作相比，整个过程非常高效，不需要额外的参数。\n多尺度减法模块可以超越其他卷积类办法的多尺度特征信息提取办法\nLoss Method 图像分割中的大多数损失函数都是基于交叉熵或重合度量。传统的交叉熵损失对类别信息一视同仁。 Long等人[24]提出了每个类别的加权交叉熵损失（WCE），以抵消数据中的类别不平衡。 Lin等人[39]引入了困难样本和简单样本的权重来提出焦点损失。 Dice loss[40]被提出作为V-Net中重合测量的损失函数，可以有效抑制类别不平衡带来的问题。 Tversky 损失[41]是 Dice 损失的正则化版本，用于控制准确率和召回率对损失函数的贡献。 Wong等人[42]通过Dice损失和WCE损失的加权求和提出指数对数损失（EL Loss）来提高小结构物体的分割精度。\nTaghanaki等人[43]发现单独使用基于重叠的损失函数存在风险，并提出comomoloss将Dice损失作为正则化项与WCE损失相结合来处理输入输出不平衡的问题。\n虽然这些各种各样的损失函数在不同层次上有不同的效果，但手动设计这些复杂的函数确实费时费力。为此，我们提出了自动且全面的分割损失结构，称为LossNet。\nLossNet权重就0.1 （ 感觉这个是为了创新而创新）\nMETHOD Encoder: Res2Net + Connection: MMSB + Decoder: Plus\nMulti-scale in Multi-scale Subtraction Module 我们使用 FA 和 FB 来表示相邻级别的特征图。\n它们都已被 ReLU 操作激活。我们定义一个基本减法单位（SU）：\n其中是逐元素减法运算，然后计算绝对值，Conv(·) 表示卷积层。直接对元素位置特征进行单尺度减法只是为了建立孤立像素级别上的差异关系，没有考虑病灶可能具有区域聚类的特征。与带有单尺度减法单元的MSNet MICCAI版本[27]相比，我们设计了一个强大的层内多尺度减法单元（MSU），并将MSNet改进为M2SNet。如图3所示，我们利用大小为1×1、3×3和5×5的固定全一权重的多尺度卷积滤波器根据像素-像素和区域区域模式计算细节和结构差异值。使用具有固定参数的多尺度滤波器不仅可以直接捕获匹配空间位置处的初始特征对之间的多尺度差异线索，而且可以在不引入额外参数负担的情况下实现高效训练。因此，M2SNet可以保持与MSNet相同的低计算量，并获得更高精度的性能。整个多尺度减法过程可以表述为：\n其中 Filter(·) n×n 表示大小为 n × n 的完整滤波器（卷积）。 MSU可以捕获FA和FB的互补信息，并突出它们从纹理到结构的差异，从而为解码器提供更丰富的信息。\n为了获得跨多个特征级别的高阶互补信息，我们水平和垂直连接多个MSU来计算一系列具有不同阶数和感受野的差分特征。多尺度减法模块中多尺度的细节可以在图2中找到。我们聚合了相应级别和任意级别之间的特定尺度特征（MSi 1 ）和跨尺度差分特征（MSi n6=1）。其他级别生成互补增强特征（CEi）。这个过程可以表述如下：\n最后，所有CEi参与解码，然后对息肉区域进行分割。\n这里就是介绍一下MSU\nLossNet 在所提出的模型中，总训练损失可以写为：\n其中L w IoU和L w BCE表示加权IoU损失和二元交叉熵（BCE）损失，它们已在分割任务中广泛采用。我们使用与[4]、[44]、[45]中相同的定义，它们的有效性已在这些工作中得到验证。与它们不同的是，我们额外使用LossNet来进一步优化从细节到结构的分割。\n具体来说，我们使用 ImageNet 预训练分类网络，例如 VGG-16，分别提取预测和地面实况的多尺度特征。然后，它们的特征差异计算为损失 Lf ：\n令 F i P 和 F i G 分别表示从预测和地面实况中提取的第 i 层特征图。 l i f 计算为其欧几里德距离（L2-Loss），该距离在像素级别进行监督：\n从图4中可以看出，低层特征图包含丰富的边界信息，高层特征图描述位置信息。因此，LossNet可以在特征层面产生全面的监督。\n","permalink":"https://swimmingliu.github.io/posts/papernotes/2023-m2snet/","summary":"(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络 Abstract 准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用逐元素加法或串联在解码器中逐步融合不同级别的特征。然而，这两种操作都容易产生大量冗余信息，从而削弱不同级别特征之间的互补性，导致病灶定位不准确和边缘模糊。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本减法单元（SU）来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器提供像素级和结构级差异信息。\n然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\n没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\n两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\nIntroduction 作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\n先说医学分割在医学领域重要\u0026hellip;(balabala) 然后当前领域存在xxx挑战\u0026hellip;(balabala)\n这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\n一般来说，编码器中不同级别的特征有不同的特征。高级别具有更多的语义信息，有助于定位对象，而低级别具有更详细的信息，可以捕捉对象的微妙边界。解码器利用特定级别和跨级别特征来生成最终的高分辨率预测。然而，上述方法直接使用逐元素加法或串联来融合来自编码器的任意两级特征并将它们传输到解码器。这些简单的操作并没有更多地关注不同层次之间的差异信息。这一缺点不仅会产生冗余信息来稀释真正有用的特征，还会削弱特定于级别的特征的特性，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于感受野有限，单尺度卷积核很难捕获大小变化物体的上下文信息。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的语义上下文和纹理细节。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取层内多尺度信息。然而，类似ASPP的多尺度卷积模块会产生许多额外的参数和计算。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\n在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于每对相邻的级别特征。 SU突出了特征之间有用的差异信息，并消除了冗余部分的干扰。其次，我们借助所提出的多尺度减法模块收集极端多尺度信息。\n对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们聚合特定于级别的特征和多路径跨级别差分特征，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\n多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\n（也就是说可以用这种办法替换注意力机制）\nRELATED WORK Medical Image Segmentation Network 根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了长连接和短连接，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，注意力门嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不同形状和大小的目标结构。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\n医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。","title":"M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation"},{"content":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation 1. Abstract 目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\n作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\n创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\n1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\n2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\n2. Introduction 背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\n相同方式可引入脑瘤、肺癌。\n为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\nAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\n先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。\n提出问题：医疗环境中计算资源的限制，复杂模块难以落地 \u0026mdash;\u0026gt; 解决办法：轻量化模型\n当前轻量化发展历程 \u0026mdash;\u0026gt; 轻量化的模型分割效果不好 \u0026mdash;\u0026gt; EGE-Unet 轻量+分割能力强\n具体来说，EGE-UNet 利用两个关键模块：群组多轴 Hadamard 产品注意力模块（GHPA）和群组聚合桥模块（GAB）。\n一方面，由于多头自注意力机制（MHSA），最近基于 ViT [7] 的模型已经显示出前景。 MHSA将输入划分为多个head，并在每个head中计算self-attention，这使得模型能够从不同的角度获取信息，整合不同的知识，提高性能。尽管如此，MHSA 的二次复杂度极大地增加了模型的大小。因此，我们提出了具有线性复杂度的哈达玛产品注意力机制（HPA）。HPA 采用可学习的权重，并使用输入执行哈达玛乘积运算以获得输出。随后，受到 MHSA 中多头模式的启发，我们提出了 GHPA，它将输入分为不同的组，并在每个组中执行 HPA。然而，值得注意的是，我们在不同组的不同轴上进行HPA，这有助于进一步从不同的角度获取信息。\n另一方面，对于GAB，由于医学图像中分割目标的大小和形状不一致，因此获得多尺度信息至关重要[19]。因此，GAB基于组聚合融合不同大小的高层和低层特征，并额外引入掩模信息来辅助特征融合。通过将上述两个模块与UNet相结合，我们提出了EGE-UNet，它以极低的参数和计算量实现了出色的分割性能。与以前仅注重提高性能的方法不同，我们的模型还优先考虑现实环境中的可用性。图 1 显示了 EGEUNet 与其他网络的清晰比较。\n具体介绍为什么引入两个创新模块（GHPA、GAB）、以及模块是基于什么论文。（模块背景+创新方法）\n(1)提出了GHPA和GAB，前者有效地获取和集成多视角信息，后者接受不同尺度的特征，以及用于高效多尺度特征融合的辅助掩模。\n(2)我们提出了EGEUNet，这是一种专为皮肤病变分割而设计的极其轻量级的模型。\n(3) 我们进行了广泛的实验，证明了我们的方法在以显着降低的资源需求实现最先进性能方面的有效性。\n主要贡献：（1）写模块作用 （2）写整体网络优势 （3）实验效果\n3. Method 3.1EGE-Unet网络结构 EGE-UNet由对称编码器-解码器部分组成的 U 形架构之上。\n编码器由六级组成，每级通道数为{8,16,24,32,48,64}。解码器同理\n前三个阶段采用内核大小为 3 的普通卷积，后三个阶段利用提出的 GHPA 从不同的角度提取表示信息。\n与 UNet 中的简单跳跃连接相比，EGE-UNet 在编码器和解码器之间的每个阶段都采用了 GAB。\n利用深度监督生成不同规模的掩模预测，这些预测用于损失函数并作为 GAB 的输入之一。\n通过集成这些高级模块，EGE-UNet 显着减少了参数和计算负载，同时与之前的方法相比增强了分割性能。\n3.2 GHPA (Group multi-axis Hadamard Product Attention module) 为了克服 MHSA 带来的二次复杂度问题，我们提出了具有线性复杂度的 HPA。给定输入 x 和随机初始化的可学习张量 p，首先使用双线性插值来调整 p 的大小以匹配 x 的大小。然后，我们在 p 上采用深度可分离卷积（DW）[10][20]，然后在 x 和 p 之间进行哈达玛乘积运算以获得输出。然而，仅利用简单的HPA不足以从多个角度提取信息，导致结果不理想。受 MHSA 中多头模式的启发，我们引入了基于 HPA 的 GHPA，如算法 1 所示。我们将输入沿通道维度平均分为四组，并在高度-宽度、通道-高度和通道上执行 HPA - 分别为前三组的宽度轴。对于最后一组，我们只在特征图上使用DW。最后，我们沿着通道维度连接四组，并应用另一个数据仓库来整合不同角度的信息。请注意，DW 中使用的所有内核大小均为 3。\n首先对输入的特征分为四组进行处理：高度-宽度、通道-高度、通道-宽度、深度可分离卷积\n然后连接4组特征，进行可分离卷积融合特征。\n具体过程：\n第一步，按通道数将输入张量分为四组。（x1, x2, x3, x4）\n设置初始化三个全一张量，分别为高度-宽度、通道-高度、通道-宽度（Pxy, Pzx, Pzy）。\n第二步，将 x1, x2, x3 的对应切片分别使用双线插值法（bilinear）在Pxy, Pzx, Pzy中进行插值。\n第三步，对插值后的Pxy, Pzx, Pzy，进行深度可分离卷积，然后分别和x1, x2, x3进行哈达玛乘积\n第四步，连接4组特征信息，然后经过深度可分离卷积融合特征。\n3.3 GAB (Group Aggregation Bridge module) 多尺度信息的获取被认为对于密集预测任务（例如医学图像分割）至关重要。因此，如图 3 所示，我们引入了 GAB，它接受三个输入：低级特征、高级特征和掩码。首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征的大小，以匹配低层特征的大小。其次，我们沿通道维度将两个特征映射分为四组，并将一组低级特征与一组高级特征连接起来，以获得四组融合特征。对于每组融合特征，掩码被连接起来。接下来，将内核大小为3和不同扩张率{1,2,5,7}的扩张卷积[25]应用于不同的组，以提取不同尺度的信息。最后，将四组沿通道维度连接起来，然后应用内核大小为 1 的普通卷积，以实现不同尺度的特征之间的交互。\nGAB模块作用： 将高级特征、低级特征、低级特征的预测掩码进行特征融合，作为新的输入特征进行解码。\n具体过程： 高级特征、低级特征、低级特征的预测掩码 (xh、xl 、Mask)\n首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征 (xh) 的大小，以匹配低层特征 (xl) 的大小。\n其次，沿通道维度将两个特征映射分为四组。（对应不同空洞卷积的扩张率：d1 = 1, d2 = 2, d3 = 5, d4 = 7）\n并将每一组的低级特、高级特征和掩码连接起来，总共四组融合特征。\n最后，将四组特征进行连接，并进行1x1卷积得到输出。\n3.4 Loss Function 在本研究中，由于不同的GAB需要不同尺度的掩模信息，因此采用深度监督来计算不同阶段的损失函数，以生成更准确的掩模信息。我们的损失函数可以表示为方程（1）和（2）。其中 Bce 和 Dice 表示二元交叉熵和dice损失。 λi是不同阶段的权重。在本文中，我们默认将i=0到i=5之间的λi设置为1、0.5、0.4、0.3、0.2、0.1。\n分为6个阶段，逐一计算每个阶段的损失。然后按照权重对损失进行求和。\n4.Experiments 4.1 Datasets and Implementation details 为了评估我们模型的有效性，我们选择了两个公共皮肤病变分割数据集，即 ISIC2017 [1][3] 和 ISIC2018 [2][6]，分别包含 2150 个和 2694 个皮肤镜图像。与之前的研究[19]一致，我们以 7:3 的比例将数据集随机划分为训练集和测试集。\nEGE-UNet是由Pytorch[17]框架开发的。所有实验均在单个 NVIDIA RTX A6000 GPU 上执行。图像被归一化并调整大小为 256×256。我们应用各种数据增强，包括水平翻转、垂直翻转和随机旋转。 AdamW [13] 用作优化器，以 0.001 的学习率初始化，CosineAnnealingLR [12] 用作调度器，最大迭代次数为 50，最小学习率为 1e-5。总共训练了 300 个 epoch，批量大小为 8。为了评估我们的方法，我们采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为指标，并进行 5 次训练\n​\t在公共皮肤病变分割数据集（ISIC2017 和 ISIC2018 ）进行对比实验，在ISIC2018进行消融实验\n采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为评估指标\n4.2 Comparison Experiments 4.3 Ablation Experiments 4.4 Qualitative Comparisons 5. ConClusions 在本文中，我们提出了两个高级模块。我们的 GHPA 使用一种新颖的 HPA 机制将自注意力的二次复杂度简化为线性复杂度。它还利用分组来充分捕获来自不同角度的信息。我们的 GAB 融合了低级和高级特征，并引入了一个掩模来集成多尺度信息。基于这些模块，我们提出了用于皮肤病变分割任务的 EGE-UNet。实验结果证明了我们的方法在显着降低资源需求的情况下实现最先进的性能的有效性。我们希望我们的工作能够激发医学图像界对轻量级模型的进一步研究。\n作者提出的EGE-UNet实现了轻量、准确的皮肤病变分割任务\n","permalink":"https://swimmingliu.github.io/posts/papernotes/2023-ege-unet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","summary":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation 1. Abstract 目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\n作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\n创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\n1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\n2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\n2. Introduction 背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\n相同方式可引入脑瘤、肺癌。\n为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\nAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\n先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。","title":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"}]