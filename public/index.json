[{"content":"Introduction YOLOSHOW is a graphical user interface (GUI) application embed withYOLOv5 YOLOv7 YOLOv8 algorithm.\nFunctions 1. Support Image / Video / Webcam / Folder (Batch ) Object Detection Choose Image / Video / Webcam / Folder (Batch ) in the menu bar on the left to detect objects.\n2. Change Models / Hyper Parameters dynamically When the program is running to detect targets, you can change models / hyper Parameters\nSupport changing model in YOLOv5 / YOLOv7 / YOLOv8 dynamically Support changing IOU / Confidence / Delay time / line thickness dynamically 3. Loading Model Automatically Our program will automatically detect pt files including YOLOv5 Models / YOLOv7 Models / YOLOv8 Models that were previously added to the ptfiles folder.\nIf you need add the new pt file, please click Import Model button in Settings box to select your pt file. Then our program will put it into ptfiles folder.\nNotice : All pt files are named including yolov5 / yolov7 / yolov8. (e.g. yolov8-test.pt)\n4. Loading Configures After startup, the program will automatically loading the last configure parameters. After closedown, the program will save the changed configure parameters. 5. Save Results If you need Save results, please click Save MP4/JPG before detection. Then you can save your detection results in selected path.\nPreparation Experimental environment OS : Windows 11 CPU : Intel(R) Core(TM) i7-10750H CPU @2.60GHz 2.59 GHz GPU : NVIDIA GeForce GTX 1660Ti 6GB 1. Create virtual environment create a virtual environment equipped with python version 3.9, then activate environment.\nconda create -n yoloshow python=3.9 conda activate yoloshow 2. Install Pytorch frame Windows: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Linux: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Change other pytorch version in 3. Install dependency package Switch the path to the location of the program\ncd {the location of the program} Install dependency package of program\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple pip install \u0026#34;PySide6-Fluent-Widgets[full]\u0026#34; -i https://pypi.tuna.tsinghua.edu.cn/simple pip install -U Pyside6 -i https://pypi.tuna.tsinghua.edu.cn/simple 4. Add Font Copy all font files *.ttf in fonts folder into C:\\Windows\\Fonts\nFrames Reference YOLO Algorithm YOLOv5 YOLOv7 YOLOv8\nYOLO Graphical User Interface YOLOSIDE\n","permalink":"https://swimmingliu.cn/posts/diary/yoloshow/","summary":"Introduction YOLOSHOW is a graphical user interface (GUI) application embed withYOLOv5 YOLOv7 YOLOv8 algorithm.\nFunctions 1. Support Image / Video / Webcam / Folder (Batch ) Object Detection Choose Image / Video / Webcam / Folder (Batch ) in the menu bar on the left to detect objects.\n2. Change Models / Hyper Parameters dynamically When the program is running to detect targets, you can change models / hyper Parameters","title":"YOLOSHOW - YOLOv5 / YOLOv7 / YOLOv8 GUI based on Pyside6"},{"content":"安装 Xshell 和 Xftp https://www.netsarang.com/en/xshell-download/ # Xshell下载连接 https://blog.csdn.net/m0_67400972/article/details/125346023 # 安装教程 添加Xshell连接 其中 server.ip 为服务器公网ip地址，端口为 6969\n安装Anaconda3 每个用户均被分配 AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh 于主目录\nbash AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh # 安装anaconda3 输入 yes 后， 再按回车键 即可\n初始化Anaconda3 conda init bash\t# 初始化conda 然后重新使用Xshell 连接即可\nMagic Network 下载外网文件、克隆Github项目等操作，必须使用Magic Network\nexport http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 取消Magic Network\nunset http_proxy unset https_proxy 如果使用上面的命令，不能连接Google. 需要远程桌面连接，打开CFW (默认是打开的)\nnohup bash /home/dell/LYJ/Clash/cfw \u0026gt; cfw.out 国内镜像下载 pip 清华源下载\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple packge # packge为包名 conda 配置镜像\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ # 以上两条是Anaconda官方库的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ # 以上是Anaconda第三方库 Conda Forge的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ # 以上是Pytorch的Anaconda第三方镜像 远程桌面连接 远程连接直接找师兄问 向日葵 和 Teamviewer 密码，连接即可\n注意事项 建议非必要不使用远程连接（由于在同一时间段内，远程连接只能单用户使用） 使用远程连接，请先阅读服务器壁纸上的注意事项 如需上传文件，尽量使用移动硬盘，到918实验室拷贝至服务器上 ","permalink":"https://swimmingliu.cn/posts/diary/zstu_server_manuscript/","summary":"安装 Xshell 和 Xftp https://www.netsarang.com/en/xshell-download/ # Xshell下载连接 https://blog.csdn.net/m0_67400972/article/details/125346023 # 安装教程 添加Xshell连接 其中 server.ip 为服务器公网ip地址，端口为 6969\n安装Anaconda3 每个用户均被分配 AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh 于主目录\nbash AnacondaAnaconda3-2023.07-1-Linux-x86_64.sh # 安装anaconda3 输入 yes 后， 再按回车键 即可\n初始化Anaconda3 conda init bash\t# 初始化conda 然后重新使用Xshell 连接即可\nMagic Network 下载外网文件、克隆Github项目等操作，必须使用Magic Network\nexport http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 取消Magic Network\nunset http_proxy unset https_proxy 如果使用上面的命令，不能连接Google. 需要远程桌面连接，打开CFW (默认是打开的)\nnohup bash /home/dell/LYJ/Clash/cfw \u0026gt; cfw.out 国内镜像下载 pip 清华源下载\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple packge # packge为包名 conda 配置镜像","title":"ZSTU服务器使用教程 (Yang Li Lab)"},{"content":"FRP配置 跳板机 # frps.ini 配置 [common] bind_port = 7000 #frps服务监听的端口 token = 123 # 链接口令 ./frps -c frps.ini # 启动frps 服务器 # frpc.ini [common] server_addr = x.x.x.x # 此处为 跳板机 的公网ip server_port = 7000 # 跳板机上frps服务监听的端口 token = 123 # 链接口令 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 # 需要暴露的内网机器的端口 remote_port = 6000 # 暴露的内网机器的端口在vps上的端口 SSH连接 ssh -p 6000 swimmingliu@server.ip # 普通ssh 连接 ssh swimmingliu@server.ip 6000\t# xshell ssh连接 用户管理 添加用户 sudo adduser xxx 删除用户 sudo deluser xxx Magic Network export http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 Anaconda3 安装和配置 安装Anaconda3 wget --user-agent=\u0026#34;Mozilla\u0026#34; https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.07-1-Linux-x86_64.sh bash Anaconda3-2023.07-1-Linux-x86_64.sh # https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive 清华源 配置之前的envs cp -r old_envs_path anaconda/envs/\t#迁移之前的envs环境 完结撒花❀❀❀ ","permalink":"https://swimmingliu.cn/posts/diary/zstu_server_management/","summary":"FRP配置 跳板机 # frps.ini 配置 [common] bind_port = 7000 #frps服务监听的端口 token = 123 # 链接口令 ./frps -c frps.ini # 启动frps 服务器 # frpc.ini [common] server_addr = x.x.x.x # 此处为 跳板机 的公网ip server_port = 7000 # 跳板机上frps服务监听的端口 token = 123 # 链接口令 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 # 需要暴露的内网机器的端口 remote_port = 6000 # 暴露的内网机器的端口在vps上的端口 SSH连接 ssh -p 6000 swimmingliu@server.ip # 普通ssh 连接 ssh swimmingliu@server.ip 6000\t# xshell ssh连接 用户管理 添加用户 sudo adduser xxx 删除用户 sudo deluser xxx Magic Network export http_proxy=http://127.","title":"ZSTU Server Management"},{"content":"Introduction 基于知识图谱和neo4j图数据库的电影知识问答系统\nWorkflow DataBase 爬取豆瓣TOP1000电影信息数据\nFrontend 获取用户输入的信息 （语音输入 / 文本输入） 向电影知识问答后端服务器发送请求 获取返回结果 (成功 -\u0026gt; 4 / 失败 -\u0026gt; 5) 如果返回结果包含image信息，则显示图片和文字，否则只显示文字 请求基于gpt的AI模型服务器，并显示返回结果 Backend ​\t[准备工作] 训练 TF-IDF 向量算法和朴素贝叶斯分类器，用于预测用户文本所属的问题类别\n接受前端请求，获取用户输入信息 使用分词库解析用户输入的文本词性，提取关键词 根据贝叶斯分类器，分类出用户文本的问题类型 结合关键词与问题类别，在 Neo4j 中查询问题的答案 返回查询结果 （若问题类型为 演员信息 / 电影介绍，则附加图片url） WorkFlow Graph Frame DataBase Frontend Backend Reference Frontend 微信小程序：微信聊天机器人\nBackEnd 基于知识图谱的电影知识问答系统\n电影知识库问答机器人\n","permalink":"https://swimmingliu.cn/posts/diary/2023-moviekgqa/","summary":"Introduction 基于知识图谱和neo4j图数据库的电影知识问答系统\nWorkflow DataBase 爬取豆瓣TOP1000电影信息数据\nFrontend 获取用户输入的信息 （语音输入 / 文本输入） 向电影知识问答后端服务器发送请求 获取返回结果 (成功 -\u0026gt; 4 / 失败 -\u0026gt; 5) 如果返回结果包含image信息，则显示图片和文字，否则只显示文字 请求基于gpt的AI模型服务器，并显示返回结果 Backend ​\t[准备工作] 训练 TF-IDF 向量算法和朴素贝叶斯分类器，用于预测用户文本所属的问题类别\n接受前端请求，获取用户输入信息 使用分词库解析用户输入的文本词性，提取关键词 根据贝叶斯分类器，分类出用户文本的问题类型 结合关键词与问题类别，在 Neo4j 中查询问题的答案 返回查询结果 （若问题类型为 演员信息 / 电影介绍，则附加图片url） WorkFlow Graph Frame DataBase Frontend Backend Reference Frontend 微信小程序：微信聊天机器人\nBackEnd 基于知识图谱的电影知识问答系统\n电影知识库问答机器人","title":"MovieKGQA: 基于知识图谱和neo4j图数据库的电影知识问答系统"},{"content":"Abstract 在本文中，我们介绍了 U-Net v2，这是一种用于医学图像分割的新的稳健且高效的 U-Net 变体。它的目的是增强语义信息在低级特征中的注入，同时用更精细的细节来细化高级特征。对于输入图像，我们首先使用深度神经网络编码器提取多级特征。接下来，我们通过注入来自更高级别特征的语义信息并通过 Hadamard 乘积集成来自较低级别特征的更精细的细节来增强每个级别的特征图。我们新颖的跳跃连接赋予所有级别的功能以丰富的语义特征和复杂的细节。改进后的特征随后被传输到解码器以进行进一步处理和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。我们在几个公共医学图像分割数据集上评估了我们的方法，用于皮肤病变分割和息肉分割，实验结果证明了我们的新方法相对于最先进的方法的分割准确性，同时保留了内存和计算效率。代码位于：https://github.com/yaoppeng/U-Net_v2。\n主要工作就在于中间的skip-connection\nIntroduction 随着现代深度神经网络的进步，语义图像分割取得了重大进展。语义图像分割的典型范例涉及具有跳跃连接的编码器-解码器网络[1]。在此框架中，编码器从输入图像中提取层次和抽象特征，而解码器获取编码器生成的特征图并重建像素级分割掩模或图，为输入图像中的每个像素分配类标签。人们进行了一系列研究[2, 3]，将全局信息纳入特征图中并增强多尺度特征，从而大大提高了分割性能。 在医学图像分析领域，精确的图像分割在计算机辅助诊断和分析中起着至关重要的作用。 U-Net [4] 最初是为了医学图像分割而引入的，利用跳跃连接来连接每个级别的编码器和解码器阶段。跳跃连接使解码器能够访问早期编码器阶段的特征，从而保留高级语义信息和细粒度空间细节。这种方法有助于精确描绘对象边界并提取医学图像中的小结构。此外，还应用了密集连接机制，通过连接所有级别和所有阶段的特征来减少编码器和解码器中特征之间的差异[5]。设计了一种机制来通过连接较高和较低级别的不同尺度的特征来增强特征[6]。 然而，基于 U-Net 的模型中的这些连接在集成低级和高级特征方面可能不够有效。例如，在 ResNet [7] 中，深度神经网络是作为多个浅层网络的集合而形成的，并且显式添加的残差连接表明，即使在百万规模的训练中，网络也很难学习恒等映射函数图像数据集。\n对于编码器提取的特征，低级特征通常保留更多细节，但缺乏足够的语义信息，并且可能包含不需要的噪声。相反，高级特征包含更多语义信息，但由于分辨率显着降低而缺乏精确的细节（例如对象边界）。通过串联简单地融合特征将在很大程度上依赖于网络的学习能力，这通常与训练数据集的大小成正比。这是一个具有挑战性的问题，特别是在医学成像领域，通常受到有限数据的限制。这种信息融合是通过密集连接跨多个级别连接低级和高级特征来实现的，可能会限制来自不同级别的信息的贡献并可能引入噪声。另一方面，尽管引入的额外卷积并没有显着增加参数数量，但 GPU 内存消耗将会增加，因为必须存储所有中间特征图和相应的梯度以进行前向传递和后向梯度计算。这会导致 GPU 内存使用量和浮点运算 (FLOP) 增加。\n(a) U-Net v2 模型的整体架构，由编码器、SDI（语义和细节注入）模块和解码器组成。 (b) SDI模块的架构。为简单起见，我们仅显示第三级特征的细化（l = 3）。 SmoothConv 表示用于特征平滑的 3 × 3 卷积。$\\bigotimes$ 表示哈达玛积。\n在[8]中，利用反向注意力来明确地建立多尺度特征之间的联系。在[9]中，ReLU激活应用于较高级别的特征，并将激活的特征与较低级别的特征相乘。此外，在[10]中，作者提出分别从 CNN 和 Transformer 模型中提取特征，在多个级别上组合来自 CNN 和 Transformer 分支的特征来增强特征图。然而，这些方法都很复杂，而且它们的性能仍然不是很令人满意，因此需要进一步改进。\n在本文中，我们提出了 U-Net v2，这是一种基于 U-Net 的新分割框架，具有简单且高效的跳跃连接。我们的模型首先使用 CNN 或 Transformer 编码器提取多级特征图。接下来，对于第 i 层的特征图，我们通过简单的哈达玛乘积操作显式地注入高层特征（包含更多语义信息）和低层特征（捕获更精细的细节），从而增强语义和细节第 i 级特征。随后，细化的特征被传输到解码器进行分辨率重建和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。\n我们使用公开的数据集在两个医学图像分割任务（皮肤病变分割和息肉分割）上评估我们的新方法。实验结果表明，我们的 U-Net v2 在这些分割任务中始终优于最先进的方法，同时保持 FLOP 和 GPU 内存效率。\nMethod 2.1 Overall Architecture 我们的 U-Net v2 的整体架构如图 1（a）所示。它包括三个主要模块：编码器、SDI（语义和细节注入）模块和解码器。给定输入图像 I，其中 I ∈ $R^{H×W×C}$ ，编码器产生 M 个级别的特征。我们将第 i 级特征表示为$ f^0_i$ , 1 ≤ i ≤ M。这些收集到的特征，{$ f^0_1$ ,$ f^0_2$,… , $ f^0_M$}，然后传输到 SDI 模块进行进一步细化。\n2.2 Semantics and Detail Infusion (SDI) Module 利用编码器生成的分层特征图，我们首先将空间和通道注意机制[11]应用于每个级别 i 的特征 $ f^0_i$。此过程使特征能够集成局部空间信息和全局通道信息，如下所示：\n其中$f^1_i$表示第 i 层处理后的特征图，$φ^s_i$ 和 $\\phi^c_i$ 分别表示第 i 层空间注意力和通道注意力的参数。此外，我们应用 1 × 1 卷积将$f^1_i$的通道减少到 c，其中 c 是超参数。得到的特征图表示为$f^2_i$，其中 $f^2_i$ ∈ $R^{H_i × W_i × c}$ ，其中$H_i$、$W_i$ 和 c 分别表示 $f^2_i$ 的宽度、高度和通道。\n接下来，我们需要将精炼后的特征图发送到解码器。在每个解码器级别 i，我们使用 $f^2_i$ 作为目标参考。然后，我们调整每个第 j 层的特征图的大小，以匹配与 $f^2_i$ 相同的分辨率，公式为：\n其中 D 、 I 和 U 分别表示自适应平均池化、恒等映射和双线性插值 $f^2_i$ 到 $H_i$、$W_i$ 的分辨率，其中 1 ≤ i，j ≤ M。\n然后，应用 3 × 3 卷积来平滑每个调整大小的特征图 $f^3_{ij}$ ，公式为：\n其中$θ_{ij}$表示平滑卷积的参数， $f^4_{ij}$ 是第i层的第j个平滑特征图。\n将所有第 i 级特征图调整为相同的分辨率后，我们将元素级哈达玛积应用于所有调整大小的特征图，以通过更多语义信息和更精细的细节来增强第 i 级特征，如下所示：\n其中$H(·)$表示哈达玛积 (见图1(b))。然后，$f^5_i$ 被分派到第i级解码器以进行进一步的分辨率重建和分割。\nExperiments 3.1 Datasets 我们使用以下数据集评估新的 U-Net v2。 ISIC 数据集：使用两个皮肤病变分割数据集：ISIC 2017 [15, 16]，其中包含 2050 个皮肤镜图像，ISIC 2018 [15]，其中包含 2694 个皮肤镜图像。为了公平比较，我们遵循[13]中概述的训练/测试分割策略。 息肉分割数据集：使用五个数据集：Kvasir-SEG [17]、ClinicDB [18]、ColonDB [19]、Endoscene [20] 和 ETIS [21]。为了公平比较，我们使用[8]中的训练/测试分割策略。具体来说，使用来自 ClinicDB 的 900 张图像和来自 Kvasir-SEG 的 548 张图像作为训练集，其余图像作为测试集。\n3.2 Experimental Setup 我们使用 PyTorch 在 NVIDIA P100 GPU 上进行实验。我们的网络使用 Adam 优化器进行优化，初始学习率 = 0.001，β1 = 0.9，β2 = 0.999。\n我们采用幂为 0.9 的多项式学习率衰减。训练时期的最大数量设置为 300。超参数 c 设置为 32。按照[13]中的方法，我们报告 ISIC 数据集的 DSC（骰子相似系数）和 IoU（并集交集）分数。对于息肉数据集，我们报告 DSC、IoU 和 MAE（平均绝对误差）分数。每个实验运行 5 次，并报告平均结果。我们使用金字塔视觉变换器（PVT）[22]作为特征提取的编码器。\n3.3 Results and Analysis 表 1 列出了 ISIC 数据集上最先进方法的比较结果。如图所示，我们提出的 UNet v2 将 DSC 分数提高了 1.44% 和 2.48%，IoU 分数提高了 2.36% 和 3.90%。分别是 ISIC 2017 和 ISIC 2018 数据集。这些改进证明了我们提出的将语义信息和更精细的细节注入每个特征图的方法的有效性。\n表 2 列出了息肉分割数据集上最先进方法的比较结果。如图所示，我们提出的 U-Net v2 在 Kavasir-SEG、ClinicDB、ColonDB 和 ETIS 上优于 Poly-PVT [14]数据集，DSC 得分分别提高了 1.1%、0.7%、0.4% 和 0.3%。这强调了我们提出的方法在将语义信息和更精细的细节注入每个级别的特征图中的一致有效性。\n3.4 Ablation Study 我们使用 ISIC 2017 和 ColonDB 数据集进行消融研究，以检查 U-Net v2 的有效性，如表 3 所示。具体来说，我们使用 PVT [22] 模型作为 UNet++ [5] 的编码器。请注意，当我们的 SDI 模块被移除时，U-Net v2 将恢复为具有 PVT 主干的普通 U-Net。 **SC 表示 SDI 模块内的空间和通道关注点。**从表 3 可以看出，与不带 SDI 的 U-Net v2（即带 PVT 编码器的 U-Net）相比，UNet++ 的性能略有下降。\n这种下降可能归因于密集连接生成的多级特征的简单串联，这可能会混淆模型并引入噪声。表 3 表明 SDI 模块对整体性能贡献最大，突出显示我们提出的跳跃连接（即 SDI）持续带来性能改进。\n3.5 Qualitative Results 图 2 给出了 ISIC 2017 数据集的一些定性示例，这表明我们的 U-Net v2 能够将语义信息和更精细的细节合并到每个级别的特征图中。因此，我们的分割模型可以捕获对象边界的更精细的细节。\n3.6 Computation, GPU Memory, and Inference Time 为了检查 U-Net v2 的计算复杂性、GPU 内存使用情况和推理时间，我们报告了我们的方法 U-Net [4] 的参数、GPU 内存使用情况、FLOP 和 FPS（每秒帧数），以及表 4 中的 UNet++ [5]。实验使用 float32 作为数据类型，这导致每个变量使用 4B 的内存。 GPU内存使用记录了前向/后向传递过程中存储的参数和中间变量的大小。\n(1, 3, 256, 256) 表示输入图像的大小。所有测试均在 NVIDIA P100 GPU 上进行。\n从表4中可以看出，UNet++引入了更多的参数，并且由于在密集前向过程中存储中间变量（例如特征图），其GPU内存使用量更大。通常，此类中间变量比参数消耗更多的 GPU 内存。\n此外，U-Net v2 的 FLOPs 和 FPS 也优于 UNet++。与 U-Net (PVT) 相比，我们的 U-Net v2 的 FPS 降低是有限的。\nConclusion 引入了新的 U-Net 变体 U-Net v2，它采用新颖且简单的跳跃连接设计，以改进医学图像分割。该设计明确地将来自较高级别特征的语义信息和来自较低级别特征的更精细细节集成到编码器使用 Hadamard 乘积生成的每个级别的特征映射中。在皮肤病变和息肉分割数据集上进行的实验验证了我们的 UNet v2 的有效性。复杂性分析表明 U-Net v2 在 FLOP 和 GPU 内存使用方面也很高效\n这篇文章比较简单，整体的行文风格一看就是会议论文。核心创新点就一个SDI（Semantic and Detail Infusion）模块。SDI模块作用就是 连接高级特征的语义信息和低级特征的细节信息。首先通过通道和特征注意力机制，分别关注不同级别的通道和空间信息。然后将所有的通道都 padding / scaling 到和第i级别相同的通道数，然后通过双线性插值 / 自适应平均池化到相同大小的尺寸。最后，使用哈达码乘积进行特征融合。 对，没错就这么简单！！！\n（思考: 能不能用减法单元来融合差异性？？？）\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-unet_v2/","summary":"Abstract 在本文中，我们介绍了 U-Net v2，这是一种用于医学图像分割的新的稳健且高效的 U-Net 变体。它的目的是增强语义信息在低级特征中的注入，同时用更精细的细节来细化高级特征。对于输入图像，我们首先使用深度神经网络编码器提取多级特征。接下来，我们通过注入来自更高级别特征的语义信息并通过 Hadamard 乘积集成来自较低级别特征的更精细的细节来增强每个级别的特征图。我们新颖的跳跃连接赋予所有级别的功能以丰富的语义特征和复杂的细节。改进后的特征随后被传输到解码器以进行进一步处理和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。我们在几个公共医学图像分割数据集上评估了我们的方法，用于皮肤病变分割和息肉分割，实验结果证明了我们的新方法相对于最先进的方法的分割准确性，同时保留了内存和计算效率。代码位于：https://github.com/yaoppeng/U-Net_v2。\n主要工作就在于中间的skip-connection\nIntroduction 随着现代深度神经网络的进步，语义图像分割取得了重大进展。语义图像分割的典型范例涉及具有跳跃连接的编码器-解码器网络[1]。在此框架中，编码器从输入图像中提取层次和抽象特征，而解码器获取编码器生成的特征图并重建像素级分割掩模或图，为输入图像中的每个像素分配类标签。人们进行了一系列研究[2, 3]，将全局信息纳入特征图中并增强多尺度特征，从而大大提高了分割性能。 在医学图像分析领域，精确的图像分割在计算机辅助诊断和分析中起着至关重要的作用。 U-Net [4] 最初是为了医学图像分割而引入的，利用跳跃连接来连接每个级别的编码器和解码器阶段。跳跃连接使解码器能够访问早期编码器阶段的特征，从而保留高级语义信息和细粒度空间细节。这种方法有助于精确描绘对象边界并提取医学图像中的小结构。此外，还应用了密集连接机制，通过连接所有级别和所有阶段的特征来减少编码器和解码器中特征之间的差异[5]。设计了一种机制来通过连接较高和较低级别的不同尺度的特征来增强特征[6]。 然而，基于 U-Net 的模型中的这些连接在集成低级和高级特征方面可能不够有效。例如，在 ResNet [7] 中，深度神经网络是作为多个浅层网络的集合而形成的，并且显式添加的残差连接表明，即使在百万规模的训练中，网络也很难学习恒等映射函数图像数据集。\n对于编码器提取的特征，低级特征通常保留更多细节，但缺乏足够的语义信息，并且可能包含不需要的噪声。相反，高级特征包含更多语义信息，但由于分辨率显着降低而缺乏精确的细节（例如对象边界）。通过串联简单地融合特征将在很大程度上依赖于网络的学习能力，这通常与训练数据集的大小成正比。这是一个具有挑战性的问题，特别是在医学成像领域，通常受到有限数据的限制。这种信息融合是通过密集连接跨多个级别连接低级和高级特征来实现的，可能会限制来自不同级别的信息的贡献并可能引入噪声。另一方面，尽管引入的额外卷积并没有显着增加参数数量，但 GPU 内存消耗将会增加，因为必须存储所有中间特征图和相应的梯度以进行前向传递和后向梯度计算。这会导致 GPU 内存使用量和浮点运算 (FLOP) 增加。\n(a) U-Net v2 模型的整体架构，由编码器、SDI（语义和细节注入）模块和解码器组成。 (b) SDI模块的架构。为简单起见，我们仅显示第三级特征的细化（l = 3）。 SmoothConv 表示用于特征平滑的 3 × 3 卷积。$\\bigotimes$ 表示哈达玛积。\n在[8]中，利用反向注意力来明确地建立多尺度特征之间的联系。在[9]中，ReLU激活应用于较高级别的特征，并将激活的特征与较低级别的特征相乘。此外，在[10]中，作者提出分别从 CNN 和 Transformer 模型中提取特征，在多个级别上组合来自 CNN 和 Transformer 分支的特征来增强特征图。然而，这些方法都很复杂，而且它们的性能仍然不是很令人满意，因此需要进一步改进。\n在本文中，我们提出了 U-Net v2，这是一种基于 U-Net 的新分割框架，具有简单且高效的跳跃连接。我们的模型首先使用 CNN 或 Transformer 编码器提取多级特征图。接下来，对于第 i 层的特征图，我们通过简单的哈达玛乘积操作显式地注入高层特征（包含更多语义信息）和低层特征（捕获更精细的细节），从而增强语义和细节第 i 级特征。随后，细化的特征被传输到解码器进行分辨率重建和分割。我们的方法可以无缝集成到任何编码器-解码器网络中。\n我们使用公开的数据集在两个医学图像分割任务（皮肤病变分割和息肉分割）上评估我们的新方法。实验结果表明，我们的 U-Net v2 在这些分割任务中始终优于最先进的方法，同时保持 FLOP 和 GPU 内存效率。","title":"U-NET V2: RETHINKING THE SKIP CONNECTIONS OF U-NET FOR MEDICAL IMAGE SEGMENTATION"},{"content":"Abstract 放射科医生拥有不同的培训和临床经验，导致肺结节的分割注释存在差异，从而导致分割的不确定性。传统方法通常选择单个注释作为学习目标或尝试学习包含多个注释的潜在空间。\n然而，这些方法无法利用多个注释之间的共识和分歧所固有的有价值的信息。在本文中，我们提出了一种不确定性感知注意机制（UAAM），它利用多个注释之间的共识和分歧来促进更好的分割。为此，我们引入了多置信度掩模（MCM），它结合了低置信度（LC）掩模和高置信度（HC）掩模。 LC 掩模表示分割置信度较低的区域，放射科医生可能有不同的分割选择。继UAAM之后，我们进一步设计了一个不确定性引导多置信分割网络（UGMCS-Net），它包含三个模块：一个捕获肺结节一般特征的特征提取模块，一个为肺结节产生三个特征的不确定性感知模块。注释的并集、交集和注释集，以及一个交集并集约束模块，该模块使用三个特征之间的距离来平衡最终分割和 MCM 的预测。为了全面展示我们方法的性能，我们提出了 LIDC-IDRI 上的复杂结节验证，它测试了 UGMCS-Net 对使用常规方法难以分割的肺结节的分割性能。实验结果表明，我们的方法可以显着提高传统方法难以分割的结节的分割性能。\nINTRODUCTION 肺结节分割在肺癌计算机辅助诊断 (CAD) 系统中至关重要 [1]，可提供结节大小、形状和其他重要医学特征等关键信息。然而，对于深度学习方法的一般训练和测试范例，每个结节图像数据只有一个由一名放射科医生描绘的注释掩模[2]-[6]。因此，网络每次只能提供结节区域的单个预测。\n然而，在临床实践中，不同的放射科医生由于其不同的培训和临床经验可能会为肺结节提供不同的分割注释[7]-[9]。\n因此，基于单一注释的传统方法无法反映临床经验的多样性，限制了深度学习方法的应用。\n解决放射科医生之间注释不同问题的一个直接解决方案是为每个肺结节图像合并多个注释。这导致了另一个问题：多个注释不可避免地会带来不确定性和冲突，因为放射科医生可能会对同一区域进行不同的注释。为了克服这个问题，Kohl 等人在 2018 年提出了一种概率 U-Net，它利用条件变分自动编码器将多个分割变体编码到低维潜在空间中 [8]、[10]。通过从该空间采样，网络可以影响相应的分割图。基于这项研究，Hu等人提出将真实不确定性与概率UNet相结合，这可以提高预测不确定性估计、样本准确性和样本多样性[7]。这些方法依赖于潜在空间和该空间中的随机样本。因此，这些方法只能通过多次预测来提供不确定区域。\n在本文中，我们提出了一个论点，即多个注释之间的不确定性遵循特定的模式。\n为了演示这种现象，我们引入了多重置信掩码 (MCM)，它结合了高置信度 (HC) 掩码和低置信度 (LC) 掩码，如图 1 所示。 A. 交叉掩码等于 HC mask，代表所有注释的交集。\n联合掩码是所有注释的联合。 LC掩模是交集掩模和并集掩模之间的差异。当在 LIDC-IDRI 数据集 [11] 上计算 HC 和 LC 的 Hounsfield 单位 (HU) 核估计时，如图 1.B 所示，我们可以观察到 LC 和 HC 掩模之间的 HU 分布存在明显区别。具体地，LC区域具有比HC区域更低的HU值。从像素分布来看，HU值越低，对应区域的密度越低。就CT图像特征而言，LC区域主要由结节边缘、毛刺和磨玻璃特征等边界相关特征组成，而HC区域主要分布在结节核心内。因此，我们提出了这样的假设：导致放射科医生之间差异的区域主要与低密度组织和边界相关特征有关。\n与其他方法不同，我们建议利用 MCM (多重置信掩码) ** 和注释集作为具有不同分割确定性的特征的学习指导**，有助于更好的分割性能。我们将这种训练称为UncertaintyAware Attention Mechanism，如图2所示。按照这种机制，我们进一步设计了用于肺结节分割的Uncertainty-Guide Multi-Confidence Segmentation Network（UGMCS-Net）。\nUGMCS-Net 包含三个模块：基于 U-Net 的特征提取模块、不确定性感知模块和交集并集约束模块。\n首先，标签分为交集标签( $L_\\cap$ )、并集标签 ( $L_\\cup$ )、原始标签 (L)。\n其次，HC (consensus) 表示 $L_\\cap$ 、 LC（disagreement）表示 $L_\\cup$ - $L_\\cap$。\nMCM (Multi Confidence Mask) 表示 HC 和 LC的统称\n首先，特征提取模块从输入的CT图像中提取通用特征图R。其次，不确定性感知模块在标签的交集、并集、和原标签的指导下，将通用特征图R转换为三个独立的特征图$R_{LC}$、$R_{HC}$和$R_{Uni}$。 $R_{LC}$、$R_{HC}$用于预测并集掩码和交集掩码，并将结果组合为MCM。$R_{Uni}$用于预测初步分割结果。我们稍后使用 $\\cup(X)$, $\\cap(X)$, $X_{Uni}$ 来表示预测的并集掩码、交集掩码和初步分割结果。第三，约束模块使用来自$R_{LC}$、$R_{HC}$ 和 $R_{Uni}$的特征感知注意块捕获首选特征，然后用特征距离约束最终预测$X_S$，确保分割结果以合理的方式受到约束。为了更好地利用多个注释，我们还引入了多注释融合损失来优化 $X_{Uni}$和 $X_S$，它计算预测和所有注释之间的平均 BCE 损失。\n该方法具有两个明显的优点： （1）与学习潜在空间的传统基于 VAE 的方法相比，该方法具有特定的学习目标，使其能够提供不确定结节区域的稳定预测。 （2）该方法利用所有注释来优化预测，以确保最终预测平衡不同条件，充分利用可用信息。\n我们在之前的出版物中报告了这项工作的初步版本[12]。本文的新贡献可概括如下: （1）一种称为不确定性感知注意机制（UAAM）的新颖机制：UAAM 最大限度地利用多个注释，并采用多重置信掩码（MCM）来指导低置信度和高置信度特征的学习。\n（2）升级后的Uncertainty-Guide Multi-Confidence Segmentation Network (UGMCS-Net)：基于该机制，我们将UGS-Net更新为UGMCSNet，其中包含特征提取模块、不确定性感知模块和新的交并集约束模块。为了充分利用多个注释，我们还引入了多注释融合损失。所提出的模块是即插即用的，可以应用于不同情况下的其他分割网络。\n（3）全面验证：我们提出了ComplexNodule Validation，测试UGMCS-Net对U-Net难以分割的肺结节的分割性能。实验表明，对于UNet上DSC分数低于60％的结节，我们网络的DSC分数可以提高11.03％，我们网络的IoU分数可以提高11.88％。我们还为不同的模块、主干和模型设置提供足够的消融研究。\nRelated Work 2.1 Lung Nodule Segmentation 肺结节分割对于肺结节计算机辅助检测 (CAD) 系统至关重要。其主要目标是准确地描绘目标结节的边界，以提供其直径、大小和语义特征等细节[13]-[16]。这项任务的主要挑战是肺结节具有各种形状、大小和微妙的特征。早年，研究人员提供了多种肺结节分割方法，例如基于形态学的方法和基于区域生长的方法[17]，[18]。近年来，深度学习已成为该领域最流行的方法。\n2017年，Wang等人提出了一种用于肺结节分割的多视图卷积网络。所提出的网络同时从 CT 图像的轴向、冠状和矢状视图中捕获了一组不同的结节敏感特征。使用多分支 CNN 网络对这些特征进行分析，平均 DSC 相似系数 (DSC) 为 77.67% [19]。此外，Wang 等人在 2017 年提出了一种具有中心池层的中心聚焦卷积神经网络，可以彻底分析 2D 和 3D 结节 [1]。 2020年，Cao等人设计了带有强度池层的双分支残差网络，增强了强度信息的学习，并将DSC提高到82.74％[20]。 2021年，Pezzano等人推出了一种CNN网络，可以通过生成两个代表CT中所有背景和次要重要元素的掩模来学习结节的背景，从而使网络可以更好地区分结节特征[15]。后来在2022年，Shariaty等人进一步提出了纹理特征提取和特征选择算法来改进分割，实现了84.75%的DSC[2]。\n根据上述研究的观察结果，显然现有方法主要优先考虑实现更精确的分割，而忽略了不同放射科医生对如何分割同一肺结节可能持有不同意见的事实。在这项研究中，我们认为注释之间的分歧也具有诊断价值。因此，我们的方法旨在生成一个分割，通过从注释集学习并识别具有不同分割确定性的区域来有效地平衡所有注释。\n2.2 Uncertainty in Lung Nodule Segmentation 许多医学图像视觉问题都存在模糊性。在临床情况下，仅通过 CT 扫描可能无法明确哪个特定区域是癌组织 [10]、[21]。因此，即使是经验丰富的医生和放射科医生也可能对相同的组织或肿瘤提供不同的分割。\n2018 年，Kohl 等人提出将此任务建模为学习肺结节多样化但合理的分割上的分布。基于 U-Net [5]，他们引入了概率 U-Net，它是 UNet 和条件 VAE 的组合，可以产生无限数量的合理分割。 2019年晚些时候，Kohl等人进一步提出了一种分层概率U-Net，它使用分层潜在空间分解来制定高保真度分割的采样和重建[8]。同样在 2019 年，Hu 等人分析了两种类型的不确定性：任意的和认知的 [7]。他们利用多个注释的可变性作为“ground truth”任意不确定性的来源，将这种不确定性与概率 UNet 结合起来，并尝试定量分析分割不确定性。 2021年，Long等人将[7]中的概念扩展到V-Net和3D肺结节CT图像。作为包含 1000 多个肺结节的多个注释的理想数据集，所有这些研究 [7]-[10] 都分析了 LIDC-IDRI。\n与基于VAE的网络不同，我们的工作更关注导致各种标注的原因，表现为分割分歧。我们引入了一种专门针对不确定性区域的替代方法，使我们能够对不确定的结节区域和整体肺结节分割做出稳定的预测。这种方法使我们能够深入了解分割差异的根本原因，并在不确定的肺结节区域中产生更可靠的结果。\n在医学图像处理中，\u0026ldquo;条件变分自编码器（Conditional Variational Autoencoder, CVAE）\u0026ldquo;是一种生成模型，它结合了变分自编码器（VAE）的特性和条件生成的能力。VAE是一种深度学习模型，能够学习输入数据的潜在表示，然后从这些表示中生成新的数据实例。CVAE在此基础上增加了条件变量，使得生成的过程可以依赖于某些条件或标签。\n对于医学图像，CVAE可以用于多种任务，如生成特定类型的医学图像（例如，根据特定疾病状态生成CT或MRI图像），数据增强（生成新的训练样本），以及特征提取和表示学习等。通过将条件信息（如疾病标签、图像类型或患者信息）融入到生成过程中，CVAE能够生成更符合特定条件的图像，从而在特定医学应用中发挥作用。\nMethod 3.1 Uncertainty-Guided Multi-Confidence Segmentation Network 在图 3 中，我们展示了 UncertaintyGuided Multi-Confidence Segmentation Network (UGMCSNet) 的架构。该网络以肺结节 CT 图像作为输入，并产生两个输出：预测的多置信度掩模（MCM）和最终的分割 $X_S$。 MCM 结合了预测的并集 $\\cup(X)$ 和交集 $\\cap(X)$。网络的学习目标是注释集 GT，以及它们的 Union Mask $\\cup(GT)$ 和 Intersection Mask $\\cap(GT)$。输入图像及其相应的掩模的尺寸为 50 × 50 像素，通过从带有官方注释的 LIDC-IDRI 数据集裁剪获得。在输入网络之前，输入图像和掩模的大小被调整为 3 × 64 × 64 像素的尺寸。\nUGMCS-Net 包含三个模块：(1) 特征提取模块，(2) 不确定性感知模块，(3) 交并并约束模块。特征提取模块可以使用任何基于UNet结构的分割网络，初步获得形状为32×64×64的特征图R。本文使用具有五个下采样和上采样层的Attention U-Net [4] 。每个上采样层由两个卷积层和一个注意力块组成。不确定性感知模块分析 R 并生成$R_{LC}$、$R_{HC}$和$R_{Uni}$。然后将这些特征图输入 MCM BCE Loss Block 和 Multiple Annotation Loss Block，生成初始的 $\\cup(X)$、$\\cap(X)$ 和合理的分割 $X_{Uni}$。计算并集以 $\\cup(X)$、$\\cap(X)$获得 MCM。 Intersection-Union Constraining Module 学习 $R_{LC}$、$R_{HC}$和$R_{Uni}$的不同特征，并将这三个特征融合到$R_{final}$ 中。然后该模块通过分析RF ianl提供更合理的最终分割$X_S$。\n3.2 Uncertainty-Aware Module 引入不确定性感知模块（UAM），通过学习$\\cup(GT)$、$\\cap(GT)$ 和 GT来充分合理地利用所有注释信息。该模块有两个任务：（1）从低置信度（LC）区域、高置信度（HC）区域和所有注释中捕获不同的特征； (2) 生成多重置信掩模 (MCM) 的初始预测和一般分割。\n如图3所示，UAM采用三分支CNN网络作为骨干。它以 R (32 × 64 × 64) 作为输入，并使用内核大小为 1×1 的三个不同卷积层提取 $R_{LC}$、$R_{HC}$和$R_{Uni}$。 $R_{LC}$、$R_{HC}$和$R_{Uni}$的大小相同，均为 32 × 64 × 64。 MCM BCE Loss Block 接收 $R_{LC}$、$R_{HC}$ ，用三个不同的卷积层生成 $\\cup(X)$ 和 $\\cap(X)$ ，内核大小为 3× 3. BCE损失计算$\\cup(X)$ 和 $\\cup(GT)$的损失以及$\\cap(X)$ 和 $\\cap(GT)$。 $\\cup(X)$和$\\cap(X)$通过归一化操作Normal($\\cup(X)$+$\\cap(X)$)组合为MCM\u0026rsquo;，反映了不同区域的不确定性程度。与我们之前的工作 [12] 不同，$R_{Uni}$ 的分支是通过多重注释损失块进行优化的，这将在稍后讨论。此外，具有相同形状的 1 × 64 × 64 的特征图 $R_{LC}$、$R_{HC}$和$R_{Uni}$ 将被输入到下一个模块中进行进一步分析。\n主要还是用来生成一个MCM\n3.3 Intersection-Union Constraining Module 如上所述，$\\cup(GT)$和$\\cap(GT)$ 是UAM的学习目标。具体地，$\\cup(GT)$表示所有可能是结节组织的区域，表示置$\\cap(GT)$信度最高的结节区域。为了在极端情况之间实现平衡，我们进一步开发了一个新模块，称为交集并集约束模块（IUCM）。\n该模块旨在捕获所有三个学习目标的特征，并产生更合理的分割预测，可以在极端情况之间取得平衡。\n如图 4 所示，IUCM 将 $R_{LC}$、$R_{HC}$和$R_{Uni}$作为输入，并将对应的 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$与特征感知注意块 (FAAB) 对应。 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$的尺寸相同，均为 32 × 32 × 32。FAAB 是基于自注意力块 [22] 和特征感知滤波器构建的。\n这些注意力块使用不同的特征感知滤波器处理$R_{LC}$、$R_{HC}$和$R_{Uni}$，使网络能够针对不同的学习目标制定不同的学习偏好，并获得更多有助于分割的图像特征[23]、[24]。更具体地说，假设输入$R_z$，FAAB的过程可以总结为：\n其中 z ∈ {Uni, LC, HC},A表示自注意力架构。 Г是一个特征感知滤波器，在本研究中，$R_{Uni}$和$R_{LC}$的Г是Gabor[25]，$R_{HC}$的Г是Otsu[26]。 Γ(A($R_z$)) 与$R_z$逐像素相加，以便网络可以从输入中保留更多信息。\n通过对数据集和Hounsfield Unit Kernel Estimations的观察，我们可以看到，$R_{HC}$主要是密度较高的实性结节，而$R_{LC}$则包括更多的低密度组织（如毛刺），主要分布在结节的边缘。 Otsu对密度特征敏感，可以帮助网络更准确地识别高密度组织。因此，我们应用 Otsu 从 $R_{HC}$ 中提取 $R_{HC}^{\\prime}$。同时，Gabor对图像边缘敏感，能够提供良好的方向选择和尺度选择特征，从而能够捕获图像局部区域多个方向的局部结构特征。因此，我们选择Gabor从$R_{LC}$和$R_{Uni}$中提取$R_{LC}^{\\prime}$和$R_{Uni}^{\\prime}$。关于过滤器选择的消融研究将在第四节中提供。\n得到 $R_{LC}^{\\prime}$、$R_{HC}^{\\prime}$和$R_{Uni}^{\\prime}$后，IUCM 得到 $S_z$ = d{$R_Z$,$R$},d是计算余弦相似度的运算。\nIUCM 的输出为 $R_{Aug}$ = Concat($S_{Uni}$× $R_{Uni}^{\\prime}$ ; $S_{LC}$× $R_{LC}^{\\prime}$; $S_{HC}$× $R_{HC}^{\\prime}$ )。$R_{Aug}$将与来自特征提取模块的 R 连接，输入到卷积层，并生成最终的分割预测 $X_S$。 R 和 $R_{Aug}$ 的串联保留了来自 CT 输入的更多信息。\n3.4 Loss Function 在图 3 中，UGMCS-Net 包含两个优化：MCM BCE 损失块和多注释损失块。\nMCM BCE Loss Block 计算$\\cup(X)$ 和 $\\cup(GT)$之间; $\\cap(X)$ 和$\\cap{(GT)}$之间的 BCE 损失可表示为：\n我们使用多注释融合损失来优化多注释损失块中的$X_{Uni}$和$X_S$，表示为Φ。在我们之前的工作中，只选择了一组注释来优化$X_{Uni}$和$X_S$，这导致其他注释中有价值的信息丢失。\n本研究引入了多重注释融合损失，它将预测与所有可能的注释进行比较。\n首先，$R_{Uni}$和$R_F$最终产生$X_{Uni}$和$X_S$。其次，如图 5 所示，多注释融合损失函数计算优化对象（$X_{Uni}$和$X_S$）与注释集之间的 BCE 损失，并合并这些损失的平均值。根据我们的设计，$X_{Uni}$应该倾向于整个注释集，XS应该从注释集中学习足够的信息并产生平衡所有注释的分割，因此我们选择使用多注释融合损失来优化$X_{Uni}$和$X_S$。我们有：\n网络的损失融合可以定义为：\n其中 α1、α2 和 α3 是预定义参数。根据经验，本文中α1设置为0.5，α2设置为0.5，α3设置为1。重量选择的消融研究将在第四节中显示。\nEXPERIMENT 4.1 Dataset and Experimental Settings 在本研究中，我们使用 LIDC-IDRI 数据集 [11] 评估所提出的网络，该数据集由 1018 个研究实例和超过 2600 个结节组成。在本研究中，我们选择了 1860 个直径为 3-30mm 且具有多个注释的肺结节。每个结节数据至少有两个注释，以便我们可以获得其CT图像、多个注释集GT、它们的并集$\\cup(GT)$和交集$\\cap(GT)$。输入图像及其掩模均为 50 × 50 像素，根据官方注释从 LIDC-IDRI 数据集中裁剪。每个像素反映 CT 图像的亨斯菲尔德单位。\n在训练之前，我们将 CT 图像的强度值裁剪到范围 [1000,1000] 并将所有强度值归一化到范围 [0, 1]。数据处理的代码在网站 https://github.com/qiuliwang/LIDCIDRI-Toolbox-python 上提供。\n我们在运行 Ubuntu 18.04、Tesla V100 GPU 的服务器上进行实验，使用 CUDA 11.2，GPU 内存约为 16G。该网络使用PyTorch-v1.0.1和Python3.7实现。我们使用五重验证来评估网络的有效性，确保数据分割的稳健性。我们使用热重启随机梯度下降（SGDR）作为优化器，初始学习率（LR）为0.00001，批量大小为32，动量为0.9，权重衰减率为0.0001。每个网络都训练 200 个 epoch，学习率每 50 个 epoch 更新一次。 UGMCS-Net的源代码、原始USGNet以及所有实验设置将上传到https://github.com/yanghan-yh/UGS-Net。\n4.2 Performance of Lung Nodule Segmentation 每个结节的相关注释Label1，它是注释集中的第一个。 Probabilistic U-Net是一种基于VAE的模糊分割方法，因此我们取其四个样本的平均值作为最终的分割结果。本研究使用三个指标来评估网络对病变区域的预测能力：平均 Dice 相似系数（DSC）、交集交集（IoU）和归一化表面 Dice（NSD）[35]。在表 I 中，所有方法均使用 Label1 进行评估。\n这实验数据太丰富了把！！！\n表I显示UGMCS-Net在DSC、IoU和NSD方面取得了最高分数，分别为87.65%（±0.56%）、78.78%（±0.83%）和95.62%（±0.59%）。与 U-Net 相比，UGMCS-Net 在三个指标上分别提高了 1.39%、1.99% 和 1.16%。同样，与 Attention U-Net 相比，UGMCS-Net 在各个指标上实现了 0.98%、1.45% 和 0.68% 的增强。这些结果凸显了 UGMCS-Net 卓越的分割性能，特别是 NSD 分数的大幅提高，表明其强大的边界特征分割能力。此外，与 UGSNet 相比，UGMCS-Net 在所有指标上都表现出了相当大的进步，DSC 分数提高了 0.49%，IoU 分数提高了 0.74%，NSD 分数提高了 0.34%。此外，从 UGMCS-Net 的五重交叉验证中获得的三个指标的方差始终小于 UGS-Net 的方差，表明通过集成多重注释融合损失和约束操作增强了网络稳定性。 nnU-Net 是用于分割任务的流行网络。然而，它在 DSC 中仅达到 84.60%，在 IoU 中仅达到 74.45%。这是因为nnU-Net的训练需要很大的数据集。然而，我们在此任务中只有 1860 个结节图像。\n图6显示了上述方法的部分分割结果。输入列中的红色框表示感兴趣的区域或结节容易出错的分割位置。结节(a)-(c)包含许多低密度区域，结节(d)-(f)在其边界处具有不规则形状，例如毛刺迹象，并且结节(g)-(h)具有空腔。 UGMCS-Net 对这些区域的分割明显比其他方法更符合病灶的实际形状。\n对比实验结果的套话 （可以学习一下）\n（U-Net、Attention U-Net、R2U-Net、Channel U-Net、Nested U-Net、UGS-Net 和 UGMCS-Net 的分割结果。输入列对应的红色框表示分割时应注意的特征或结节容易出错的位置。 UGMCS-Net 列中的红色框表示 UGMCS-Net 在这些位置的分割细节。绿色框表示次优分割结果的不足之处。最后一栏是相应结节的直径，单位为毫米）\n这个定性结果的对比方法，值得学习。\n图 6 和表 I 表明：（1）从注释集及其并集和交集中学习，为分割任务提供了更丰富的视觉信息。\n(2)从LC区域学习提高了网络识别低密度区域的能力。\n值得注意的是，图6（g）-（h）中，标注中没有空洞，但UGMCS-Net得到的分割结果反映了空洞特征。我们选择保留这些特征有两个原因：（1）常见结节组织的密度高于肺实质。但肺结节内的空腔密度极低，甚至是空的。\n我们可以将它们视为结节的一部分，也可以不视为结节的一部分。 (2) 分割图应提供更多有关结节特征的信息。空腔是重要的特征，因此保留这些空腔是更好的选择。如果需要，可以使用 cv2.findContours 等方法轻松去除预测中的空洞。\n表II，UGMCS-Net对于U-Net、Attention UNet、UGS-Net在Dice和IoU分数中的p值远小于0.05，并且t值的绝对值较大，表明UGMCS-Net在性能上明显优于U-Net、Attention U-Net和UGS-Net。\nP值 (P-value)：在统计分析中，P值用于量化结果发生的概率，假设零假设（通常是无效假设，如两组之间无差异）为真。在医学图像分析的研究中，如果涉及到统计检验（例如，比较两种分割方法的性能），P值可以用来表示这种比较的显著性水平。 V值：这个术语在医学图像分析中不常见，可能指的是特定研究或技术中使用的一个特定参数或度量。例如，它可能代表体积（Volume）的度量，特别是在分析器官大小或肿瘤体积时。在不同的上下文中，它可能有不同的含义。 上面的实验是基于注释集中的第一个注释Label1 。为了消除掩码选择的影响，我们还提供了 U-Net、Attention U-Net、UGS-Net 和 UGMCS-Net 在标注集中的第二个标签 Label2 上的性能。表III中列出的实验结果表明UGMCS-Net在Label2上保持了其优越的性能。这意味着所提出的方法可以在不同的掩模选择上保持稳定的性能。在传统的训练方法中，每个结节都被分配一个单一的掩模，无法为具有复杂结构特征的结节提供足够的信息。\n在我们的网络中，每个结节与 2-4 个掩模相关联。通过整合多重注释融合损失，我们将更全面的信息注入到学习过程中。它对于分割具有复杂结构和低密度纹理的结节特别有益。因此，多重注释融合损失显着提高了性能，特别是对于具有复杂结构的结节。\n4.3 Uncertain Region Prediction 除了能够分割结节之外，UGMCS-Net 还可以预测更有可能是结节组织的区域和可能性较低的区域。图 7 说明了预测结果 $\\cup(X)$和$\\cap(X)$、最终分割结果 $X_S$ 以及生成的 MCM’。在MCM’和MCM+UGSNet中，红色表示高置信度区域，蓝色表示低置信度区域，绿色对应于最终分割$X_S$。在理想情况下，$X_S$应有效地在高置信度和低置信度区域之间取得平衡。\n（预测交集$\\cap(X)$、预测并集$\\cup(X)$、最终分割$X_S$ 和 MCM 由 UGMCS-Net 生成。 MCM 中的颜色用于更好的可视化，红色表示$\\cap(X)$，蓝色表示$\\cup(X)$。此外，最终的分割在 MCM 中表示并标记为绿色以方便比较。红色框表示不易区分的结节区域或特征。最后一栏是相应结节的直径（以毫米为单位）)\n根据图 7，我们的最终分割结果位于高置信度区域和低置信度区域这两种极端情况之间。这些中间结果表明（1）我们的预测认识到所有潜在的注释，并且（2）预测被限制在$\\cap(X)$和$\\cup(X)$之间。\n具体来说，在肺结节特征分割中使用MCM具有以下几个优点，可以更好地显示结节的语义特征：（1）MCM可以更好地突出结节的显着空腔特征（图7.（g）（h））。与其他方法相比，UGMCS-Net 的预测掩模上的结节腔特征更加明显。这是由于UGMCS-Net能够在交叉掩模$\\cap(GT)$和联合掩模$\\cup(GT)$的指导下捕获结节组织的密度差异，这有助于保留结节腔的更多特征。\n(2)MCM可以更好地分割毛刺征象，这是诊断良恶性肺结节的重要特征(图7.(a)-(f))。针状结构是由结节侵入周围组织引起的星状变形，通常是低密度的并分布在结节边缘周围。这一特征是传统深度学习方法难以分割的。 UGMCS-Net在union mask $\\cup(GT)$的指导下可以更加关注结节边界特征，从而对分布在结节边界的毛刺有更好的分割性能。\n（3）MCM可以更好地分割结节的低密度组织（图7.（i）-（l）），该区域常见于磨玻璃结节，是造成专家标记差异的主要区域。 UGMCSNet通过对注释集GT和union mask $\\cup(GT)$的研究，可以最大程度地识别低密度组织，这对于磨玻璃结节的诊断很有帮助。\n由于LC掩模尺寸较小，传统的定量评估指标如DSC和IoU不足以衡量MCM的预测质量。为了解决这个问题，我们将预测的 HC 和 LC 掩模的 HU 分布与实际的 HC 和 LC 掩模进行比较。我们假设UGMCS-Net能够合理地预测不同区域的不确定性程度。因此，预测的 HC 和 LC 掩模的 HU 值分布应与实际分布相似。图8显示预测曲线与实际曲线吻合较好，表明我们预测的区域不确定性水平在统计上是可靠的。\n4.4 Ablation Study 我真的要被这个工作量给吓鼠了，太能做实验了把\n模块的消融研究：如果不指出实用性，我们会在每个部分进行五倍验证。为了更好地利用多个注释的信息并增强 $R_{LC}$ 、 $R_{HC}$ 和 $R{final}$ 之间的关系，我们用多注释融合损失和交叉联合约束模块更新了 UGMCS-Net。为了进一步证明这两个模块的贡献，我们基于 UGMCS-Net 构建了 UGMCS-$Φ_a$、UGMCS-$Φ_b$、UGMCS-$Φ_a$+$Φ_b$ 和 UGMCS-IUCM 进行消融实验。在UGMCS-$Φ_a$中，多注释融合损失仅应用于$\\cup(X)$;在UGMCS-$Φ_b$中，Multiple Annotation Fusion Loss仅应用于网络XS的最终输出；在UGMCS-$Φ_a$+$Φ_b$ 中，多重注释融合损失应用于$\\cup(X)$和$X_S$；在UGMCS-IUCM中，我们使用USG-Net中的Intersection-Union约束模块，但没有多重注释融合损失。\n我们的 UGMCS-Net 及其四种变体的性能列于表 IV，“-”表示 Attention U-Net。 V1是指没有IUCM和多重注释融合损失的UGS-Net，它作为其他变体的基础网络。\n结果表明：（1）UGMCS-$Φ_a$、UGMCS-$Φ_b$网络相对于UGMCS-Net的性能改进表明，所有标注信息的融合可以使网络更准确地捕获结节区域并获得更好的分割性能。 (2) UGMCS-$Φ_a$+$Φ_b$ 网络的 DSC、IoU 和 NSD 高于 UGMCS-$Φ_a$、UGMCS-$Φ_b$网络，表明同时对 UAM 和最终输出使用多重注释融合损失优于单独使用其中之一。 (3) 我们的 UGMCS-Net 优于UGMCS-$Φ_a$+$Φ_b$和 UGMCS-IUCM，证明了交集并集约束模块的有效性。尽管仅添加交集-并集约束模块（UGMCS-IUCM）时网络的定量性能略有下降，但观察到了显着的定性改进，这将在后面讨论。此外，UGMCS-Net的优越性能表明，多重注释融合损失和交叉联合约束模块可以相互增强，约束不确定性并促进更好的分割性能。\n为了进一步验证多注释融合损失和交叉联合约束模块的有效性，我们在图 9 中使用 Grad-CAM [36]、[37] 演示了特征图可视化。每种情况下的结果代表了网络的最终预测。 M3、M2和M1分别表示不同网络配置下倒数第三个、第二个和第一个卷积层的视觉特征图。基于图9，我们观察到：（1）当将多重注释融合损失应用于$\\cup(X)$或$X_S$时，网络对低密度组织的识别能力显着提高（UGMCS-$Φ_a$和UGMCS-$Φ_b$，结节A-D)； （2）$\\cup(X)$或$X_S$中同时使用Multiple Annotation Fusion Loss，可以在提高对低密度组织的敏感性的基础上，使网络勾勒出结节边界更加清晰（UGMCS-$Φ_a$+$Φ_b$网络，结节A-D）。 (3) IntersectionUnion Constraining Module使网络能够学习更多的边界特征，例如spiculation（毛刺）（UGMCS-IUCM网络，Nodule A-C）。 （4）当同时使用Multiple Annotation Fusion Loss和Intersection-Union Constraining Module时，网络注意力转移到结节边界，勾画出更加合理完整的结节区域（UGMCS-Net，Nodule A-E）。\n如图9所示，在IUCM的帮助下，网络可以针对复杂结节获得更语义化、更合理的分割结果。然而，如表IV所示，与UGMCS-$Φ_a$+$Φ_b$相比，UGMCS-Net在DSC、IoU和NSD上的性能增益较弱。我们认为造成这种现象的原因有3个：（1）IUCM专注于提高复杂结节的分割性能，与测量上的改进相比，IUCM使得模型对分割结果的性能提升更加显着（进一步验证在第 IV-E 节）。\n（2）复杂结节仅占结节总数的一小部分，因此IUCM无法显着提高模型在各项指标上的得分。 (3)数据集中仍然存在一些失败案例。如图10所示，在这些情况下，UGMCS-IUCM和UGMCS-Net获得的分割掩模包含更多的结节组织并且是更准确的病变区域，但它们的DSC分数较低。\n由于不同医生领域的知识偏差会影响groundtruth，因此对于分割任务来说，获得更准确、更合理的分割掩模通常比更高的度量分数更有意义。\n表V显示了加入UAM和IUCM后模型复杂度的增加。显然，UAM 和 IUCM 可以使模型以很少的计算成本获得更好的性能。\nBackbone 的消融研究：我们测试 U-Net 和 R2U-Net 作为三个模块的骨干。如表六所示，以U-Net为骨干的模型在DSC、IoU和NSD上分别获得了87.04%、78.07%和94.50%的分数。\n以R2U-Net为骨干的模型在DSC、IoU和NSD上分别获得了86.20%、76.82%和93.75%的结果。实验结果表明，所提出的特征提取模块、不确定性感知模块和交集并集约束模块是即插即用的。\n我们之前的工作评估了 nnU-Net [22] 作为骨干网络。\n这个网络在这个任务中有两个缺点：（1）它占用了太多的计算资源，（2）我们只有 1860 个结节，这可能会导致过度拟合。为了平衡计算资源和性能增益，我们在本工作中选择 Attention U-Net。\n交叉点联合约束模块中滤波器的消融研究：在第 III-C 节中，我们讨论了 Otsu 对密度特征的敏感性，使模型能够准确识别高密度组织。因此，我们利用 Otsu 的方法来提取 $R_{HC}$ 的特征。相反，Gabor对图像边缘敏感并提供有效的方向和尺度选择特征，被选择用于 $R_{LC}$的特征提取。在这些部分中，我们进行了涉及 Fold1 上 IntersectionUnion 约束模块内的五个过滤器设置的实验。表 VII 概述了这些设置。如表 VII 所示，我们的最终配置产生了最高的 DSC 分数。\n我们在图 11 中提供了 $R_{LC}^{\\prime}$ 和 $R_{HC}^{\\prime}$ 的特征可视化。可以看出， $R_{LC}^{\\prime}$ 的可视化更多地集中在结节的边缘，其中包含更多的低置信度区域。相比之下， $R_{HC}^{\\prime}$ 的可视化更关注结节的核心，具有更高的分割置信度。这些可视化结果重新证明了我们过滤器设计的有效性。\n参数消融研究：在方程 4 中，存在三个手动设置的参数：α1 指定为 0.5，α2 指定为 0.5，α3 指定为 1。在本节中，我们进行五重验证并说明这些参数选择背后的基本原理。\n如表VIII所示，当α1设置为0.5，α2设置为0.5，α3设置为1时，所提出的方法达到其峰值性能。值得注意的是，α3代表最终分割的权重，这意味着XS在训练过程中发挥着重要作用。\n此外，我们对三个分支的概率图加权平均融合进行了实验，希望网络能够为IUCM中的每个分支选择合适的权重。然而，我们观察到与 $R_{Uni}$ 对应的分支可以为结节分割提供更通用的特征，并且比其他两个分支显得“更强”。因此，其他两个分支的权重往往会减少到 0。根据这些观察结果，我们选择直接融合概率图。\n4.5 Complex-Nodule Validation UGMCS-Net 从可能导致分割不确定性的区域学习特征。因此，它可以更好地分割具有大的低密度区域或复杂结构的结节。为了更好地证明其对 U-Net 难以分割的结节的改进，我们设计了一个 Complex Nodule Validation。基于五重验证U-Net，我们进一步选择了DSC分数低于60％、70％和80％的三组结节。然后使用相同的数据设置训练 Attention U-Net、UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net，以再次测试这些结节并比较 DSC 和 IoU 分数。\nUGMCS-Net 在复杂结节上的性能提升是显而易见的。与Attention U-Net相比，对于U-Net上DSC分数低于60%的结节，UGMCS-$Φ_a$+$Φ_b$导致平均DSC分数提高1.58%，UGMCS-IUCM使平均DSC分数提高5.12%，UGMCS-平均 DSC 得分净产量增加了 5.45%。对于 U-Net 上 DSC 得分在 60% 到 70% 之间的结节，UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 的平均 DSC 得分分别提高了 0.69%、3.11% 和 5.07%。同样，对于 U-Net 上 DSC 分数在 70% 至 80% 之间的结节，UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 的平均 DSC 分数相应提高了 0.14%、1.52% 和 1.64%。从上述差异可以看出，UGMCS-IUCM对于复杂结节有很大程度的性能提升。根据表一，我们的网络在整个数据集上仅将 DSC 提高了 0.89%，但相对于 U-Net 上 DSC 分数低于 60% 的结节而言，相对于 Attention U-Net 具有很大的性能提升。这是因为具有复杂结构的结节仅占所有数据的一小部分。\n图12显示了一些复杂结节的分割结果。红色下标是UGMCS-Net的分段DSC，黑色下标是UNet的DSC。可以看出，U-Net分割DSC得分低于60%的结节是一些低密度或毛玻璃组织。 UGMCS-Net在这些结节中的显着改进表明UGMCS-Net可以更好地学习结节低密度组织的特征并更准确地分割低密度结节病变区域。当U-Net分割DSC得分低于70%时，可以观察到除了一些低密度结节外，一些结节还存在不规则空洞、毛刺、组织内突然出现亮点或肺壁过亮。 UGMCS-Net在这些结节上令人信服的分割性能反映了UGMCS-Net对边界特征、密度差异的学习能力以及良好的抗噪声能力。当U-Net分割DSC得分低于80%时，我们观察到许多新结节具有更多的实体组织。在这种情况下，UGMCS-Net可以准确地确定结节区域。此外，对于大多数结节，UGMCS-Net的分割结果反映了更多的语义特征，具有更强的可解释性。\n复杂结节验证中的分割性能分析。 UGMCS-$Φ_a$+$Φ_b$、UGMCS-IUCM 和 UGMCS-Net 平均 DSC 和 IoU 后面是与 Attention U-Net 相应度量的差值（绿色数字）。所有指标均以百分比表示。\n（复杂结节验证：该验证测试了 UGMCS-Net 对 U-Net 难以分割的肺结节进行三个级别的分割性能。每张CT图像的最后两个掩模分别是UGMCS-Net和U-Net的分割结果。U-Net的分割结果以黑色显示，UGMCS-Net以红色显示。所有指标均以百分比表示。）\nDISCUSSIONS 长期以来，肺结节分割的任务一直致力于实现高精度，其中 DSC 或 IoU 是主要目标。然而，考虑到肺结节尺寸小且结构复杂，如果分割结果以不同置信度突出显示区域，可能对放射科医生更有帮助。高置信度区域提供了结节或肿瘤组织的主要部分，而低置信度区域则包含重要的低密度特征，例如毛玻璃状和毛刺征，放射科医生也应注意这些特征。\n所提出的方法旨在提供对临床诊断更有用的信息，而不是简单地改进 DSC。它并不寻求取代医生的临床作用，而是通过允许他们利用人工智能方法的优势来补充医生的临床作用。我们相信，这是将人工智能融入临床实践的更好方法。\n5.1 Data Requirement 我们的方法并不需要每个放射科医生都对所有结节进行注释。如 [11] 所示，项目期间共有 12 名放射科医生参与了所有五个站点的图像注释程序。鉴于大多数结节由 1-4 名放射科医生进行注释，可以想象结节可能由不同的放射科医生进行注释。\n尽管如此，不同放射科医生参与注释结节不会妨碍我们方法的适用性。尽管不同放射科医生之间的注释风格可能存在差异，但值得注意的是，训练有素的放射科医生遵循既定的结节注释标准，例如[38]。这些标准确保不同组的放射科医生提供多样化但基本一致的注释。\n5.2 Limitation 我们研究的一个局限性是我们仅在 LIDC-IDRI 数据集上测试我们提出的方法，该数据集是目前唯一公开可用的肺结节完整注释数据集。尽管已有十多年的历史，该数据集仍然提供了许多研究机会。然而，对多个注释的需求可能会限制我们的方法在现实临床环境中的实用性，其中获得多个注释可能并不总是可行。为了解决这个限制，我们计划探索能够基于单个注释自动识别高置信度和低置信度区域的技术，从而提高我们方法的可行性和适用性。\nCONCLUSIONS 多个注释之间的协议，以改进分割并识别分割置信度较低的区域。UAAM 从多置信度模板 (MCM) 中捕获特征，多置信度模板是低置信度 (LC) 模板和高置信度 (HC) 模板的组合。基于UAAM，我们进一步设计了不确定性引导分割网络（UGMCS-Net），其中包含特征提取模块、不确定性感知模块和交集并集约束模块。这些模块共同从多个注释之间的共识或分歧中学习有价值的信息，提供具有高和低分割置信度的区域，以及可以平衡所有可能性的分割结果。除了传统的验证方法之外，我们还提出了 LIDC-IDRI 上的复杂结节验证，测试 UGMCS-Net 对 U-Net 难以分割的肺结节的分割性能。\n实验结果表明，我们的方法可以显着提高 U-Net 分割效果不佳的结节的分割性能。\n总结：牛逼牛逼牛逼！！！ 确实牛逼！ 不愧是一区顶刊\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-uncertainty-aware-attentionmechanism/","summary":"Abstract 放射科医生拥有不同的培训和临床经验，导致肺结节的分割注释存在差异，从而导致分割的不确定性。传统方法通常选择单个注释作为学习目标或尝试学习包含多个注释的潜在空间。\n然而，这些方法无法利用多个注释之间的共识和分歧所固有的有价值的信息。在本文中，我们提出了一种不确定性感知注意机制（UAAM），它利用多个注释之间的共识和分歧来促进更好的分割。为此，我们引入了多置信度掩模（MCM），它结合了低置信度（LC）掩模和高置信度（HC）掩模。 LC 掩模表示分割置信度较低的区域，放射科医生可能有不同的分割选择。继UAAM之后，我们进一步设计了一个不确定性引导多置信分割网络（UGMCS-Net），它包含三个模块：一个捕获肺结节一般特征的特征提取模块，一个为肺结节产生三个特征的不确定性感知模块。注释的并集、交集和注释集，以及一个交集并集约束模块，该模块使用三个特征之间的距离来平衡最终分割和 MCM 的预测。为了全面展示我们方法的性能，我们提出了 LIDC-IDRI 上的复杂结节验证，它测试了 UGMCS-Net 对使用常规方法难以分割的肺结节的分割性能。实验结果表明，我们的方法可以显着提高传统方法难以分割的结节的分割性能。\nINTRODUCTION 肺结节分割在肺癌计算机辅助诊断 (CAD) 系统中至关重要 [1]，可提供结节大小、形状和其他重要医学特征等关键信息。然而，对于深度学习方法的一般训练和测试范例，每个结节图像数据只有一个由一名放射科医生描绘的注释掩模[2]-[6]。因此，网络每次只能提供结节区域的单个预测。\n然而，在临床实践中，不同的放射科医生由于其不同的培训和临床经验可能会为肺结节提供不同的分割注释[7]-[9]。\n因此，基于单一注释的传统方法无法反映临床经验的多样性，限制了深度学习方法的应用。\n解决放射科医生之间注释不同问题的一个直接解决方案是为每个肺结节图像合并多个注释。这导致了另一个问题：多个注释不可避免地会带来不确定性和冲突，因为放射科医生可能会对同一区域进行不同的注释。为了克服这个问题，Kohl 等人在 2018 年提出了一种概率 U-Net，它利用条件变分自动编码器将多个分割变体编码到低维潜在空间中 [8]、[10]。通过从该空间采样，网络可以影响相应的分割图。基于这项研究，Hu等人提出将真实不确定性与概率UNet相结合，这可以提高预测不确定性估计、样本准确性和样本多样性[7]。这些方法依赖于潜在空间和该空间中的随机样本。因此，这些方法只能通过多次预测来提供不确定区域。\n在本文中，我们提出了一个论点，即多个注释之间的不确定性遵循特定的模式。\n为了演示这种现象，我们引入了多重置信掩码 (MCM)，它结合了高置信度 (HC) 掩码和低置信度 (LC) 掩码，如图 1 所示。 A. 交叉掩码等于 HC mask，代表所有注释的交集。\n联合掩码是所有注释的联合。 LC掩模是交集掩模和并集掩模之间的差异。当在 LIDC-IDRI 数据集 [11] 上计算 HC 和 LC 的 Hounsfield 单位 (HU) 核估计时，如图 1.B 所示，我们可以观察到 LC 和 HC 掩模之间的 HU 分布存在明显区别。具体地，LC区域具有比HC区域更低的HU值。从像素分布来看，HU值越低，对应区域的密度越低。就CT图像特征而言，LC区域主要由结节边缘、毛刺和磨玻璃特征等边界相关特征组成，而HC区域主要分布在结节核心内。因此，我们提出了这样的假设：导致放射科医生之间差异的区域主要与低密度组织和边界相关特征有关。\n与其他方法不同，我们建议利用 MCM (多重置信掩码) ** 和注释集作为具有不同分割确定性的特征的学习指导**，有助于更好的分割性能。我们将这种训练称为UncertaintyAware Attention Mechanism，如图2所示。按照这种机制，我们进一步设计了用于肺结节分割的Uncertainty-Guide Multi-Confidence Segmentation Network（UGMCS-Net）。\nUGMCS-Net 包含三个模块：基于 U-Net 的特征提取模块、不确定性感知模块和交集并集约束模块。","title":"Uncertainty-Aware Attention Mechanism:利用不确定性感知注意机制进行肺结节分割和不确定区域预测"},{"content":"程序设计作业接口文档 统一返回格式\n{ code: ...,\t# 状态码 msg: ...,\t# 描述信息 data: { # 数据 ... } } code = { 200 == 成功, 500 == 失败, } msg = { success == 成功 fail == 失败 ... } data = { key : value } 前端 虚拟换衣功能 @请求格式 （请求后端） # 前后端需统一样例图片id { userId: ...\u0026lt;int\u0026gt;,\t# 标识哪个用户的请求 isUploadCloth: ...\u0026lt;bool\u0026gt;, # 若上传衣服图片使用base64，否则用id isUploadPerson: ...\u0026lt;bool\u0026gt;, # 若上传人物图片使用base64，否则用id clothData: ...\u0026lt;base64||null\u0026gt;, # 衣服图片base64编码 personData: ...\u0026lt;base64||null\u0026gt;,\t# 人物图片base64编码 exampleClothId: ...\u0026lt;int\u0026gt;,\t# 衣服样例图片id examplePersonId: ...\u0026lt;int\u0026gt;\t# 任务样例图片id } 动漫头像功能 @请求格式 （请求后端） { userId: ...\u0026lt;int\u0026gt;, # 标识哪个用户的请求 imgData: ...\u0026lt;base64\u0026gt; # 需要动漫化的图片 } 后端 虚拟换衣功能 前端请求API: https://talented-civet-separately.ngrok-free.app/tryon/ @返回格式 (返回前端) { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { tryon_result : ...\u0026lt;url\u0026gt;, # 处理后的图片url } } 动漫头像功能 前端请求API: https://talented-civet-separately.ngrok-free.app/anime/ @返回格式 (返回前端) { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg : ...\u0026lt;string\u0026gt;, # 消息 (success / fail) data: { anime_result : ...\u0026lt;url\u0026gt;, # 处理后的图片url } } 模型端 虚拟换衣功能 后端请求API: https://certain-ideally-foal.ngrok-free.app/tryon/predict/ @请求格式\t(后端发出请求) { userid : ...\u0026lt;int\u0026gt;, # 用户id cloth : ...\u0026lt;url\u0026gt;, # 衣服图片url链接 person : ...\u0026lt;url\u0026gt;\t# 人物图片url链接 } @返回格式\t（返回后端） { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { image_value : ...\u0026lt;base64\u0026gt;, # 处理后的图片base64编码 } } 动漫头像功能 后端请求API: https://certain-ideally-foal.ngrok-free.app/anime/predict/ @请求格式 { userid : ...\u0026lt;int\u0026gt;, # 用户id origin_image : ...\u0026lt;url\u0026gt;, # 原图片url链接 } @返回格式 { code: ...\u0026lt;int\u0026gt;, # 状态码 (200表示成功, 500表示失败) msg: ...\u0026lt;string\u0026gt;,\t# 消息 (success / fail) data: { image_value : ...\u0026lt;base64\u0026gt;, # 处理后的图片base64编码 } } ","permalink":"https://swimmingliu.cn/posts/diary/2023-%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%BD%9C%E4%B8%9A%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3/","summary":"程序设计作业接口文档 统一返回格式\n{ code: ...,\t# 状态码 msg: ...,\t# 描述信息 data: { # 数据 ... } } code = { 200 == 成功, 500 == 失败, } msg = { success == 成功 fail == 失败 ... } data = { key : value } 前端 虚拟换衣功能 @请求格式 （请求后端） # 前后端需统一样例图片id { userId: ...\u0026lt;int\u0026gt;,\t# 标识哪个用户的请求 isUploadCloth: ...\u0026lt;bool\u0026gt;, # 若上传衣服图片使用base64，否则用id isUploadPerson: ...\u0026lt;bool\u0026gt;, # 若上传人物图片使用base64，否则用id clothData: ...\u0026lt;base64||null\u0026gt;, # 衣服图片base64编码 personData: .","title":"程序设计作业接口文档"},{"content":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。\n2）级联网络在医学分割中的应用：级联网络已广泛应用于正常组织和病变的分割以及不同类型病变的分割，包括肝脏病变、脑肿瘤、硬化病变和前列腺癌的分割[4] ，[15][5]，[16]。例如，Awad 等人[17]提出了一个名为 CU-Net 的级联框架，用于在 CT 扫描中对肝脏和病变进行自动分割。他们还提供了可以指导临床治疗的有用信息和解释。 Xi等人[18]提出了一种级联U-ResNets，它遵循一种新颖的垂直级联策略，并在他们的工作中评估了不同类型的损失函数。除了肝脏病灶分割之外，级联策略在 BraTS 挑战中也很流行。例如，BraTS 2019挑战赛的Top-2解决方案[6]、[7]都是具有不同级联策略的级联网络。\n3）神经网络中的注意力：注意力机制受到人类感知和视觉认知的启发，并已普遍应用于计算机视觉任务中[19]，[20][11]，[21]。计算机视觉任务中的注意力机制是在神经网络提取的特征表示上生成空间或通道权重图。例如，Woo等人[20]开发了一个卷积块注意力模块（CBAM）来引入一种融合注意力机制，其中包括通道注意力和空间注意力。这种注意力模块可以插入到常用的分类或分割网络中。Oktay等人[11]在Attention U-Net中提出了一种新颖的注意力门，用于细化网络编码器提取的特征表示，以促进网络专注于ROI。最近，首先在自然语言处理任务中提出的变压器[22]已被引入到医学分割任务中。例如，Wang 等人 [23] 提出了一种 TransBTS，用于从多模态脑 MRI 中执行脑肿瘤分割。\n综上所述，注意力机制已被广泛用于突出ROI并抑制不相关信息，但目前基于注意力的方法的研究并没有对注意力如何产生以及网络为何关注某些区域提供清晰的解释，这使得限制了注意力机制的可解释性。\nMethod 在本节中，我们将详细介绍所提出的先验注意网络架构。在第一部分中，我们将概述所提议的网络。然后，我们相应地提供有关具有中间监督、参数化跳跃连接和具有深度监督的多类解码器的所提出的注意引导解码器的详细信息。\n3.1 . Overview of Network Architecture 基本上，我们提出的网络是基于 U-Net [3] 架构进行修改的，该架构具有 U 形拓扑以及编码器和解码器之间的跳跃连接。在提出的先验注意网络中，一种新颖的注意引导解码器模块被集成到网络的跳跃连接中，以通过空间注意来细化特征表示。网络中还引入了一种新颖的参数化跳跃连接，以指导网络学习普通特征图和精炼特征图之间的比率。注意力引导解码器从编码器获取丰富的语义特征并生成空间注意力图来指导接下来的多类分割。为了产生与投资回报率相关的注意力，框架中使用了中间监督策略。然后将细化的特征图发送到多类解码器以进行最终的密集预测。\n多类解码器采用深度监督策略以获得更好的收敛性并提高分割性能。这种网络拓扑通过注意力引导解码器生成的注意力图在单个网络中实现了传统级联网络的两个步骤。组网方案如图2所示。\n3.2 Attention Guiding Decoder 在典型的级联网络中，分割的第一步是执行粗分割并找到输入图像中的 ROI。在提出的先验注意网络中，我们提出了一个注意引导解码器来执行该过程。所提出的注意力引导解码器被集成到网络中以生成与 ROI 相关的注意力图，然后利用这些图来细化特征表示并提高多类分割性能。\n1）模块拓扑：所提出的注意力引导解码器的基本拓扑基于FCN [2]中提出的特征融合。从网络解码器最深三层提取的特征表示被馈送到该模块。由于特征图的空间大小不同，因此首先执行线性插值以对特征图进行上采样。然后对特征进行压缩，抑制通道维度中的不相关信息，降低计算成本。然后将压缩后的特征分别在通道维度上连接起来以进行特征融合。(torch.cat) 最后，融合三个特征图以获得最终的预测。\n为了简单起见，我们使用 2D 分割来说明注意力图的计算。我们使用 Xi ∈ R Ci × Hi × Wi,i ∈ (3, 4, 5) 表示从网络编码器提取的特征图，其中 X5 表示最深的特征。特征压缩和融合计算如下：\n其中Z5 ∈ R C4 × H4 × W4表示X5和X4的融合特征，Z4 ∈ R C3 × H3 × W3表示X4和X3的融合特征，Wc5 ∈RC5×C4和Wc4 ∈RC4×C3表示相应的压缩卷积， W4 ∈ RC4×C4 表示融合 X5 和 X4 的融合卷积，⊕ 表示特征串联。\n注意力引导解码器的输出计算如下：\n其中 W3 ∈ R C3×C3 表示融合 X4 和 X3 的融合卷积，Wout ∈ R C3 × 1 表示输出卷积，σ 分别表示 Sigmoid 激活。\n2）中间监督：计算机视觉中的传统注意力机制自动生成注意力图，但注意力生成的过程通常是人类无法解释的，并且网络关注的区域可能与人类关注的区域不同。\n这种差距会限制注意力机制的性能和可解释性，有时还会导致网络容量的恶化。为了解决这些问题，我们在网络中引入了中间监督策略。在遵循从粗到细的方式的多病灶分割任务中，我们首先生成一个二元Ground Truth，其中前景表示所有类型的病灶，背景表示其他一切。在具有 C 种病变的多病变分割任务中，我们使用 Gi,i ∈ (1,\u0026hellip;,C) 表示第 i 类病变的二元基本事实，其中前景表示特定病变背景代表其他一切。二进制真实值 Gb 计算如下：\n然后利用二元损失函数来计算二元真实值 yb 和注意力引导解码器生成的注意力图 Y 之间的二元损失 l：\n其中 Lb 表示二元损失函数。然后利用计算出的损失 l 来监督注意力引导解码器的参数更新。\n这个地方的中间监督主要是 要让中间的注意力机制起作用，不能随便生成。\n通过引入中间监督，生成的注意力由输入图像的二元真实值进行监督。这样，网络被迫学习多病灶分割任务的分解，即首先提取病灶区域，然后对病灶区域进行细粒度分类。这种分解降低了多病灶分割的难度，并且与当前在“黑匣子”中生成注意力的基于注意力的医学分割方法相比，具有更好的可解释性。\n3.3 Parameterized Skip Connections 跳跃连接已广泛应用于流行的卷积网络中，包括U-Net [3]、ResNet [24]等。受[11]的启发，我们建议将注意力图集成到连接网络编码器和多网络的跳跃连接中，形成多级解码器。在跳跃连接中，我们还引入了额外的残差路径来恢复普通特征图并进一步提高分割性能。与传统的残差路径相比，残差路径的幅值因子αi,i ∈ (1, 2,…, 5)被设置为网络的可学习参数，并在反向传播过程中更新。我们相信这样的设置可以为网络增加额外的非线性能力并增强跳过连接的有效性。\n我们使用 Fi,i ∈ (1, 2,\u0026hellip;, 5) 表示来自网络编码器的普通特征图，Y 表示注意力图，精炼后的特征图 Fri,i ∈ (1, 2,. .., 5) 多类解码器接收的信息计算如下：\n然后将细化的特征图发送到多类解码器以进行最终的多类预测。\n3.4 Multi-Class Decoder With Deep Supervision U形分割网络中的解码器用于接收编码器发送的特征图，随着解码器中的特征通道数量的减少和空间分辨率的增加，分割性能逐步细化。然而，随着网络变深，最深的解码器块变得难以训练，这可能会限制最终的分割性能。深度监督策略已经被提出来训练深度卷积网络[25]、[26]。在所提出的先验注意网络中，辅助预测是从不同级别的解码器块中提取的，并使用相同的基本事实进行监督。我们使用 Pi,i ∈ (1, 2, 3) 表示来自多类解码器的辅助预测，Pm 表示最终的多类预测，g 表示真实值，Lm 表示多类损失函数。最终的多类损失计算如下：\n所提出的多类解码器的解码器块也与当前网络设置具有共同的设置，即卷积层、归一化层和非线性激活单元的堆栈。\n总结一下：\n① 注意力机制 + 中间监督：最后三层特征融合 + 这一部分做深度监督（原来这样也叫创新）\n② 跳跃连接的部分加了一个α因子 （感觉像权重一样的东西 ）\n③ 多阶段的深度监督 （这个就算一个trick吧，大家都在用）， 不过这里变成了多类别\nExperiments 4.1 Strong Baselines and Evaluation Metrics 为了研究网络架构的性能差异，我们将所提出的先验注意网络与医学分割中最流行的方法进行了比较，包括 U-Net [3]、Attention U-Net [11] 和级联 U-Net，在两个 2D 中和 3D 分割任务。值得注意的是，与他们论文中提出的原始版本相比，基线方法根据网络拓扑方面的某些任务进行了修改和优化，以获得性能提升。我们将残差连接[24]、批量归一化[30]和来自 ImageNet 的预训练编码器引入到 2D COVIDlesion 分割任务的基线方法中，并且我们还将残差连接、实例归一化和 PReLU 激活引入到 3D 脑肿瘤分割任务中。除了网络拓扑之外，基线方法与所提出的先验注意网络共享相同的数据增强和训练配置。\n对于 COVID-19 病变的 2D 分割，我们使用 Dice 指数、精度分数和召回分数来评估所提出的网络的性能。 Dice指数是一种用来衡量两个样本相似度的统计量，已广泛用于分割算法的评估。精确率衡量的是实际正确的阳性识别的比例，召回率衡量的是算法对阳性样本的敏感度。对于 BraTS 2020 挑战赛的 3D 分割，在在线门户上进行评估，并根据 Dice 指数和 95% Hausdorff 距离（HD）对算法进行排名。\n我们使用 G 表示ground truth，P 表示密集预测，TP 表示正确预测的正样本，FP 表示错误预测的正样本，TN 表示正确预测的副样本，FN 表示错误预测的副样本。这些指标的计算方式如下：\n这个HD（Hausdorff ）也是一种评估分割结果的方式 （alright 又多了一种指标）\n4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割 1）数据：由于可用的开源COVID-19 CT分割数据集通常很小，因此利用两个独立的公开可用数据集，即COVID-19 CT分割数据集[27]和CC-CCII数据集[28]来验证所提出的方法二维分割任务中的方法。第一个数据集包含来自 40 多名患者的 100 个轴向 CT 切片，这些切片已重新缩放至 512 × 512 像素并进行灰度化。所有切片均由放射科医生用不同的标签进行分割，以识别不同类型的肺部感染。第二个数据集由 150 名 COVID-19 患者的 750 张 CT 切片组成，这些切片被手动分割为背景、肺野、毛玻璃混浊和实变。由于并非所有 750 个切片都包含病变，我们最终使用了 150 名患者的 549 个带注释的切片。对于这两个数据集，利用 5 倍交叉验证来评估所提出模型的性能。\n折叠之间的数据根据患者进行分割，以避免潜在的数据泄漏。最终的标签和分割图包含 3 个类别，包括背景、毛玻璃不透明度 (GGO) 和合并 (CON.)。\n2）实现细节：a）模型设置和损失函数：对于预训练网络编码器，我们采用来自ImageNet的预训练ResNeXt-50（32 × 4d）[31]作为基线方法和所提出的先验注意网络的编码器。对于解码器中的上采样，采用双线性插值，比例因子为2。对于中间监督和级联U-Net第一阶段的二元损失函数，我们采用Dice Loss [13]和Focal的线性组合损失[32]作为损失函数。对于最终输出的多类损失函数，我们采用Focal Tversky Loss [33]作为损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 16.04 服务器上使用 PyTorch 1.7.1 框架实现的。我们使用 NVIDIA RTX 2080 Ti GPU 来加速我们的训练过程。在我们的训练过程中使用Albumentations [34] 进行数据增强，以减少过度拟合并提高泛化能力。首先，将所有输入图像重新缩放为 560 × 560，然后进行随机亮度和对比度偏移以及随机仿射变换。然后将图像随机裁剪为 512 × 512，然后进行随机弹性变换，最后输入网络。该模型由 Adam 优化器优化，β1 = 0.9、β2 = 0.999、γ = 1e − 8。L2 正则化也用于减少过度拟合。我们将模型权重衰减设置为 1e − 5。初始学习率设置为 1e −4 并降低，然后采用余弦退火策略。批量大小设置为 4，模型训练 40 轮。该模型使用 5 倍交叉验证进行评估。\n3）定量结果：我们在两个数据集上的实验中不同模型的详细比较分别如表一和表二所示。如图所示，我们提出的网络在毛玻璃不透明度和固结的 Dice 分数方面优于 U-Net、Attention U-Net。所提出的 PANet 以更少的参数和计算成本实现了与级联 U-Net 竞争的结果。由于这些模型在模型主干和训练策略上是相同的，很明显，所提出的注意力引导解码器、中间监督和深度监督的组合对分割性能有很大贡献。注意力引导解码器的利用有助于模型更准确地检测感染组织并生成与感染相关的注意力图，从而有利于解码器中的多类分割。\n此外，中间监督和深度监督的引入促进了网络的收敛，这也有助于提高性能。\n好好好，这哥们儿，睁着眼睛说瞎话是吧（这Unet明明比你低啊，精度也没差多少啊）\n4）定性结果：不同模型在 2D COVID-19 切片上的视觉比较如图 4 所示。由于模型在 Dice 分数方面非常接近，因此乍一看这些模型的表现相似。但与 U-Net 和 Attention U-Net 相比，所提出的 PANet 在实变和微小病变的分割上表现更好。\n与 U-Net 和 Attention U-Net 相比，PANet 产生更准确的分割掩模，并且与 Cascaded U-Net 相比，所提出的网络以更少的计算成本实现了有竞争力的结果。\n额 只要定量结果上去了，好像定性结果都是挑好的说吧？\n) 消融实验：进行了几次消融实验来评估我们模型中组件的性能，如表 III 所示。 a）具有深度监督的多类解码器的有效性：为了探索深度监督策略的贡献，我们建立了两个实验：No.1（U-Net）和No.2（U-Net + DS）。表三的结果表明，深度监督在一定程度上对绩效有所贡献。\nb）注意力引导解码器的有效性：我们通过构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中所提出的注意力引导解码器的有效性。如表III所示，与实验1相比，注意力引导解码器的引入提供了显着的性能提升。这表明注意力引导解码器在所提出的网络中提供了有效的注意力图，从而指导解码器中的多类分割。\nc）参数化跳跃连接的有效性：为了探索所提出的参数化跳跃连接的有效性，我们建立了两个实验4（U-Net + AGD*）和5（U-Net + AGD）。引入参数化跳跃连接后，分割性能得到了提高，几乎没有额外的参数或计算成本。\nd）中间监督的有效性：为了研究所提出的 PANet 中中间监督策略的有效性，我们比较了第 3 号（U-Net + AGD w/o IS）和第 5 号（U-Net + AGD）。如表 III 所示，与第 3 种相比，具有中间监督的网络获得了额外的改进。此外，在比较第 6 种（U-Net + DS + AGD w/o IS）和第 7 种（PANet）时也可以观察到改进。 ）尽管改进相对较小。可以看出，深度监管的引入也对绩效产生了提升，因此中级监管对绩效的提升并不像以前那么显着。\n4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs 1）数据：我们使用来自 BraTS 2020 挑战赛的开源多模态 MRI 数据集 [29]、[36] [37]。训练集由 369 个多对比 MRI 扫描组成，其中每个扫描包含四种模式，即原生 T1 加权、对比后 T1 加权 (T1Gd)、T2 加权 (T2) 和 T2 流体衰减反转恢复 (FLAIR) ）。\n每次扫描都有相应的 4 类标签：背景（标签 0）、GD 增强肿瘤（ET，标签 4）、瘤周水肿（ED，标签 2）以及坏死和非增强肿瘤核心（NET/ NCR，标签 1)。验证集由 125 个多重对比 MRI 扫描组成，其模式与训练集和隐藏的基本事实相同。所有 MRI 扫描均去除颅骨，与相同的大脑模板 (SRI24) 对齐，并插值至 1mm3 分辨率。验证阶段通过在线门户进行，算法根据 3 个重叠肿瘤区域的性能进行排名，即增强肿瘤 (ET)、肿瘤核心 (ET + NET/NCR) 和整个肿瘤 (ET) + NET/NCR + ED）\n2）实现细节：a）模型设置和损失函数：对于3D分割，由于缺乏开源预训练编码器，所有模型都是从头开始训练的。下采样通过跨步 3 × 3 × 3 填充卷积执行，上采样通过三线性插值实现。为了进一步提高在BraTS数据集上的性能，我们采用基于区域的训练策略（直接在重叠区域而不是独立标签上优化）并增强肿瘤抑制（如果增强肿瘤的预测体积为，则用坏死替换预测的增强肿瘤）小于某个阈值）在训练过程中。对于损失函数，我们采用 Dice Loss [13] 和 Cross Entropy Loss 的线性组合作为网络中二分类和多分类阶段的损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 服务器上使用 PyTorch 1.7.1 框架实现的。由于3D分割的训练，尤其是级联U-Net对显存的要求较高，因此我们使用NVIDIA RTX 2080 Ti GPU和NVIDIA RTX 3090 GPU分别训练单个模型和级联模型。\n由于训练过程的显存占用大于11Gb，我们采用PyTorch框架提供的原生混合精度训练程序来节省显存使用并加速训练过程。人工智能医学开放网络（MONAI）项目[38]和TorchIO[39]分别用于训练和推理阶段的数据加载过程。数据增强是在训练过程中通过 MONAI 项目进行的。\n首先，分别使用 z 分数标准化对所有模态进行标准化。然后通过随机翻转、随机强度偏移、随机强度缩放和弹性变换来增强图像。最后，我们将图像块随机裁剪为 128 × 128 × 128 并将其输入网络。对于推理，我们还采用基于补丁的推理管道来生成 BraTS 2020 验证集的预测。面片大小设置为 128 × 128 × 128，面片之间的重叠设置为 75%。重叠区域中的预测是重叠块的平均值。这种重叠配置可以被视为自集成并产生更好的分割性能。对于模型评估，我们进行了 2 个单独的评估程序来比较分割模型的性能。首先，我们对 BraTS 2020 训练集进行 5 倍交叉验证，以比较离散区域（增强肿瘤、瘤周水肿和非增强肿瘤核心）上的分割性能。然后，我们对 BraTS 2020 验证集进行评估，以比较重叠区域（增强肿瘤、肿瘤核心和整个肿瘤）的分割性能，其中我们可以将所提出的方法与最先进的方法进行比较BraTS 挑战。\n3）定量结果：BraTS 2020 训练集的交叉验证性能已在表 IV 中报告。\n所提出的 PANet 在 Dice 分数和 Hausdorff 距离方面优于所有其他网络。此外，我们还在 BraTS 2020 验证集上对训练后的模型进行了验证。通过这种方式，我们还将所提出的网络与模态配对学习（BraTS 2020 中的 Top-2 解决方案）[35] 和研究中的 Transformer TransBTS [23] 进行了比较，除了基于 U 的常用网络之外-网。\n性能列于表五中。我们提出的 PANet 在 BraTS 2020 验证集上优于 U-Net、Attention U-Net、级联 U-Net 和 TransBTS。此外，所提出的 PANet 在 BraTS 2020 上实现了与 Top2 解决方案类似的 Dice 分数，并且具有更好的 Hausdorff 距离。另外，值得注意的是，与 U-Net 相比，所提出的 PANet 仅增加了 3.9% 的额外 GFlops，并且以更少的计算成本实现了比级联 U-Net 更好的性能。这些结果表明，所提出的先验注意网络在 BraTS 2020 数据集上的分割性能和计算效率之间取得了复杂的平衡。\n4）定性结果：我们可视化具有不同分割难度的 3 幅 MRI 图像，以展示不同模型的性能。对于最简单的情况（BraTS20_Validation_077），所有模型都能够分割病变，而所提出的 PANet 获得最高的分割 Dice 分数。对于有一定难度的病例（BraTS20_Validation_028），Attention U-Net和级联U-Net在水肿分割方面都产生了严重的误报，导致整个肿瘤的Dice评分较低。对于最难的情况（BraTS20_Validation_076），由于增强肿瘤的假阳性分割，U-Net、Attention U-Net 和级联 U-Net 的性能并不乐观，而所提出的 PANet 产生了最好的分割性能。所提出的 PANet 的成功归功于具有中间监督的注意力引导解码器，特征图通过空间注意力图进行细化，这可以防止潜在的误报预测。\n关于医学影像中的轴位面（横断面）、冠状面、矢状面的解释\n1.冠状面 （Coronal），又称额状面。即从左右方向，沿人体的长轴将人体纵切为前、后两部分的切面。这种提法只是为了在临床中将器官位置描述的更具体，英文名称是：Coronal section；\n2.矢状面 (Sagittal)就是把人体分成左右两面的解剖面，于这个面平行的也是矢状面。出于这个位置的叫矢状位。矢状位的英文名称是：Median sagittal section；\n3.水平位 (Axial)又称横断位，即左右、前后构成的面为水平位，英文名称是:Transverse section。\n5）消融分析：在 BraTS 2020 验证数据集上也进行了与 2D 分割类似的消融实验，以评估我们模型中呈现的组件的有效性，如表六所示。\na）具有深度监督的多类解码器的有效性：我们通过向网络解码器引入深度监督来构建实验2（U-Net + DS）。与基线（No.1）相比，实验No.2在增强肿瘤、肿瘤核心和整个肿瘤的分割性能方面提供了一定程度的性能提升。此外，当引入注意力引导解码器时，可以观察到性能的提高（第3和第6）。\nb) 注意力引导解码器的有效性：我们构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中注意力引导解码器的有效性。与基线相比，注意力引导解码器在增强肿瘤和肿瘤核心的分割性能方面产生了显着的改善。这表明注意力引导解码器对从编码器提取的特征图产生有效的注意力，从而更容易区分病变。\nc）参数化跳跃连接的有效性：我们建立了两个实验No.4（U-Net + AGD*）和No.5（UNet + AGD）来探索所提出的参数化跳跃连接的有效性。引入参数化跳跃连接后，在增强肿瘤和肿瘤核心方面分割性能有所提高，但整个肿瘤的分割性能略有下降。\nd）中间监督的有效性：我们通过比较表六中的第3号和第5号来调查中间监督策略的有效性。在No.5（U-Net + AGD）中，通过引入中间监督，增强肿瘤和肿瘤核心的Dice得分显着提高。但是当引入深度监督时，中间监督的性能提升并不像以前那么显着（第6和第7），这与2D分割情况类似。\nDISCUSSION AND CONCLUSION 多病灶分割在临床场景中具有重要意义，因为某种疾病可能同时发生多种类型的感染，不同感染阶段的患者可能会出现不同类型的病灶。例如，磨玻璃样混浊（GGO）和实变（CON.）是COVID-19患者典型的肺部病变，前者通常发生在早期患者，而后者的增加可能表明病情恶化。胶质瘤可分为低级别胶质瘤（LGG）和胶质母细胞瘤，即高级别胶质瘤（GBM/HGG），并且在发生HGG的患者中更有可能发现强化肿瘤。因此，多病灶分割在患者的筛查和预后方面具有巨大的潜力。\n多病灶分割问题可以分解为粗分割和细分割，粗分割是对病灶进行粗略分割，而细分割则基于前一分割，以产生最终的分割图。级联网络广泛用于多病变分割任务，因为这些算法背后的逻辑非常自然。然而，级联网络在潜在的临床部署中受到限制，因为它们缺乏灵活性并且对计算资源的要求很高。与现有的级联网络相比，我们开发了一种先验注意网络，它将粗分割和细分割集成到一个网络中。我们提出的网络架构的优点是分割性能和计算效率的平衡。通过将分割的两个步骤结合在一个网络中，所提出的先验注意网络能够实现多病灶分割的端到端训练，在训练和推理方面都具有更大的灵活性，并且在临床部署中具有更大的潜力。此外，我们设法保持所提出的先验注意网络的性能，在分割性能和运行效率之间实现复杂的平衡。\n与级联 U-Net 相比，我们提出的先验注意力网络在 2D 和 3D 任务中都实现了更好的性能和效率，如图 6 所示。\n总之，我们提出了一种新颖的分割网络，即先验注意网络，用于医学图像中的多病灶分割。受流行的从粗到细策略的启发，我们通过空间注意机制将级联网络的两个步骤聚合成一个网络。此外，我们引入了一种新颖的中间监督机制来指导与病变相关的注意图的生成，这可以指导解码器中的后续多类分割。所提出的网络在 2D 和 3D 医学图像（包括 CT 扫描和多模态 MRI）上进行评估。对于 2D 分割，与级联 U-Net 相比，所提出的先验注意力网络以更少的计算成本获得了有竞争力的结果。对于 3D 脑肿瘤分割，所提出的先验注意网络在 BraTS 2020 验证数据集上产生了最先进的性能，并且优于基于 U-Net 的其他基线方法。实验结果表明，该方法在医学影像中的许多多病灶分割任务中具有巨大的应用潜力，与二元分割相比可以提供更多信息，并有助于医生未来的临床诊断。\n","permalink":"https://swimmingliu.cn/posts/papernotes/2022-priorattentionnetwork/","summary":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。","title":"Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络"},{"content":"地大服务器使用教程 1. 服务器环境介绍 NVIDIA RTX 3090 (24GB) NVIDIA RTX 2080 Ti (11GB) 2. 配置实验环境 2.1 Conda环境安装 每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\nConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\n大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\n2.2 Conda环境配置 （以YOLOv8为例） # 创建conda环境 名为yolov8_lyj python版本为3.9 conda create -n yolov8_lyj python=3.9 # 激活环境 conda activate yolov8_lyj # 选择合适的路径，克隆github项目代码 git clone https://github.com/ultralytics/ultralytics # 进入到项目路径下 cd ultralytics/ # 安装相关依赖包 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 2.3 准备数据集 下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\n按照算法指定的数据集格式，对数据集格式进行调整。\n​\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\n​\thttps://zhuanlan.zhihu.com/p/461488682\n2.4 开始实验 在算法中指定数据集的存放路径 （相对/绝对路径均可）\n初始化算法的参数\nbatch-size 批处理大小：每一次处理图片的个数，根据显卡内存进行调整\repochs\t迭代次数：算法总共需要训练的轮次\rworkers 载入数据进程数：每一次调用多少个进程来载入数据\rdevice 选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡 开始训练 # 运行训练代码 python mian.py (注：使用向日葵的同学，可以直接在Pycharm当中运行)\n3. 注意事项 1. 查看显卡使用情况 两种办法：\n# 第一种 使用nivida驱动直接查看\rnvidia-smi\r# 第二种 使用第三方库 gpustat动态查看\r# 先安装第三方库\rpip install gpustat -i https://pypi.tuna.tsinghua.edu.cn/simple\r# 每两秒刷新一次 动态查看显存使用情况\rwatch -n2 gpustat 2. Magic Network 使用向日葵的同学，可以使用Magic Network进行github仓库克隆、google网盘数据集下载等\n使用方法：\nexport http_proxy=http://127.0.0.1:7890\rexport https_proxy=http://127.0.0.1:7890 预祝大家科研顺利，硕果累累，offer拿到手软！！！\n博客地址： SwimmingLiu.cn\n","permalink":"https://swimmingliu.cn/posts/diary/2023-%E5%9C%B0%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"地大服务器使用教程 1. 服务器环境介绍 NVIDIA RTX 3090 (24GB) NVIDIA RTX 2080 Ti (11GB) 2. 配置实验环境 2.1 Conda环境安装 每位同学都会分配个人用户，大家在自己的用户上使用Conda进行环境配置。\nConda安装教程：https://blog.csdn.net/JineD/article/details/129507719\n大家按照教程步骤安装即可, 由于安装时间较长, 视频中暂不进行演示。\n2.2 Conda环境配置 （以YOLOv8为例） # 创建conda环境 名为yolov8_lyj python版本为3.9 conda create -n yolov8_lyj python=3.9 # 激活环境 conda activate yolov8_lyj # 选择合适的路径，克隆github项目代码 git clone https://github.com/ultralytics/ultralytics # 进入到项目路径下 cd ultralytics/ # 安装相关依赖包 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 2.3 准备数据集 下载需要训练的数据集 （最好找顶刊/顶会论文中的公开数据集）\n按照算法指定的数据集格式，对数据集格式进行调整。\n​\t目标检测中数据集格式之间的相互转换：（VOC、COCO、YOLO格式）\n​\thttps://zhuanlan.zhihu.com/p/461488682\n2.4 开始实验 在算法中指定数据集的存放路径 （相对/绝对路径均可）\n初始化算法的参数\nbatch-size 批处理大小：每一次处理图片的个数，根据显卡内存进行调整\repochs\t迭代次数：算法总共需要训练的轮次\rworkers 载入数据进程数：每一次调用多少个进程来载入数据\rdevice 选择显卡设备： \u0026#39;0\u0026#39;使用3090，\u0026#39;1\u0026#39;使用2080ti，\u0026#39;0,1\u0026#39;使用两张卡 开始训练 # 运行训练代码 python mian.","title":"地大服务器使用教程"},{"content":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023) 1. Abstract 由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。 作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。 ACC-UNet结合了卷积神经网络（ConvNets）的内在归纳偏差和Transformer的设计决策 卷积神经网络（ConvNets）的内在归纳偏差：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。 Transformer的设计决策：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。 ACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\n2.Introduction 语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\n学习一下这里的背景描述\n原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。\n从 UNet出发，然后逐步介绍他的变体（UNet++等）。随后介绍Transformer和UNet的各种结合体，为后续对比实验做铺垫。最后结合一张发展图，简明扼要描述UNet的创新历程。\n最近，鉴于 Transformer 带来的进步，研究开始重新发现 CNN 的潜力。这方面的开创性工作是“A ConvNet for the 2020s”[16]，它探讨了 Transformer 引入的各种想法及其在卷积网络中的适用性。通过逐渐融合训练协议和微观-宏观设计选择的思想，这项工作使 ResNet 模型的性能优于 Swin Transformer 模型。\n在本文中，我们在 UNet 模型的背景下提出了同样的问题。我们研究仅基于卷积的 UNet 模型是否可以与基于 Transformer 的 UNet 竞争。在此过程中，我们从 Transformer 架构中获得动力并开发了纯卷积 UNet 模型。我们提出了一种基于补丁的上下文聚合，与基于窗口的自注意力相反。此外，我们通过融合来自多个级别编码器的特征图来创新跳跃连接。对 5 个基准数据集的广泛实验表明，我们提出的修改有可能改进 UNet 模型。\n通过介绍CONvNet的重要性，引入本文重点 （纯卷积模块的UNet）。\n3. Method 3.1 A high-level view of transformers in UNet Transformers 显然在两个不同方面改进了 UNet 模型:\n1.利用自注意力的远程依赖性: Transformer 可以通过使用（窗口式）自注意力，从更大的上下文视图中计算特征。此外，他们还通过采用反向瓶颈（即增加 MLP 层中的神经元）来提高表达能力。此外，它们包含快捷连接，这有助于学习。 2.通过通道注意力的自适应多级特征组合: 基于 Transformer 的 UNet 使用通道注意力自适应地融合来自多个编码器级别的特征图。与受当前级别信息限制的简单跳跃连接相比，由于来自不同级别的各种感兴趣区域的组合，这会生成丰富的特征。\n主要就两个点：\n自主注意力在UNet的编码和解码阶段都能够快速、准确地根据上下文信息提取特征。\n自注意力机制可以用来连接encoder阶段不同stage的特征图。\n猜想：把新出的自注意力机制加在ACC-Unet是不是也会有提升呢？\n3.2 Hierarchical Aggregation of Neighborhood Context (HANC) 我们首先探索引入远程依赖性并提高卷积块的表达能力的可能性。我们仅使用逐点和深度卷积来降低计算复杂度。为了增加表达能力，我们建议在卷积块中包含反向瓶颈[16]，这可以通过使用逐点卷积将通道数从 cin 增加到 cinv = cin * inv_f ctr 来实现。由于这些额外的通道会增加模型复杂度，因此我们使用3×3深度卷积来补偿。输入特征图 xin ∈ R cin,n,m 被转换为 x1 ∈ R cinv,n,m （图 2b）\n接下来，我们希望在卷积块中模拟自注意力，其核心是将一个像素与其邻域中的其他像素进行比较[15]。通过将像素值与其邻域的平均值和最大值进行比较可以简化这种比较。因此，我们可以通过附加相邻像素特征的平均值和最大值来提供邻域比较的近似概念。因此，连续逐点卷积可以考虑这些并捕获对比视图。\n由于分层分析对图像有益[23]，我们不是在单个大窗口中计算这种聚合，而是在多个级别中分层计算，例如 2 × 2, 2 ^2 × 2^ 2 , · · · , 2^(k− 1) × 2^(k−1) 个补丁。当 k = 1 时，这将是普通的卷积运算，但是当我们增加 k 的值时，将提供更多的上下文信息，从而绕过对更大卷积核的需要。因此，我们提出的分层邻域上下文聚合通过上下文信息丰富了特征图 x1 ∈ R cinv,n,m 作为 x2 ∈ R cinv*(2k−1),n,m （图 2b），其中 ||对应于沿通道维度的串联。\n下面与Transfomer类似，我们在卷积块中包含一个快捷连接，以实现更好的梯度传播。因此，我们执行另一个逐点卷积以减少 cin 的通道数并与输入特征图相加。因此，x2 ∈ R cinv*(2k−1),n,m 变为 x3 ∈ R cin,n,m （图 2b）\n最后，我们使用逐点卷积将滤波器的数量更改为cout作为输出\n\u0026ldquo;inverted bottlenecks\u0026rdquo;（反向瓶颈） ：将Bottleneck放在卷积块的开始，而不是结束。它先使用1x1卷积来减少通道数，然后才是较大的卷积层。\n3.3 Multi Level Feature Compilation (MLFC) 接下来，我们研究多级特征组合的可行性，这是使用基于 Transformer 的 UNet 的另一个优点。\n基于 Transformer 的跳跃连接已经证明了所有编码器级别的有效特征融合以及各个解码器从编译的特征图中进行的适当过滤[24,22,27]。这是通过连接不同级别的投影令牌来执行的[22]。按照这种方法，我们调整从不同编码器级别获得的卷积特征图的大小，以使它们均衡并连接它们。这为我们提供了跨不同语义级别的特征图的概述。我们应用逐点卷积运算来总结这种表示并与相应的编码器特征图合并。整体信息和个体信息的融合通过另一个卷积传递，我们假设它用来自其他级别特征的信息丰富了当前级别特征。\n对于来自 4 个不同级别的特征 x1、x2、x3、x4，特征图可以通过多级信息来丰富（图 2d）\n这里， resizei(xj ) 是将 xj 的大小调整为 xi 的大小且 ctot = c1 + c2 + c3 + c4 的操作。此操作针对所有不同级别单独完成。因此，我们提出了另一个名为多级特征编译（MLFC）的新颖块，它聚合来自多个编码器级别的信息并丰富各个编码器特征图。该块如图 2d 所示。\n总结： 把各stage的特征联合起来进行逐点卷积，然后各stage得到新的特征信息\n3.4 ACC-UNet 因此，我们提出全卷积ACC-UNet（图2a）。我们从普通的 UNet 模型开始，并将滤波器的数量减少了一半。然后，我们用我们提出的 HANC 块替换了编码器和解码器中的卷积块。我们考虑 inv_f ctr = 3，而不是第 3 级的最后一个解码器块 (inv_f ctr = 34)，以模拟 Swin Transformer 第 3 阶段的扩展。 k = 3，最多考虑 4 × 4 个补丁，被选择用于除瓶颈级别 (k = 1) 及其旁边的级别 (k = 2) 之外的所有级别。接下来，我们通过使用残差块（图 2c）来修改跳跃连接以减少语义间隙 [11] 并堆叠 3 个 MLFC 块。所有卷积层均经过批量归一化 [12]，由 Leaky-RELU [17] 激活，并通过挤压和激励 [10] 重新校准。\n总而言之，在 UNet 模型中，我们用我们提出的 HANC 块替换了经典的卷积块，该 HANC 块执行近似版本的自注意力，并修改了与 MLFC 块的跳跃连接，MLFC 块考虑来自不同编码器级别的特征图。所提出的模型有 16.77 M 个参数，比普通 UNet 模型大约增加了 2M。\n4.Experiments 4.1 Datasets 为了评估 ACC-UNet，我们在 5 个跨不同任务和模式的公共数据集上进行了实验。我们使用 ISIC-2018 [6,21]（皮肤镜检查，2594 张图像）、BUSI [3]（乳腺超声，使用类似于 [13] 的 437 张良性图像和 210 张恶性图像）、CVC-ClinicDB [4]（结肠镜检查，612 张图像） ）、COVID [1]（肺炎病灶分割，100 张图像）和 GlaS [20]（腺体分割，85 张训练图像和 80 张测试图像）。所有图像和掩模的大小都调整为224×224。对于GlaS数据集，我们将原始测试分割作为测试数据，对于其他数据集，我们随机选择20％的图像作为测试数据。\n剩余的 60% 和 20% 图像用于训练和验证，并使用不同的随机改组重复实验 3 次。\nISIC-2018、BUSI、 CVC-ClinicDB、COVID、GlaS 五个数据集\n4.2 Comparison Experiments 4.3 Ablation Study 4.4 Qualitative Results ","permalink":"https://swimmingliu.cn/posts/papernotes/2023-acc-unet/","summary":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023) 1. Abstract 由于ViT （Vision Transformer）的引入，UNet和Transformer融合已成为大趋势。最近，又有很多研究人员开始重新思考卷积模型，比如将ConvNext嵌入到ResNet，能够达到Swin Transformer的水平。受此启发，作者提出了一个纯粹的卷积UNET模型 （ACC-UNet），并且超越基于Transfomer的模型(如Swin-UNET或UCTransNet)。 作者研究了基于Transfomer的UNET模型优点：长范围依赖关系和跨级别跳过连接。 ACC-UNet结合了卷积神经网络（ConvNets）的内在归纳偏差和Transformer的设计决策 卷积神经网络（ConvNets）的内在归纳偏差：卷积神经网络具有天生的归纳偏差，这意味着它们在处理图像等数据时具有一些固有的假设和特点。例如，卷积神经网络擅长处理局部特征、平移不变性等，这些特点使它们在图像处理任务中表现出色。 Transformer的设计决策：Transformer是一种不同的神经网络架构，它采用了一些独特的设计决策，例如自注意力机制和位置编码等。这些设计决策使得Transformer在处理长距离依赖性、全局关系等方面表现出色，适合处理序列数据和具有远程依赖的任务。 ACC-UNet 在 5 个不同的医学图像分割基准上进行了评估，并且始终优于卷积网络、Transfomer及其混合网络。\n2.Introduction 语义分割是计算机辅助医学图像分析的重要组成部分，可识别并突出显示各种诊断任务中感兴趣的区域。然而，由于涉及图像模态和采集以及病理和生物变化的各种因素，这通常变得复杂[18]。深度学习在这一领域的应用无疑在这方面受益匪浅。最值得注意的是，自推出以来，UNet 模型 [19] 在医学图像分割方面表现出了惊人的功效。结果，UNet 及其衍生品已成为事实上的标准[25]。\n学习一下这里的背景描述\n原始的 UNet 模型包含对称的编码器-解码器架构（图 1a）并采用跳跃连接，这为解码器提供了在编码器的池化操作期间可能丢失的空间信息。尽管通过简单串联的信息传播提高了性能，但编码器-解码器特征图之间可能存在语义差距。这导致了第二类 UNet 的发展（图 1b）。 U-Net++ [26] 利用密集连接，而 MultiResUNet [11] 在跳过连接上添加了额外的卷积块作为潜在的补救措施。到目前为止，UNet 的历史上所有创新都是使用 CNN 进行的。然而，2020 年的十年给计算机视觉领域带来了根本性的变化。 CNN 在视觉领域的长期主导地位被视觉转换器打破了 [7]。 Swin Transformers [15] 进一步针对一般视觉应用调整了变压器。因此，UNet 模型开始采用 Transformer [5]。 Swin-Unet [9] 用 Swin Transformer 块取代了卷积块，从而开创了一类新的模型（图 1c）。尽管如此，CNN 在图像分割方面仍然具有各种优点，导致了融合这两者的发展[2]。这种混合类 UNet 模型（图 1d）在编码器-解码器中采用卷积块，并沿跳跃连接使用变换器层。 UCTransNet [22]和MCTrans[24]是此类的两个代表性模型。最后，还尝试开发全变压器 UNet 架构（图 1e），例如，SMESwin Unet [27] 在编码器-解码器块和跳跃连接中都使用变压器。","title":"ACC-UNet: A Completely Convolutional UNet model for the 2020s (MICCAI2023)"},{"content":"(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络 Abstract 准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用逐元素加法或串联在解码器中逐步融合不同级别的特征。然而，这两种操作都容易产生大量冗余信息，从而削弱不同级别特征之间的互补性，导致病灶定位不准确和边缘模糊。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本减法单元（SU）来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器提供像素级和结构级差异信息。\n然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\n没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\n两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\nIntroduction 作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\n先说医学分割在医学领域重要\u0026hellip;(balabala) 然后当前领域存在xxx挑战\u0026hellip;(balabala)\n这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\n一般来说，编码器中不同级别的特征有不同的特征。高级别具有更多的语义信息，有助于定位对象，而低级别具有更详细的信息，可以捕捉对象的微妙边界。解码器利用特定级别和跨级别特征来生成最终的高分辨率预测。然而，上述方法直接使用逐元素加法或串联来融合来自编码器的任意两级特征并将它们传输到解码器。这些简单的操作并没有更多地关注不同层次之间的差异信息。这一缺点不仅会产生冗余信息来稀释真正有用的特征，还会削弱特定于级别的特征的特性，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于感受野有限，单尺度卷积核很难捕获大小变化物体的上下文信息。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的语义上下文和纹理细节。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取层内多尺度信息。然而，类似ASPP的多尺度卷积模块会产生许多额外的参数和计算。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\n在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于每对相邻的级别特征。 SU突出了特征之间有用的差异信息，并消除了冗余部分的干扰。其次，我们借助所提出的多尺度减法模块收集极端多尺度信息。\n对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们聚合特定于级别的特征和多路径跨级别差分特征，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\n多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\n（也就是说可以用这种办法替换注意力机制）\nRELATED WORK Medical Image Segmentation Network 根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了长连接和短连接，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，注意力门嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不同形状和大小的目标结构。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\n医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。\n我们可以看到，医学通用方法通常针对通用挑战（即丰富的特征表示、多尺度信息提取和跨级别特征聚合）。并且，医学特异性方法根据当前器官或病变的特征提出有针对性的解决方案，例如设计一系列注意力机制、边缘增强模块、不确定性估计等。然而，通用医学模型和医学特异性模型都依赖于通过大量的加法或串联操作来实现特征融合，削弱了互补特征之间的特殊性部分。我们提出的多尺度减法模块自然专注于提取差异信息，从而为解码器提供有效的目标特征。\n主要是说大部分特征融合都是用加法/乘法/串联实现的，但是减法可以削弱互补特征之间的特殊性部分。所以多尺度减法模块提取差异信息，然后再用加法进行特征融合。\nMulti-scale Feature Extraction 尺度线索在捕捉对象的上下文信息中发挥着重要作用。受到被广泛验证为有效且理论上合理的框架的尺度空间理论的启发，越来越多的多尺度方法被提出。与单尺度特征相比，多尺度特征有利于解决自然发生的尺度变化。这一特性可以帮助医学分割模型感知不同尺度的病变。根据形式，当前基于多尺度的方法可以大致分为两类，即层间多尺度结构和层内多尺度结构。前者基于特征编码器提取的不同尺度的特征，并在解码器中逐步聚合它们，例如U形[1]、[2]、[4]、[9]-[11]、[37] ，[38]架构。后者通常配备多尺度可插拔模块，如ASPP [16]、DenseASPP [17]、FoldASPP [6]和PAFEM [12]，构建具有不同扩张率的并行多分支卷积层，以获得丰富的组合感受野。与它们不同的是，我们通过同时引入层间和层内多尺度，提出了具有极端多尺度信息的多尺度减法模块中的多尺度。并且，层内多尺度减法单元专注于挖掘从像素到像素到区域到区域的特征对的自差分性质。与单尺度操作相比，整个过程非常高效，不需要额外的参数。\n多尺度减法模块可以超越其他卷积类办法的多尺度特征信息提取办法\nLoss Method 图像分割中的大多数损失函数都是基于交叉熵或重合度量。传统的交叉熵损失对类别信息一视同仁。 Long等人[24]提出了每个类别的加权交叉熵损失（WCE），以抵消数据中的类别不平衡。 Lin等人[39]引入了困难样本和简单样本的权重来提出焦点损失。 Dice loss[40]被提出作为V-Net中重合测量的损失函数，可以有效抑制类别不平衡带来的问题。 Tversky 损失[41]是 Dice 损失的正则化版本，用于控制准确率和召回率对损失函数的贡献。 Wong等人[42]通过Dice损失和WCE损失的加权求和提出指数对数损失（EL Loss）来提高小结构物体的分割精度。\nTaghanaki等人[43]发现单独使用基于重叠的损失函数存在风险，并提出comomoloss将Dice损失作为正则化项与WCE损失相结合来处理输入输出不平衡的问题。\n虽然这些各种各样的损失函数在不同层次上有不同的效果，但手动设计这些复杂的函数确实费时费力。为此，我们提出了自动且全面的分割损失结构，称为LossNet。\nLossNet权重就0.1 （ 感觉这个是为了创新而创新）\nMETHOD Encoder: Res2Net + Connection: MMSB + Decoder: Plus\nMulti-scale in Multi-scale Subtraction Module 我们使用 FA 和 FB 来表示相邻级别的特征图。\n它们都已被 ReLU 操作激活。我们定义一个基本减法单位（SU）：\n其中是逐元素减法运算，然后计算绝对值，Conv(·) 表示卷积层。直接对元素位置特征进行单尺度减法只是为了建立孤立像素级别上的差异关系，没有考虑病灶可能具有区域聚类的特征。与带有单尺度减法单元的MSNet MICCAI版本[27]相比，我们设计了一个强大的层内多尺度减法单元（MSU），并将MSNet改进为M2SNet。如图3所示，我们利用大小为1×1、3×3和5×5的固定全一权重的多尺度卷积滤波器根据像素-像素和区域区域模式计算细节和结构差异值。使用具有固定参数的多尺度滤波器不仅可以直接捕获匹配空间位置处的初始特征对之间的多尺度差异线索，而且可以在不引入额外参数负担的情况下实现高效训练。因此，M2SNet可以保持与MSNet相同的低计算量，并获得更高精度的性能。整个多尺度减法过程可以表述为：\n其中 Filter(·) n×n 表示大小为 n × n 的完整滤波器（卷积）。 MSU可以捕获FA和FB的互补信息，并突出它们从纹理到结构的差异，从而为解码器提供更丰富的信息。\n为了获得跨多个特征级别的高阶互补信息，我们水平和垂直连接多个MSU来计算一系列具有不同阶数和感受野的差分特征。多尺度减法模块中多尺度的细节可以在图2中找到。我们聚合了相应级别和任意级别之间的特定尺度特征（MSi 1 ）和跨尺度差分特征（MSi n6=1）。其他级别生成互补增强特征（CEi）。这个过程可以表述如下：\n最后，所有CEi参与解码，然后对息肉区域进行分割。\n这里就是介绍一下MSU\nLossNet 在所提出的模型中，总训练损失可以写为：\n其中L w IoU和L w BCE表示加权IoU损失和二元交叉熵（BCE）损失，它们已在分割任务中广泛采用。我们使用与[4]、[44]、[45]中相同的定义，它们的有效性已在这些工作中得到验证。与它们不同的是，我们额外使用LossNet来进一步优化从细节到结构的分割。\n具体来说，我们使用 ImageNet 预训练分类网络，例如 VGG-16，分别提取预测和地面实况的多尺度特征。然后，它们的特征差异计算为损失 Lf ：\n令 F i P 和 F i G 分别表示从预测和地面实况中提取的第 i 层特征图。 l i f 计算为其欧几里德距离（L2-Loss），该距离在像素级别进行监督：\n从图4中可以看出，低层特征图包含丰富的边界信息，高层特征图描述位置信息。因此，LossNet可以在特征层面产生全面的监督。\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-m2snet/","summary":"(2023) M2SNet: 新颖多尺度模块 + 智能损失函数 = 通用图像分割SOTA网络 Abstract 准确的医学图像分割对于早期医学诊断至关重要。大多数现有方法基于U形结构，并使用逐元素加法或串联在解码器中逐步融合不同级别的特征。然而，这两种操作都容易产生大量冗余信息，从而削弱不同级别特征之间的互补性，导致病灶定位不准确和边缘模糊。为了应对这一挑战，我们提出了一种通用的多尺度减法网络（M2SNet）来完成医学图像的多样化分割。具体来说，我们首先设计一个基本减法单元（SU）来产生编码器中相邻级别之间的差异特征。接下来，我们将单尺度 SU 扩展到层内多尺度 SU，它可以为解码器提供像素级和结构级差异信息。\n然后，我们金字塔式地为不同层次的多尺度SU配备不同的感受野，从而实现层间多尺度特征聚合并获得丰富的多尺度差异信息。此外，我们构建了一个免训练网络“LossNet”来全面监督从底层到顶层的任务感知特征，这驱动我们的多尺度减法网络同时捕获细节和结构线索。\n没有花里胡哨的东西，我们的方法在不同的评估指标下，在不同图像模态的四种不同医学图像分割任务的 11 个数据集上表现优于大多数最先进的方法，包括彩色结肠镜成像、超声成像、计算机断层扫描 (CT) ）和光学相干断层扫描（OCT）。\n两个主要创新点：多尺度金字塔减法单元 （确实牛逼）+ LossNet（为了创新而创新的损失函数）\nIntroduction 作为计算机辅助诊断系统中的重要作用，精确的医学图像分割技术可以为医生做出临床决策提供重要指导。精确分割存在三个普遍的挑战：首先，U形结构[1]、[2]由于其利用多级信息重建高分辨率特征图的能力而受到了相当多的关注。在UNet [2]中，上采样的特征图与从编码器跳过的特征图连接在一起，并在上采样步骤之间添加卷积和非线性，如图1（a）所示。后续基于UNet的方法通过注意力机制[3]、[4]、门机制[5]、[6]、变压器技术[7]、[8]设计不同的特征增强模块，如图1（b）所示。 UNet++[9]使用嵌套和密集的跳跃连接来减少编码器和解码器的特征图之间的语义差距，如图1（c）所示。\n先说医学分割在医学领域重要\u0026hellip;(balabala) 然后当前领域存在xxx挑战\u0026hellip;(balabala)\n这里是以医学图像分割挑战的视角，介绍UNet发展的情况。然后在描述不同UNet变体发展过程中解决的不同问题（感觉可以借鉴）\n一般来说，编码器中不同级别的特征有不同的特征。高级别具有更多的语义信息，有助于定位对象，而低级别具有更详细的信息，可以捕捉对象的微妙边界。解码器利用特定级别和跨级别特征来生成最终的高分辨率预测。然而，上述方法直接使用逐元素加法或串联来融合来自编码器的任意两级特征并将它们传输到解码器。这些简单的操作并没有更多地关注不同层次之间的差异信息。这一缺点不仅会产生冗余信息来稀释真正有用的特征，还会削弱特定于级别的特征的特性，从而导致网络无法平衡精确定位和微妙的边界细化。其次，由于感受野有限，单尺度卷积核很难捕获大小变化物体的上下文信息。一些方法[1]、[2]、[9]-[11]依赖于层间多尺度特征，并逐步整合来自不同尺度表示的语义上下文和纹理细节。其他人[6]、[12]-[15]专注于基于网络中的空洞空间金字塔池化模块[16]（ASPP）或DenseASPP [17]提取层内多尺度信息。然而，类似ASPP的多尺度卷积模块会产生许多额外的参数和计算。许多方法[5]、[18]-[21]通常将多个ASPP模块安装到不同级别的编码器/解码器块中，而有些方法[13]、[14]、[22]、[23]将其安装在不同级别的编码器/解码器块中。最高级别的编码器块。第三，损失函数的形式直接为网络的梯度优化提供了方向。在分割领域，提出了许多损失函数来监督不同级别的预测，例如像素级别的L1损失、交叉熵损失和加权交叉熵损失[24]，SSIM[25]损失区域层面的不确定性损失[26]，全局层面的IoU损失、Dice损失和一致性增强损失[11]。尽管这些基本损失函数及其变体具有不同的优化特性，但复杂的手动数学形式的设计对于许多研究来说确实非常耗时。为了获得综合性能，模型通常会集成多种损失函数，这对研究人员的训练技能提出了很高的要求。因此，我们认为有必要引入一种无需复杂人工设计的智能损失函数来全面监督分割预测。\n在本文中，我们提出了一种用于一般医学图像分割的新型多尺度减法网络（M2SNet）。首先，我们设计一个减法单元（SU）并将其应用于每对相邻的级别特征。 SU突出了特征之间有用的差异信息，并消除了冗余部分的干扰。其次，我们借助所提出的多尺度减法模块收集极端多尺度信息。\n对于层间多尺度信息，我们以金字塔方式连接多个减法单元来捕获大跨度的跨层信息。然后，我们聚合特定于级别的特征和多路径跨级别差分特征，然后在解码器中生成最终预测。对于层内多尺度信息，我们通过一组不同内核大小的full one滤波器将单尺度减法单元改进为多尺度减法单元，可以自然地实现多尺度减法聚合，而无需引入额外的参数。如图1所示，MSNet配备了层间多尺度减法模块，M2SNet同时具有层间和层内多尺度减法结构。第三，我们提出了一个LossNet来自动监督从底层到顶层提取的特征图，它可以通过简单的L2损失函数优化从细节到结构的分割。\n多尺度减法单元可以去特征之间的差异信息，消除冗余干扰。\n（也就是说可以用这种办法替换注意力机制）\nRELATED WORK Medical Image Segmentation Network 根据不同器官或病变的特点，我们将现有的医学图像分割方法分为两类：医学通用的和医学专用的。随着U-Net[2]在医学图像分割领域取得稳定的性能，带有编码器-解码器的U形结构已成为基本的分割基线。 U-Net++[9]集成了长连接和短连接，可以减少编码器和解码器子网络的特征图之间的语义差距。对于注意力 U-Net [28]，注意力门嵌入在编码器和解码器块之间的每个过渡层中，它可以自动学习关注不同形状和大小的目标结构。最近，Transformer [29]架构在许多自然语言处理任务中取得了成功。一些作品[7]、[8]探讨了其对医学视觉任务的有效性。 UTNet [7] 是一种简单但功能强大的混合变压器架构，它在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖关系。另一个具有代表性的基于 Transformer 的模型是 TransUNet [8]，它通过将图像特征视为序列来编码强全局上下文，并通过 U 形混合架构设计利用低级 CNN 特征。\n医学特定方法。在息肉分割任务中，SFA [30]和PraNet [4]专注于恢复息肉与其周围粘膜之间的清晰边界。前者提出了共享编码器和两个相互约束的解码器下的选择性特征聚合结构和边界敏感损失函数。后者利用反向注意模块来建立区域和边界线索之间的关系。此外，Ji等人[31]利用时空信息构建视频息肉分割模型。在COVID-19肺部感染任务中，Paluru等人[32]提出了一种基于变形深度嵌入的轻量级CNN来分割COVID-19胸部CT图像中的异常。 Inf-Net [33] 构建隐式反向注意力和显式边缘注意力来对边界进行建模。 BCS-Net [34]具有三个渐进边界上下文语义重建块，可以帮助解码器捕获肺部感染的零散区域。在乳腺分割任务中，Byra等人[35]通过注意力机制开发了选择性核来调整U-Net的感受野，可以进一步提高乳腺肿瘤的分割精度。 Chen 等人 [36] 提出了一种嵌套 U 网，通过利用不同的深度和共享权重来实现乳腺肿瘤的稳健表示。","title":"M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation"},{"content":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation 1. Abstract 目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\n作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\n创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\n1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\n2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\n2. Introduction 背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\n相同方式可引入脑瘤、肺癌。\n为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\nAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\n先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。\n提出问题：医疗环境中计算资源的限制，复杂模块难以落地 \u0026mdash;\u0026gt; 解决办法：轻量化模型\n当前轻量化发展历程 \u0026mdash;\u0026gt; 轻量化的模型分割效果不好 \u0026mdash;\u0026gt; EGE-Unet 轻量+分割能力强\n具体来说，EGE-UNet 利用两个关键模块：群组多轴 Hadamard 产品注意力模块（GHPA）和群组聚合桥模块（GAB）。\n一方面，由于多头自注意力机制（MHSA），最近基于 ViT [7] 的模型已经显示出前景。 MHSA将输入划分为多个head，并在每个head中计算self-attention，这使得模型能够从不同的角度获取信息，整合不同的知识，提高性能。尽管如此，MHSA 的二次复杂度极大地增加了模型的大小。因此，我们提出了具有线性复杂度的哈达玛产品注意力机制（HPA）。HPA 采用可学习的权重，并使用输入执行哈达玛乘积运算以获得输出。随后，受到 MHSA 中多头模式的启发，我们提出了 GHPA，它将输入分为不同的组，并在每个组中执行 HPA。然而，值得注意的是，我们在不同组的不同轴上进行HPA，这有助于进一步从不同的角度获取信息。\n另一方面，对于GAB，由于医学图像中分割目标的大小和形状不一致，因此获得多尺度信息至关重要[19]。因此，GAB基于组聚合融合不同大小的高层和低层特征，并额外引入掩模信息来辅助特征融合。通过将上述两个模块与UNet相结合，我们提出了EGE-UNet，它以极低的参数和计算量实现了出色的分割性能。与以前仅注重提高性能的方法不同，我们的模型还优先考虑现实环境中的可用性。图 1 显示了 EGEUNet 与其他网络的清晰比较。\n具体介绍为什么引入两个创新模块（GHPA、GAB）、以及模块是基于什么论文。（模块背景+创新方法）\n(1)提出了GHPA和GAB，前者有效地获取和集成多视角信息，后者接受不同尺度的特征，以及用于高效多尺度特征融合的辅助掩模。\n(2)我们提出了EGEUNet，这是一种专为皮肤病变分割而设计的极其轻量级的模型。\n(3) 我们进行了广泛的实验，证明了我们的方法在以显着降低的资源需求实现最先进性能方面的有效性。\n主要贡献：（1）写模块作用 （2）写整体网络优势 （3）实验效果\n3. Method 3.1EGE-Unet网络结构 EGE-UNet由对称编码器-解码器部分组成的 U 形架构之上。\n编码器由六级组成，每级通道数为{8,16,24,32,48,64}。解码器同理\n前三个阶段采用内核大小为 3 的普通卷积，后三个阶段利用提出的 GHPA 从不同的角度提取表示信息。\n与 UNet 中的简单跳跃连接相比，EGE-UNet 在编码器和解码器之间的每个阶段都采用了 GAB。\n利用深度监督生成不同规模的掩模预测，这些预测用于损失函数并作为 GAB 的输入之一。\n通过集成这些高级模块，EGE-UNet 显着减少了参数和计算负载，同时与之前的方法相比增强了分割性能。\n3.2 GHPA (Group multi-axis Hadamard Product Attention module) 为了克服 MHSA 带来的二次复杂度问题，我们提出了具有线性复杂度的 HPA。给定输入 x 和随机初始化的可学习张量 p，首先使用双线性插值来调整 p 的大小以匹配 x 的大小。然后，我们在 p 上采用深度可分离卷积（DW）[10][20]，然后在 x 和 p 之间进行哈达玛乘积运算以获得输出。然而，仅利用简单的HPA不足以从多个角度提取信息，导致结果不理想。受 MHSA 中多头模式的启发，我们引入了基于 HPA 的 GHPA，如算法 1 所示。我们将输入沿通道维度平均分为四组，并在高度-宽度、通道-高度和通道上执行 HPA - 分别为前三组的宽度轴。对于最后一组，我们只在特征图上使用DW。最后，我们沿着通道维度连接四组，并应用另一个数据仓库来整合不同角度的信息。请注意，DW 中使用的所有内核大小均为 3。\n首先对输入的特征分为四组进行处理：高度-宽度、通道-高度、通道-宽度、深度可分离卷积\n然后连接4组特征，进行可分离卷积融合特征。\n具体过程：\n第一步，按通道数将输入张量分为四组。（x1, x2, x3, x4）\n设置初始化三个全一张量，分别为高度-宽度、通道-高度、通道-宽度（Pxy, Pzx, Pzy）。\n第二步，将 x1, x2, x3 的对应切片分别使用双线插值法（bilinear）在Pxy, Pzx, Pzy中进行插值。\n第三步，对插值后的Pxy, Pzx, Pzy，进行深度可分离卷积，然后分别和x1, x2, x3进行哈达玛乘积\n第四步，连接4组特征信息，然后经过深度可分离卷积融合特征。\n3.3 GAB (Group Aggregation Bridge module) 多尺度信息的获取被认为对于密集预测任务（例如医学图像分割）至关重要。因此，如图 3 所示，我们引入了 GAB，它接受三个输入：低级特征、高级特征和掩码。首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征的大小，以匹配低层特征的大小。其次，我们沿通道维度将两个特征映射分为四组，并将一组低级特征与一组高级特征连接起来，以获得四组融合特征。对于每组融合特征，掩码被连接起来。接下来，将内核大小为3和不同扩张率{1,2,5,7}的扩张卷积[25]应用于不同的组，以提取不同尺度的信息。最后，将四组沿通道维度连接起来，然后应用内核大小为 1 的普通卷积，以实现不同尺度的特征之间的交互。\nGAB模块作用： 将高级特征、低级特征、低级特征的预测掩码进行特征融合，作为新的输入特征进行解码。\n具体过程： 高级特征、低级特征、低级特征的预测掩码 (xh、xl 、Mask)\n首先，采用深度可分离卷积（DW）和双线性插值来调整高层特征 (xh) 的大小，以匹配低层特征 (xl) 的大小。\n其次，沿通道维度将两个特征映射分为四组。（对应不同空洞卷积的扩张率：d1 = 1, d2 = 2, d3 = 5, d4 = 7）\n并将每一组的低级特、高级特征和掩码连接起来，总共四组融合特征。\n最后，将四组特征进行连接，并进行1x1卷积得到输出。\n3.4 Loss Function 在本研究中，由于不同的GAB需要不同尺度的掩模信息，因此采用深度监督来计算不同阶段的损失函数，以生成更准确的掩模信息。我们的损失函数可以表示为方程（1）和（2）。其中 Bce 和 Dice 表示二元交叉熵和dice损失。 λi是不同阶段的权重。在本文中，我们默认将i=0到i=5之间的λi设置为1、0.5、0.4、0.3、0.2、0.1。\n分为6个阶段，逐一计算每个阶段的损失。然后按照权重对损失进行求和。\n4.Experiments 4.1 Datasets and Implementation details 为了评估我们模型的有效性，我们选择了两个公共皮肤病变分割数据集，即 ISIC2017 [1][3] 和 ISIC2018 [2][6]，分别包含 2150 个和 2694 个皮肤镜图像。与之前的研究[19]一致，我们以 7:3 的比例将数据集随机划分为训练集和测试集。\nEGE-UNet是由Pytorch[17]框架开发的。所有实验均在单个 NVIDIA RTX A6000 GPU 上执行。图像被归一化并调整大小为 256×256。我们应用各种数据增强，包括水平翻转、垂直翻转和随机旋转。 AdamW [13] 用作优化器，以 0.001 的学习率初始化，CosineAnnealingLR [12] 用作调度器，最大迭代次数为 50，最小学习率为 1e-5。总共训练了 300 个 epoch，批量大小为 8。为了评估我们的方法，我们采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为指标，并进行 5 次训练\n​\t在公共皮肤病变分割数据集（ISIC2017 和 ISIC2018 ）进行对比实验，在ISIC2018进行消融实验\n采用并集平均交集 (mIoU)、Dice 相似度得分 (DSC) 作为评估指标\n4.2 Comparison Experiments 4.3 Ablation Experiments 4.4 Qualitative Comparisons 5. ConClusions 在本文中，我们提出了两个高级模块。我们的 GHPA 使用一种新颖的 HPA 机制将自注意力的二次复杂度简化为线性复杂度。它还利用分组来充分捕获来自不同角度的信息。我们的 GAB 融合了低级和高级特征，并引入了一个掩模来集成多尺度信息。基于这些模块，我们提出了用于皮肤病变分割任务的 EGE-UNet。实验结果证明了我们的方法在显着降低资源需求的情况下实现最先进的性能的有效性。我们希望我们的工作能够激发医学图像界对轻量级模型的进一步研究。\n作者提出的EGE-UNet实现了轻量、准确的皮肤病变分割任务\n","permalink":"https://swimmingliu.cn/posts/papernotes/2023-ege-unet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","summary":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation 1. Abstract 目前的医学图像分割模型大多是 Transformer + Unet，这些模型的大量参数和计算负载使得它们不适合移动健康应用。\n作者提出的EGE-UNet 模型轻量、高效。（与 TransFuse 相比，参数和计算成本分别降低了 494 倍和 160 倍，模型参数量只有50KB）\n创新点：组多轴哈达玛产品注意力模块（GHPA）和组聚合桥模块（GAB）。\n1.GHPA 对输入特征进行分组，并在不同轴上执行哈达玛产品注意力机制（HPA），以从不同角度提取病理信息。\n2.GAB 通过对低级特征、高级特征以及解码器在每个阶段生成的掩码进行分组，有效地融合了多尺度信息。\n2. Introduction 背景: 恶性黑色素瘤是世界上增长最快的癌症之一。据美国癌症协会估计，2020 年约有 100,350 例新发病例，超过 6,500 例死亡。因此，自动化皮肤病变分割系统势在必行，因为它可以帮助医疗专业人员快速识别病变区域并促进后续治疗过程。\n相同方式可引入脑瘤、肺癌。\n为了提高分割性能，最近的研究倾向于采用具有更大参数和计算复杂度的模块，例如结合视觉变换器（ViT）的自注意力机制[7]。例如，Swin-UNet [4]，基于Swin Transformer [11]，利用自注意力机制的特征提取能力来提高分割性能。 TransUNet [5] 开创了用于医学图像分割的 CNN 和 ViT 的串行融合。 TransFuse [26]采用双路径结构，利用 CNN 和 ViT 分别捕获局部和全局信息。UTNetV2[8]利用混合分层架构、高效的双向注意力和语义图来实现全局多尺度特征融合，结合了CNN和ViT的优点。 TransBTS [23] 将自注意力引入脑肿瘤分割任务中，并用它来聚合高级信息。\nAbstract提到当前医学分割模型大部分是Transformer + Unet，这里做出具体阐述。\n先前的工作通过引入复杂的模块来提高性能，但忽略了实际医疗环境中计算资源的限制。因此，迫切需要为移动医疗中的分割任务设计一种低参数、低计算负载的模型。最近，UNeXt [22] 结合了 UNet [18] 和 MLP [21] 开发了一种轻量级模型，该模型可以获得优异的性能，同时减少参数和计算量。此外，MALUNet [19]通过减少模型通道数并引入多个注意力模块来减小模型大小，从而比 UNeXt 具有更好的皮肤病变分割性能。然而，尽管MALUNet大大减少了参数数量和计算量，但其分割性能仍然低于一些大型模型，例如TransFuse。因此，在本研究中，我们提出了 EGE-UNet，这是一种轻量级皮肤病变分割模型，可实现最先进的效果，同时显着降低参数和计算成本。此外，据我们所知，这是第一个将参数减少到大约 50KB 的工作。","title":"EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"}]