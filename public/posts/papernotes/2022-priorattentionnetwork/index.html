<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 | SwimmingLiu&#39;s Blog</title>
<meta name="keywords" content="PANet, DS, IS">
<meta name="description" content="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取
问题导向：
①组织的大小、对比度和高类间相似性的不确定性
②多类别病灶分割
③普遍采用的级联策略对硬件要求较高
Introduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。
这一段都可以当成经典医学图像分割的背景引入
然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。
此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。
级联网络：参数量大、容易欠拟合。
在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。
然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。
利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）
中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）
这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。
一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。
Related Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net&#43;&#43; [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。">
<meta name="author" content="SwimmingLiu">
<link rel="canonical" href="https://swimmingliu.github.io/posts/papernotes/2022-priorattentionnetwork/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fce773af76300c55732470d4e395524d3dc8800c50784ec518bb2f1635a0ef35.css" integrity="sha256-/Odzr3YwDFVzJHDU45VSTT3IgAxQeE7FGLsvFjWg7zU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://swimmingliu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://swimmingliu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://swimmingliu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://swimmingliu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://swimmingliu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络" />
<meta property="og:description" content="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取
问题导向：
①组织的大小、对比度和高类间相似性的不确定性
②多类别病灶分割
③普遍采用的级联策略对硬件要求较高
Introduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。
这一段都可以当成经典医学图像分割的背景引入
然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。
此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。
级联网络：参数量大、容易欠拟合。
在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。
然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。
利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）
中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）
这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。
一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。
Related Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net&#43;&#43; [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://swimmingliu.github.io/posts/papernotes/2022-priorattentionnetwork/" /><meta property="og:image" content="https://swimmingliu.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-28T14:55:47+08:00" />
<meta property="article:modified_time" content="2023-11-28T14:55:47+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://swimmingliu.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络"/>
<meta name="twitter:description" content="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取
问题导向：
①组织的大小、对比度和高类间相似性的不确定性
②多类别病灶分割
③普遍采用的级联策略对硬件要求较高
Introduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。
这一段都可以当成经典医学图像分割的背景引入
然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。
此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。
级联网络：参数量大、容易欠拟合。
在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。
然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。
利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）
中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）
这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。
一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。
Related Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net&#43;&#43; [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "📚 Posts",
      "item": "https://swimmingliu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📝 Paper Notes",
      "item": "https://swimmingliu.github.io/posts/papernotes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络",
      "item": "https://swimmingliu.github.io/posts/papernotes/2022-priorattentionnetwork/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络",
  "name": "Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络",
  "description": "Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。",
  "keywords": [
    "PANet", "DS", "IS"
  ],
  "articleBody": "Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络 Abstract 医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从粗到精策略的卷积神经网络（CNN）已广泛应用于该领域。然而，由于组织的大小、对比度和高类间相似性的不确定性，多病灶分割仍然具有挑战性。此外，普遍采用的级联策略对硬件要求较高，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它遵循从粗到细的策略来在医学图像中执行多病灶分割。所提出的网络通过在网络中插入与病变相关的空间注意机制，在单个网络中实现了两个步骤的分割。此外，我们还提出了中间监督策略，用于生成与病变相关的注意力来获取感兴趣区域（ROI），这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：肺部 CT 切片中多发性肺部感染的 2D 分割和脑 MRI 中多发性病变的 3D 分割。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 https://github.com/hsiangyuzhao/PANet 获取\n问题导向：\n①组织的大小、对比度和高类间相似性的不确定性\n②多类别病灶分割\n③普遍采用的级联策略对硬件要求较高\nIntroduction 医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有深度编码器，可从输入图像中自动提取特征，并通过以下操作生成密集预测。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。\n这一段都可以当成经典医学图像分割的背景引入\n然而，尽管医学分割取得了这些突破，但目前的医学分割方法主要集中在病灶的二元分割上，即区分病灶（前景）和其他一切（背景）。尽管二元分割确实有助于隔离某些感兴趣区域并允许对医学图像进行精确分析，但在某些需要对病变进行多类分割的场景中，二元分割还不够。与二元分割相比，由于组织的类间相似性，这种情况要困难得多，因为不同类型的病变在纹理、大小和形状上可能相似。具有从粗到细策略的级联网络已广泛应用于此类场景，例如肝脏和病变的分割、脑肿瘤分割[4]、[5][6]、[7]。\n此类网络通常由两个独立的网络组成，其中第一个网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割。然而，尽管级联网络已广泛应用于医学图像的多病灶分割，但级联策略也有其缺点。由于级联网络由两个独立的网络组成，参数量和显存占用通常是单个网络的两倍，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此级联网络的训练过程有时比单个网络更困难，这可能导致欠拟合。\n级联网络：参数量大、容易欠拟合。\n在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行多病灶分割。所提出的网络由一个用于特征提取的编码器和两个分别生成病变区域注意力和最终预测的解码器组成。该网络与注意力机制结合在一起。为了减少参数大小和硬件占用，我们使用网络编码器的深层、语义丰富的特征来生成病变区域的空间注意力。\n然后，编码器生成的特征表示通过空间注意力进行细化，并将其发送到解码器以进行最终的多类预测。为了提高分割性能并加速收敛，我们还在网络结构中引入了中间监督和深度监督。通过这些改进，与传统的级联网络相比，所提出的网络以显着降低的参数大小和计算成本实现了有竞争力的结果。\n利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）\n中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）\n这项工作的贡献体现在三个方面。首先，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在单个网络中，遵循 2D 和 3D 医学图像中多病灶分割的从粗到细的策略。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。其次，我们提出了一种监督空间注意力机制，将病变区域的注意力与网络提取的特征相结合，将多病变分割分解为两个更容易的阶段，并且与当前基于注意力的方法相比具有更好的可解释性。第三，所提出的网络已在两个实际应用中得到验证，包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。\n一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。\nRelated Work 1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。\n2）级联网络在医学分割中的应用：级联网络已广泛应用于正常组织和病变的分割以及不同类型病变的分割，包括肝脏病变、脑肿瘤、硬化病变和前列腺癌的分割[4] ，[15][5]，[16]。例如，Awad 等人[17]提出了一个名为 CU-Net 的级联框架，用于在 CT 扫描中对肝脏和病变进行自动分割。他们还提供了可以指导临床治疗的有用信息和解释。 Xi等人[18]提出了一种级联U-ResNets，它遵循一种新颖的垂直级联策略，并在他们的工作中评估了不同类型的损失函数。除了肝脏病灶分割之外，级联策略在 BraTS 挑战中也很流行。例如，BraTS 2019挑战赛的Top-2解决方案[6]、[7]都是具有不同级联策略的级联网络。\n3）神经网络中的注意力：注意力机制受到人类感知和视觉认知的启发，并已普遍应用于计算机视觉任务中[19]，[20][11]，[21]。计算机视觉任务中的注意力机制是在神经网络提取的特征表示上生成空间或通道权重图。例如，Woo等人[20]开发了一个卷积块注意力模块（CBAM）来引入一种融合注意力机制，其中包括通道注意力和空间注意力。这种注意力模块可以插入到常用的分类或分割网络中。Oktay等人[11]在Attention U-Net中提出了一种新颖的注意力门，用于细化网络编码器提取的特征表示，以促进网络专注于ROI。最近，首先在自然语言处理任务中提出的变压器[22]已被引入到医学分割任务中。例如，Wang 等人 [23] 提出了一种 TransBTS，用于从多模态脑 MRI 中执行脑肿瘤分割。\n综上所述，注意力机制已被广泛用于突出ROI并抑制不相关信息，但目前基于注意力的方法的研究并没有对注意力如何产生以及网络为何关注某些区域提供清晰的解释，这使得限制了注意力机制的可解释性。\nMethod 在本节中，我们将详细介绍所提出的先验注意网络架构。在第一部分中，我们将概述所提议的网络。然后，我们相应地提供有关具有中间监督、参数化跳跃连接和具有深度监督的多类解码器的所提出的注意引导解码器的详细信息。\n3.1 . Overview of Network Architecture 基本上，我们提出的网络是基于 U-Net [3] 架构进行修改的，该架构具有 U 形拓扑以及编码器和解码器之间的跳跃连接。在提出的先验注意网络中，一种新颖的注意引导解码器模块被集成到网络的跳跃连接中，以通过空间注意来细化特征表示。网络中还引入了一种新颖的参数化跳跃连接，以指导网络学习普通特征图和精炼特征图之间的比率。注意力引导解码器从编码器获取丰富的语义特征并生成空间注意力图来指导接下来的多类分割。为了产生与投资回报率相关的注意力，框架中使用了中间监督策略。然后将细化的特征图发送到多类解码器以进行最终的密集预测。\n多类解码器采用深度监督策略以获得更好的收敛性并提高分割性能。这种网络拓扑通过注意力引导解码器生成的注意力图在单个网络中实现了传统级联网络的两个步骤。组网方案如图2所示。\n3.2 Attention Guiding Decoder 在典型的级联网络中，分割的第一步是执行粗分割并找到输入图像中的 ROI。在提出的先验注意网络中，我们提出了一个注意引导解码器来执行该过程。所提出的注意力引导解码器被集成到网络中以生成与 ROI 相关的注意力图，然后利用这些图来细化特征表示并提高多类分割性能。\n1）模块拓扑：所提出的注意力引导解码器的基本拓扑基于FCN [2]中提出的特征融合。从网络解码器最深三层提取的特征表示被馈送到该模块。由于特征图的空间大小不同，因此首先执行线性插值以对特征图进行上采样。然后对特征进行压缩，抑制通道维度中的不相关信息，降低计算成本。然后将压缩后的特征分别在通道维度上连接起来以进行特征融合。(torch.cat) 最后，融合三个特征图以获得最终的预测。\n为了简单起见，我们使用 2D 分割来说明注意力图的计算。我们使用 Xi ∈ R Ci × Hi × Wi,i ∈ (3, 4, 5) 表示从网络编码器提取的特征图，其中 X5 表示最深的特征。特征压缩和融合计算如下：\n其中Z5 ∈ R C4 × H4 × W4表示X5和X4的融合特征，Z4 ∈ R C3 × H3 × W3表示X4和X3的融合特征，Wc5 ∈RC5×C4和Wc4 ∈RC4×C3表示相应的压缩卷积， W4 ∈ RC4×C4 表示融合 X5 和 X4 的融合卷积，⊕ 表示特征串联。\n注意力引导解码器的输出计算如下：\n其中 W3 ∈ R C3×C3 表示融合 X4 和 X3 的融合卷积，Wout ∈ R C3 × 1 表示输出卷积，σ 分别表示 Sigmoid 激活。\n2）中间监督：计算机视觉中的传统注意力机制自动生成注意力图，但注意力生成的过程通常是人类无法解释的，并且网络关注的区域可能与人类关注的区域不同。\n这种差距会限制注意力机制的性能和可解释性，有时还会导致网络容量的恶化。为了解决这些问题，我们在网络中引入了中间监督策略。在遵循从粗到细的方式的多病灶分割任务中，我们首先生成一个二元Ground Truth，其中前景表示所有类型的病灶，背景表示其他一切。在具有 C 种病变的多病变分割任务中，我们使用 Gi,i ∈ (1,…,C) 表示第 i 类病变的二元基本事实，其中前景表示特定病变背景代表其他一切。二进制真实值 Gb 计算如下：\n然后利用二元损失函数来计算二元真实值 yb 和注意力引导解码器生成的注意力图 Y 之间的二元损失 l：\n其中 Lb 表示二元损失函数。然后利用计算出的损失 l 来监督注意力引导解码器的参数更新。\n这个地方的中间监督主要是 要让中间的注意力机制起作用，不能随便生成。\n通过引入中间监督，生成的注意力由输入图像的二元真实值进行监督。这样，网络被迫学习多病灶分割任务的分解，即首先提取病灶区域，然后对病灶区域进行细粒度分类。这种分解降低了多病灶分割的难度，并且与当前在“黑匣子”中生成注意力的基于注意力的医学分割方法相比，具有更好的可解释性。\n3.3 Parameterized Skip Connections 跳跃连接已广泛应用于流行的卷积网络中，包括U-Net [3]、ResNet [24]等。受[11]的启发，我们建议将注意力图集成到连接网络编码器和多网络的跳跃连接中，形成多级解码器。在跳跃连接中，我们还引入了额外的残差路径来恢复普通特征图并进一步提高分割性能。与传统的残差路径相比，残差路径的幅值因子αi,i ∈ (1, 2,…, 5)被设置为网络的可学习参数，并在反向传播过程中更新。我们相信这样的设置可以为网络增加额外的非线性能力并增强跳过连接的有效性。\n我们使用 Fi,i ∈ (1, 2,…, 5) 表示来自网络编码器的普通特征图，Y 表示注意力图，精炼后的特征图 Fri,i ∈ (1, 2,. .., 5) 多类解码器接收的信息计算如下：\n然后将细化的特征图发送到多类解码器以进行最终的多类预测。\n3.4 Multi-Class Decoder With Deep Supervision U形分割网络中的解码器用于接收编码器发送的特征图，随着解码器中的特征通道数量的减少和空间分辨率的增加，分割性能逐步细化。然而，随着网络变深，最深的解码器块变得难以训练，这可能会限制最终的分割性能。深度监督策略已经被提出来训练深度卷积网络[25]、[26]。在所提出的先验注意网络中，辅助预测是从不同级别的解码器块中提取的，并使用相同的基本事实进行监督。我们使用 Pi,i ∈ (1, 2, 3) 表示来自多类解码器的辅助预测，Pm 表示最终的多类预测，g 表示真实值，Lm 表示多类损失函数。最终的多类损失计算如下：\n所提出的多类解码器的解码器块也与当前网络设置具有共同的设置，即卷积层、归一化层和非线性激活单元的堆栈。\n总结一下：\n① 注意力机制 + 中间监督：最后三层特征融合 + 这一部分做深度监督（原来这样也叫创新）\n② 跳跃连接的部分加了一个α因子 （感觉像权重一样的东西 ）\n③ 多阶段的深度监督 （这个就算一个trick吧，大家都在用）， 不过这里变成了多类别\nExperiments 4.1 Strong Baselines and Evaluation Metrics 为了研究网络架构的性能差异，我们将所提出的先验注意网络与医学分割中最流行的方法进行了比较，包括 U-Net [3]、Attention U-Net [11] 和级联 U-Net，在两个 2D 中和 3D 分割任务。值得注意的是，与他们论文中提出的原始版本相比，基线方法根据网络拓扑方面的某些任务进行了修改和优化，以获得性能提升。我们将残差连接[24]、批量归一化[30]和来自 ImageNet 的预训练编码器引入到 2D COVIDlesion 分割任务的基线方法中，并且我们还将残差连接、实例归一化和 PReLU 激活引入到 3D 脑肿瘤分割任务中。除了网络拓扑之外，基线方法与所提出的先验注意网络共享相同的数据增强和训练配置。\n对于 COVID-19 病变的 2D 分割，我们使用 Dice 指数、精度分数和召回分数来评估所提出的网络的性能。 Dice指数是一种用来衡量两个样本相似度的统计量，已广泛用于分割算法的评估。精确率衡量的是实际正确的阳性识别的比例，召回率衡量的是算法对阳性样本的敏感度。对于 BraTS 2020 挑战赛的 3D 分割，在在线门户上进行评估，并根据 Dice 指数和 95% Hausdorff 距离（HD）对算法进行排名。\n我们使用 G 表示ground truth，P 表示密集预测，TP 表示正确预测的正样本，FP 表示错误预测的正样本，TN 表示正确预测的副样本，FN 表示错误预测的副样本。这些指标的计算方式如下：\n这个HD（Hausdorff ）也是一种评估分割结果的方式 （alright 又多了一种指标）\n4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割 1）数据：由于可用的开源COVID-19 CT分割数据集通常很小，因此利用两个独立的公开可用数据集，即COVID-19 CT分割数据集[27]和CC-CCII数据集[28]来验证所提出的方法二维分割任务中的方法。第一个数据集包含来自 40 多名患者的 100 个轴向 CT 切片，这些切片已重新缩放至 512 × 512 像素并进行灰度化。所有切片均由放射科医生用不同的标签进行分割，以识别不同类型的肺部感染。第二个数据集由 150 名 COVID-19 患者的 750 张 CT 切片组成，这些切片被手动分割为背景、肺野、毛玻璃混浊和实变。由于并非所有 750 个切片都包含病变，我们最终使用了 150 名患者的 549 个带注释的切片。对于这两个数据集，利用 5 倍交叉验证来评估所提出模型的性能。\n折叠之间的数据根据患者进行分割，以避免潜在的数据泄漏。最终的标签和分割图包含 3 个类别，包括背景、毛玻璃不透明度 (GGO) 和合并 (CON.)。\n2）实现细节：a）模型设置和损失函数：对于预训练网络编码器，我们采用来自ImageNet的预训练ResNeXt-50（32 × 4d）[31]作为基线方法和所提出的先验注意网络的编码器。对于解码器中的上采样，采用双线性插值，比例因子为2。对于中间监督和级联U-Net第一阶段的二元损失函数，我们采用Dice Loss [13]和Focal的线性组合损失[32]作为损失函数。对于最终输出的多类损失函数，我们采用Focal Tversky Loss [33]作为损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 16.04 服务器上使用 PyTorch 1.7.1 框架实现的。我们使用 NVIDIA RTX 2080 Ti GPU 来加速我们的训练过程。在我们的训练过程中使用Albumentations [34] 进行数据增强，以减少过度拟合并提高泛化能力。首先，将所有输入图像重新缩放为 560 × 560，然后进行随机亮度和对比度偏移以及随机仿射变换。然后将图像随机裁剪为 512 × 512，然后进行随机弹性变换，最后输入网络。该模型由 Adam 优化器优化，β1 = 0.9、β2 = 0.999、γ = 1e − 8。L2 正则化也用于减少过度拟合。我们将模型权重衰减设置为 1e − 5。初始学习率设置为 1e −4 并降低，然后采用余弦退火策略。批量大小设置为 4，模型训练 40 轮。该模型使用 5 倍交叉验证进行评估。\n3）定量结果：我们在两个数据集上的实验中不同模型的详细比较分别如表一和表二所示。如图所示，我们提出的网络在毛玻璃不透明度和固结的 Dice 分数方面优于 U-Net、Attention U-Net。所提出的 PANet 以更少的参数和计算成本实现了与级联 U-Net 竞争的结果。由于这些模型在模型主干和训练策略上是相同的，很明显，所提出的注意力引导解码器、中间监督和深度监督的组合对分割性能有很大贡献。注意力引导解码器的利用有助于模型更准确地检测感染组织并生成与感染相关的注意力图，从而有利于解码器中的多类分割。\n此外，中间监督和深度监督的引入促进了网络的收敛，这也有助于提高性能。\n好好好，这哥们儿，睁着眼睛说瞎话是吧（这Unet明明比你低啊，精度也没差多少啊）\n4）定性结果：不同模型在 2D COVID-19 切片上的视觉比较如图 4 所示。由于模型在 Dice 分数方面非常接近，因此乍一看这些模型的表现相似。但与 U-Net 和 Attention U-Net 相比，所提出的 PANet 在实变和微小病变的分割上表现更好。\n与 U-Net 和 Attention U-Net 相比，PANet 产生更准确的分割掩模，并且与 Cascaded U-Net 相比，所提出的网络以更少的计算成本实现了有竞争力的结果。\n额 只要定量结果上去了，好像定性结果都是挑好的说吧？\n) 消融实验：进行了几次消融实验来评估我们模型中组件的性能，如表 III 所示。 a）具有深度监督的多类解码器的有效性：为了探索深度监督策略的贡献，我们建立了两个实验：No.1（U-Net）和No.2（U-Net + DS）。表三的结果表明，深度监督在一定程度上对绩效有所贡献。\nb）注意力引导解码器的有效性：我们通过构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中所提出的注意力引导解码器的有效性。如表III所示，与实验1相比，注意力引导解码器的引入提供了显着的性能提升。这表明注意力引导解码器在所提出的网络中提供了有效的注意力图，从而指导解码器中的多类分割。\nc）参数化跳跃连接的有效性：为了探索所提出的参数化跳跃连接的有效性，我们建立了两个实验4（U-Net + AGD*）和5（U-Net + AGD）。引入参数化跳跃连接后，分割性能得到了提高，几乎没有额外的参数或计算成本。\nd）中间监督的有效性：为了研究所提出的 PANet 中中间监督策略的有效性，我们比较了第 3 号（U-Net + AGD w/o IS）和第 5 号（U-Net + AGD）。如表 III 所示，与第 3 种相比，具有中间监督的网络获得了额外的改进。此外，在比较第 6 种（U-Net + DS + AGD w/o IS）和第 7 种（PANet）时也可以观察到改进。 ）尽管改进相对较小。可以看出，深度监管的引入也对绩效产生了提升，因此中级监管对绩效的提升并不像以前那么显着。\n4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs 1）数据：我们使用来自 BraTS 2020 挑战赛的开源多模态 MRI 数据集 [29]、[36] [37]。训练集由 369 个多对比 MRI 扫描组成，其中每个扫描包含四种模式，即原生 T1 加权、对比后 T1 加权 (T1Gd)、T2 加权 (T2) 和 T2 流体衰减反转恢复 (FLAIR) ）。\n每次扫描都有相应的 4 类标签：背景（标签 0）、GD 增强肿瘤（ET，标签 4）、瘤周水肿（ED，标签 2）以及坏死和非增强肿瘤核心（NET/ NCR，标签 1)。验证集由 125 个多重对比 MRI 扫描组成，其模式与训练集和隐藏的基本事实相同。所有 MRI 扫描均去除颅骨，与相同的大脑模板 (SRI24) 对齐，并插值至 1mm3 分辨率。验证阶段通过在线门户进行，算法根据 3 个重叠肿瘤区域的性能进行排名，即增强肿瘤 (ET)、肿瘤核心 (ET + NET/NCR) 和整个肿瘤 (ET) + NET/NCR + ED）\n2）实现细节：a）模型设置和损失函数：对于3D分割，由于缺乏开源预训练编码器，所有模型都是从头开始训练的。下采样通过跨步 3 × 3 × 3 填充卷积执行，上采样通过三线性插值实现。为了进一步提高在BraTS数据集上的性能，我们采用基于区域的训练策略（直接在重叠区域而不是独立标签上优化）并增强肿瘤抑制（如果增强肿瘤的预测体积为，则用坏死替换预测的增强肿瘤）小于某个阈值）在训练过程中。对于损失函数，我们采用 Dice Loss [13] 和 Cross Entropy Loss 的线性组合作为网络中二分类和多分类阶段的损失函数。\nb) 训练细节：我们的模型是在 Ubuntu 服务器上使用 PyTorch 1.7.1 框架实现的。由于3D分割的训练，尤其是级联U-Net对显存的要求较高，因此我们使用NVIDIA RTX 2080 Ti GPU和NVIDIA RTX 3090 GPU分别训练单个模型和级联模型。\n由于训练过程的显存占用大于11Gb，我们采用PyTorch框架提供的原生混合精度训练程序来节省显存使用并加速训练过程。人工智能医学开放网络（MONAI）项目[38]和TorchIO[39]分别用于训练和推理阶段的数据加载过程。数据增强是在训练过程中通过 MONAI 项目进行的。\n首先，分别使用 z 分数标准化对所有模态进行标准化。然后通过随机翻转、随机强度偏移、随机强度缩放和弹性变换来增强图像。最后，我们将图像块随机裁剪为 128 × 128 × 128 并将其输入网络。对于推理，我们还采用基于补丁的推理管道来生成 BraTS 2020 验证集的预测。面片大小设置为 128 × 128 × 128，面片之间的重叠设置为 75%。重叠区域中的预测是重叠块的平均值。这种重叠配置可以被视为自集成并产生更好的分割性能。对于模型评估，我们进行了 2 个单独的评估程序来比较分割模型的性能。首先，我们对 BraTS 2020 训练集进行 5 倍交叉验证，以比较离散区域（增强肿瘤、瘤周水肿和非增强肿瘤核心）上的分割性能。然后，我们对 BraTS 2020 验证集进行评估，以比较重叠区域（增强肿瘤、肿瘤核心和整个肿瘤）的分割性能，其中我们可以将所提出的方法与最先进的方法进行比较BraTS 挑战。\n3）定量结果：BraTS 2020 训练集的交叉验证性能已在表 IV 中报告。\n所提出的 PANet 在 Dice 分数和 Hausdorff 距离方面优于所有其他网络。此外，我们还在 BraTS 2020 验证集上对训练后的模型进行了验证。通过这种方式，我们还将所提出的网络与模态配对学习（BraTS 2020 中的 Top-2 解决方案）[35] 和研究中的 Transformer TransBTS [23] 进行了比较，除了基于 U 的常用网络之外-网。\n性能列于表五中。我们提出的 PANet 在 BraTS 2020 验证集上优于 U-Net、Attention U-Net、级联 U-Net 和 TransBTS。此外，所提出的 PANet 在 BraTS 2020 上实现了与 Top2 解决方案类似的 Dice 分数，并且具有更好的 Hausdorff 距离。另外，值得注意的是，与 U-Net 相比，所提出的 PANet 仅增加了 3.9% 的额外 GFlops，并且以更少的计算成本实现了比级联 U-Net 更好的性能。这些结果表明，所提出的先验注意网络在 BraTS 2020 数据集上的分割性能和计算效率之间取得了复杂的平衡。\n4）定性结果：我们可视化具有不同分割难度的 3 幅 MRI 图像，以展示不同模型的性能。对于最简单的情况（BraTS20_Validation_077），所有模型都能够分割病变，而所提出的 PANet 获得最高的分割 Dice 分数。对于有一定难度的病例（BraTS20_Validation_028），Attention U-Net和级联U-Net在水肿分割方面都产生了严重的误报，导致整个肿瘤的Dice评分较低。对于最难的情况（BraTS20_Validation_076），由于增强肿瘤的假阳性分割，U-Net、Attention U-Net 和级联 U-Net 的性能并不乐观，而所提出的 PANet 产生了最好的分割性能。所提出的 PANet 的成功归功于具有中间监督的注意力引导解码器，特征图通过空间注意力图进行细化，这可以防止潜在的误报预测。\n关于医学影像中的轴位面（横断面）、冠状面、矢状面的解释\n1.冠状面 （Coronal），又称额状面。即从左右方向，沿人体的长轴将人体纵切为前、后两部分的切面。这种提法只是为了在临床中将器官位置描述的更具体，英文名称是：Coronal section；\n2.矢状面 (Sagittal)就是把人体分成左右两面的解剖面，于这个面平行的也是矢状面。出于这个位置的叫矢状位。矢状位的英文名称是：Median sagittal section；\n3.水平位 (Axial)又称横断位，即左右、前后构成的面为水平位，英文名称是:Transverse section。\n5）消融分析：在 BraTS 2020 验证数据集上也进行了与 2D 分割类似的消融实验，以评估我们模型中呈现的组件的有效性，如表六所示。\na）具有深度监督的多类解码器的有效性：我们通过向网络解码器引入深度监督来构建实验2（U-Net + DS）。与基线（No.1）相比，实验No.2在增强肿瘤、肿瘤核心和整个肿瘤的分割性能方面提供了一定程度的性能提升。此外，当引入注意力引导解码器时，可以观察到性能的提高（第3和第6）。\nb) 注意力引导解码器的有效性：我们构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中注意力引导解码器的有效性。与基线相比，注意力引导解码器在增强肿瘤和肿瘤核心的分割性能方面产生了显着的改善。这表明注意力引导解码器对从编码器提取的特征图产生有效的注意力，从而更容易区分病变。\nc）参数化跳跃连接的有效性：我们建立了两个实验No.4（U-Net + AGD*）和No.5（UNet + AGD）来探索所提出的参数化跳跃连接的有效性。引入参数化跳跃连接后，在增强肿瘤和肿瘤核心方面分割性能有所提高，但整个肿瘤的分割性能略有下降。\nd）中间监督的有效性：我们通过比较表六中的第3号和第5号来调查中间监督策略的有效性。在No.5（U-Net + AGD）中，通过引入中间监督，增强肿瘤和肿瘤核心的Dice得分显着提高。但是当引入深度监督时，中间监督的性能提升并不像以前那么显着（第6和第7），这与2D分割情况类似。\nDISCUSSION AND CONCLUSION 多病灶分割在临床场景中具有重要意义，因为某种疾病可能同时发生多种类型的感染，不同感染阶段的患者可能会出现不同类型的病灶。例如，磨玻璃样混浊（GGO）和实变（CON.）是COVID-19患者典型的肺部病变，前者通常发生在早期患者，而后者的增加可能表明病情恶化。胶质瘤可分为低级别胶质瘤（LGG）和胶质母细胞瘤，即高级别胶质瘤（GBM/HGG），并且在发生HGG的患者中更有可能发现强化肿瘤。因此，多病灶分割在患者的筛查和预后方面具有巨大的潜力。\n多病灶分割问题可以分解为粗分割和细分割，粗分割是对病灶进行粗略分割，而细分割则基于前一分割，以产生最终的分割图。级联网络广泛用于多病变分割任务，因为这些算法背后的逻辑非常自然。然而，级联网络在潜在的临床部署中受到限制，因为它们缺乏灵活性并且对计算资源的要求很高。与现有的级联网络相比，我们开发了一种先验注意网络，它将粗分割和细分割集成到一个网络中。我们提出的网络架构的优点是分割性能和计算效率的平衡。通过将分割的两个步骤结合在一个网络中，所提出的先验注意网络能够实现多病灶分割的端到端训练，在训练和推理方面都具有更大的灵活性，并且在临床部署中具有更大的潜力。此外，我们设法保持所提出的先验注意网络的性能，在分割性能和运行效率之间实现复杂的平衡。\n与级联 U-Net 相比，我们提出的先验注意力网络在 2D 和 3D 任务中都实现了更好的性能和效率，如图 6 所示。\n总之，我们提出了一种新颖的分割网络，即先验注意网络，用于医学图像中的多病灶分割。受流行的从粗到细策略的启发，我们通过空间注意机制将级联网络的两个步骤聚合成一个网络。此外，我们引入了一种新颖的中间监督机制来指导与病变相关的注意图的生成，这可以指导解码器中的后续多类分割。所提出的网络在 2D 和 3D 医学图像（包括 CT 扫描和多模态 MRI）上进行评估。对于 2D 分割，与级联 U-Net 相比，所提出的先验注意力网络以更少的计算成本获得了有竞争力的结果。对于 3D 脑肿瘤分割，所提出的先验注意网络在 BraTS 2020 验证数据集上产生了最先进的性能，并且优于基于 U-Net 的其他基线方法。实验结果表明，该方法在医学影像中的许多多病灶分割任务中具有巨大的应用潜力，与二元分割相比可以提供更多信息，并有助于医生未来的临床诊断。\n",
  "wordCount" : "789",
  "inLanguage": "en",
  "datePublished": "2023-11-28T14:55:47+08:00",
  "dateModified": "2023-11-28T14:55:47+08:00",
  "author":[{
    "@type": "Person",
    "name": "SwimmingLiu"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://swimmingliu.github.io/posts/papernotes/2022-priorattentionnetwork/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "SwimmingLiu's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://swimmingliu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://swimmingliu.github.io/" accesskey="h" title="SwimmingLiu&#39;s Blog (Alt + H)">SwimmingLiu&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://swimmingliu.github.io/index.html" title="🏡 Home">
                    <span>🏡 Home</span>
                </a>
            </li>
            <li>
                <a href="https://swimmingliu.github.io/search/" title="🔍 Search">
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="https://swimmingliu.github.io/posts/" title="🗒️ Posts">
                    <span>🗒️ Posts</span>
                </a>
            </li>
            <li>
                <a href="https://swimmingliu.github.io/archives/" title="📃 Archive">
                    <span>📃 Archive</span>
                </a>
            </li>
            <li>
                <a href="https://swimmingliu.github.io/aboutme/" title="👨🏻‍🎓 About Me">
                    <span>👨🏻‍🎓 About Me</span>
                </a>
            </li>
            <li>
                <a href="https://www.emojisearch.app/" title="Emoji">
                    <span>Emoji</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://swimmingliu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://swimmingliu.github.io/posts/">📚 Posts</a>&nbsp;»&nbsp;<a href="https://swimmingliu.github.io/posts/papernotes/">📝 Paper Notes</a></div>
    <h1 class="post-title">
      Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络
    </h1>
    <div class="post-meta"><span title='2023-11-28 14:55:47 +0800 CST'>November 28, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SwimmingLiu

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details >
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#prior-attention-network-%e7%94%a8%e4%ba%8e%e5%8c%bb%e5%ad%a6%e5%9b%be%e5%83%8f%e4%b8%ad%e5%a4%9a%e7%97%85%e7%81%b6%e5%88%86%e5%89%b2%e7%9a%84%e9%a2%84%e5%85%88%e6%b3%a8%e6%84%8f%e7%bd%91%e7%bb%9c" aria-label="Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络">Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络</a><ul>
                            
                    <li>
                        <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#related-work" aria-label="Related Work">Related Work</a></li>
                    <li>
                        <a href="#method" aria-label="Method">Method</a><ul>
                            
                    <li>
                        <a href="#31--overview-of-network-architecture" aria-label="3.1 . Overview of Network Architecture">3.1 . Overview of Network Architecture</a></li>
                    <li>
                        <a href="#32-attention-guiding-decoder" aria-label="3.2 Attention Guiding Decoder">3.2 Attention Guiding Decoder</a></li>
                    <li>
                        <a href="#33-parameterized-skip-connections" aria-label="3.3 Parameterized Skip Connections">3.3 Parameterized Skip Connections</a></li>
                    <li>
                        <a href="#34-multi-class-decoder-with-deep-supervision" aria-label="3.4 Multi-Class Decoder With Deep Supervision">3.4 Multi-Class Decoder With Deep Supervision</a></li></ul>
                    </li>
                    <li>
                        <a href="#experiments" aria-label="Experiments">Experiments</a><ul>
                            
                    <li>
                        <a href="#41-strong-baselines-and-evaluation-metrics" aria-label="4.1 Strong Baselines and Evaluation Metrics">4.1 Strong Baselines and Evaluation Metrics</a></li>
                    <li>
                        <a href="#42-%e8%82%ba%e9%83%a8-ct-%e5%88%87%e7%89%87%e4%b8%ad%e7%9a%84-covid-19-%e7%97%85%e7%81%b6%e7%9a%84-2d-%e5%a4%9a%e7%97%85%e7%81%b6%e5%88%86%e5%89%b2" aria-label="4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割">4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割</a></li>
                    <li>
                        <a href="#43-3d-multi-lesion-segmentation-of-brain-tumor-from-multi-modality-brain-mris" aria-label="4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs">4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs</a></li></ul>
                    </li>
                    <li>
                        <a href="#discussion-and-conclusion" aria-label="DISCUSSION AND CONCLUSION">DISCUSSION AND CONCLUSION</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h1 id="prior-attention-network-用于医学图像中多病灶分割的预先注意网络">Prior Attention Network: 用于医学图像中多病灶分割的预先注意网络<a hidden class="anchor" aria-hidden="true" href="#prior-attention-network-用于医学图像中多病灶分割的预先注意网络">#</a></h1>
<h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<blockquote>
<p>医学图像中邻近组织的多种类型病变的准确分割在临床实践中具有重要意义。基于从<strong>粗到精策略的卷积神经网络（CNN）<strong>已广泛应用于该领域。然而，由于</strong>组织的大小、对比度和高类间相似性的不确定性</strong>，多病灶分割仍然具有挑战性。此外，<strong>普遍采用的级联策略</strong>对<strong>硬件要求较高</strong>，限制了临床部署的潜力。为了解决上述问题，我们提出了一种新颖的先验注意网络（PANet），它<strong>遵循从粗到细的策略</strong>来在医学图像中执行多病灶分割。所提出的网络通过在网络中<strong>插入与病变相关的空间注意机制</strong>，在单个网络中实现了<strong>两个步骤的分割</strong>。此外，我们还提出了<strong>中间监督策略</strong>，用于<strong>生成与病变相关的注意力</strong>来获取<strong>感兴趣区域（ROI）</strong>，这加速了收敛并明显提高了分割性能。我们在两个应用中研究了所提出的分割框架：<strong>肺部 CT 切片</strong>中多发性<strong>肺部感染的 2D 分割</strong>和<strong>脑 MRI 中多发性病变的 3D 分割</strong>。实验结果表明，与级联网络相比，在 2D 和 3D 分割任务中，我们提出的网络以更少的计算成本实现了更好的性能。所提出的网络可以被视为 2D 和 3D 任务中多病灶分割的通用解决方案。源代码可在 <a href="https://github.com/hsiangyuzhao/PANet">https://github.com/hsiangyuzhao/PANet</a> 获取</p>
</blockquote>
<p>问题导向：</p>
<p>①<strong>组织的大小、对比度和高类间相似性的不确定性</strong></p>
<p>②多类别病灶分割</p>
<p>③<strong>普遍采用的级联策略</strong>对<strong>硬件要求较高</strong></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<blockquote>
<p><strong>医学图像分割对于疾病的准确筛查和患者的预后具有重要意义。基于病灶分割的病灶评估提供了疾病进展的信息，帮助医生提高临床诊断和治疗的质量。然而，手动病变分割相当主观且费力，这限制了其潜在的临床应用。近年来，随着人工智能的快速发展，基于深度学习的算法得到了广泛的应用，并在医学图像分割方面取得了最先进的性能[1]</strong>。卷积神经网络（CNN）由于其高分割质量而在医学图像中的病变分割中很受欢迎。此类算法通常具有<strong>深度编码器</strong>，可从输入图像中自动提取特征，并通过以下操作<strong>生成密集预测</strong>。例如，Long等人[2]提出了一种用于图像语义分割的全卷积网络，该网络颇具影响力，并启发了后来的医学分割中的端到端框架。 Ronneberger等人[3]提出了一种用于医学图像分割的U形网络（U-Net），该网络在医学分割的许多领域都显示出了可喜的结果，并已成为许多医学分割任务的虚拟基准。</p>
</blockquote>
<p>这一段都可以当成经典医学图像分割的背景引入</p>
<blockquote>
<p>然而，尽管医学分割取得了这些突破，但目前的<strong>医学分割方法主要集中在病灶的二元分割上</strong>，即<strong>区分病灶（前景）和其他一切（背景）</strong>。尽管二元分割确实有助于<strong>隔离某些感兴趣区域</strong>并允许对<strong>医学图像进行精确分析</strong>，但在某些需要<strong>对病变进行多类分割的场景中，二元分割还不够</strong>。与二元分割相比，由于<strong>组织的类间相似性，这种情况要困难得多</strong>，因为<strong>不同类型的病变在纹理、大小和形状上可能相似</strong>。具有从粗到细策略的级联网络已广泛应用于此类场景，例如<strong>肝脏和病变的分割、脑肿瘤分割</strong>[4]、[5][6]、[7]。</p>
<p>此类网络通常由两个独立的网络组成，其中第一个<strong>网络执行粗分割，第二个网络基于从第一个网络分割的 ROI 细化分割</strong>。然而，尽管<strong>级联网络</strong>已广泛应用于医学图像的<strong>多病灶分割</strong>，但级联策略也有其缺点。由于<strong>级联网络由两个独立的网络组成</strong>，<strong>参数量和显存占用通常是单个网络的两倍</strong>，这对硬件要求较高，限制了其在临床使用的潜力。更重要的是，由于级联网络中的两个网络通常是独立的，因此<strong>级联网络的训练过程有时比单个网络更困难</strong>，这可能导致欠拟合。</p>
</blockquote>
<p>级联网络：参数量大、容易欠拟合。</p>
<blockquote>
<p>在本文中，我们提出了一种名为先验注意网络（PANet）的新型网络结构，用于在医学图像中执行<strong>多病灶分割</strong>。所提出的网络由<strong>一个</strong>用于<strong>特征提取的编码器</strong>和<strong>两个分别生成病变区域注意力和最终预测的解码器组成</strong>。该网络与<strong>注意力机制</strong>结合在一起。为了<strong>减少参数大小和硬件</strong>占用，我们使用<strong>网络编码器的深层、语义丰富的特征</strong>来<strong>生成病变区域的空间注意力</strong>。</p>
<p>然后，<strong>编码器生成的特征</strong>表示通过<strong>空间注意力</strong>进行细化，并将其发送到解码器以进行<strong>最终的多类预测</strong>。为了<strong>提高分割性能并加速收敛</strong>，我们还在网络结构中引入了<strong>中间监督和深度监督</strong>。通过这些改进，与传统的级联网络相比，所提出的网络以<strong>显着降低的参数大小和计算成本</strong>实现了有竞争力的结果。</p>
</blockquote>
<p>利用网络编码器的深层、特征信息来生成空间注意力（WTF ???）</p>
<p>中间监督、深度监督 （不错不错， 好多一区和顶会的文章都用深度监督）</p>
<blockquote>
<p>这项工作的贡献体现在三个方面。<strong>首先</strong>，我们提出了一种新颖的网络架构，通过将传统级联网络中的两个分割步骤结合在<strong>单个网络</strong>中，遵循 2D 和 3D 医学图像中多病灶分割的<strong>从粗到细的策略</strong>。与级联网络相比，所提出的架构以更少的额外计算成本实现了有竞争力的分割性能，更容易训练和部署到生产环境。<strong>其次</strong>，我们提出了一种<strong>监督空间注意力机制</strong>，将<strong>病变区域的注意力与网络提取的特征相结合</strong>，将多病变分割分解为<strong>两个更容易的阶段</strong>，并且与当前<strong>基于注意力的方法相比具有更好的可解释性</strong>。<strong>第三</strong>，所提出的网络已在两个实际应用中得到验证，<strong>包括肺部 CT 切片中的 COVID-19 病变的 2D 分割和多模态 MRI 中的脑肿瘤的 3D 分割</strong>。所提出的网络在 2D 和 3D 任务中都优于前沿方法，并且在参数和计算成本方面比当前网络更高效。</p>
</blockquote>
<p>一个网络、监督空间注意力机制、参数和计算成本方面比当前网络更高效。</p>
<h2 id="related-work">Related Work<a hidden class="anchor" aria-hidden="true" href="#related-work">#</a></h2>
<blockquote>
<p>1）图像分割的网络结构：用于图像分割的典型卷积神经网络通常由一个卷积特征提取器组成，其拓扑类似于常见的分类网络，自动从输入图像中提取特征，并进行基于卷积的操作以生成最终的密集预测。在自然图像分割领域，FCN [2]、DeepLab [8]、PSPNet [9] 和 SegNet [10] 因其性能和效率而颇受欢迎。对于医学分割，U-Net [3] 在许多任务中相当流行，并且已被修改为许多改进版本，例如 Attention U-Net [11]、U-Net++ [12]、V-Net [13] 和H-DenseUNet [14]在某些领域获得更好的性能。</p>
<p>2）级联网络在医学分割中的应用：<strong>级联网络</strong>已广泛应用于<strong>正常组织和病变的分割以及不同类型病变的分割</strong>，包括<strong>肝脏病变、脑肿瘤、硬化病变和前列腺癌</strong>的分割[4] ，[15][5]，[16]。例如，Awad 等人[17]提出了一个名为 <strong>CU-Net</strong> 的级联框架，用于在 <strong>CT 扫描中对肝脏和病变进行自动分割</strong>。他们还提供了可以指导临床治疗的有用信息和解释。 Xi等人[18]提出了一种<strong>级联U-ResNets</strong>，它遵循一种新颖的<strong>垂直级联策略</strong>，并在他们的工作中评估了<strong>不同类型的损失函数</strong>。除了肝脏病灶分割之外，级联策略在 BraTS 挑战中也很流行。例如，BraTS 2019挑战赛的Top-2解决方案[6]、[7]都是具有不同级联策略的级联网络。</p>
<p>3）神经网络中的注意力：注意力机制受到人类感知和视觉认知的启发，并已普遍应用于计算机视觉任务中[19]，[20][11]，[21]。计算机视觉任务中的<strong>注意力机制</strong>是在<strong>神经网络提取的特征表示上生成空间或通道权重图</strong>。例如，Woo等人[20]开发了一个卷积块注意力模块（CBAM）来引入一种融合注意力机制，其中包括<strong>通道注意力和空间注意力</strong>。这种注意力模块可以插入到<strong>常用的分类或分割网络中</strong>。Oktay等人[11]在Attention U-Net中提出了一种新颖的注意力门，用于细化网络编码器提取的特征表示，<strong>以促进网络专注于ROI</strong>。最近，首先在自然语言处理任务中提出的变压器[22]已被引入到医学分割任务中。例如，Wang 等人 [23] 提出了一种 TransBTS，用于从多模态脑 MRI 中执行脑肿瘤分割。</p>
<p>综上所述，注意力机制已被广泛用于突出ROI并抑制不相关信息，但目前基于注意力的方法的研究并没有对注意力如何产生以及网络为何关注某些区域提供清晰的解释，这使得限制了注意力机制的可解释性。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3bh0.png" alt="image-20231127123123123"  />
</p>
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<p>在本节中，我们将详细介绍所提出的先验注意网络架构。在第一部分中，我们将概述所提议的网络。然后，我们相应地提供有关具有<strong>中间监督、参数化跳跃连接和具有深度监督的多类解码器</strong>的所提出的<strong>注意引导解码器的详细信息</strong>。</p>
<h3 id="31--overview-of-network-architecture">3.1 . Overview of Network Architecture<a hidden class="anchor" aria-hidden="true" href="#31--overview-of-network-architecture">#</a></h3>
<blockquote>
<p>基本上，我们提出的网络是基于 U-Net [3] 架构进行修改的，该架构具有 U 形拓扑以及编码器和解码器之间的跳跃连接。在提出的<strong>先验注意网络</strong>中，一种新颖的<strong>注意引导解码器模块</strong>被集成到网络的<strong>跳跃连接</strong>中，以通过<strong>空间注意来细化特征表示</strong>。网络中还引入了一种新颖的<strong>参数化跳跃连接</strong>，以指导网络学习<strong>普通特征图和精炼特征图之间</strong>的<strong>比率</strong>。注意力引导解码器从编码器获取丰富的语义特征并<strong>生成空间注意力图</strong>来指导接下来的<strong>多类分割</strong>。为了产生与投资回报率相关的注意力，框架中使用了<strong>中间监督策略</strong>。然后将<strong>细化的特征图发送到多类解码器</strong>以进行最终的密集预测。</p>
<p>多类解码器采用<strong>深度监督策略</strong>以获得<strong>更好的收敛性并提高分割性能</strong>。这种网络拓扑<strong>通过注意力引导解码器生成的注意力图</strong>在单个网络中实现了<strong>传统级联网络</strong>的两个步骤。组网方案如图2所示。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3oUC.png" alt="image-20231127114826756"  />
</p>
<h3 id="32-attention-guiding-decoder">3.2 Attention Guiding Decoder<a hidden class="anchor" aria-hidden="true" href="#32-attention-guiding-decoder">#</a></h3>
<blockquote>
<p>在典型的级联网络中，分割的第一步是<strong>执行粗分割</strong>并找到输入图像中的 <strong>ROI</strong>。在提出的先验注意网络中，我们提出<strong>了一个注意引导解码器</strong>来执行该过程。所提出的<strong>注意力引导解码器</strong>被集成到网络中<strong>以生成与 ROI 相关的注意力图</strong>，然后利用<strong>这些图来细化特征</strong>表示并提高多类分割性能。</p>
<p>1）模块拓扑：所提出的<strong>注意力引导解码器的基本拓扑</strong>基于FCN [2]中提出的特征融合。从<strong>网络解码器最深三层提取的特征表示</strong>被馈送到<strong>该模块</strong>。由于特征图的空间大小不同，因此<strong>首先执行线性插值以对特征图进行上采样</strong>。然后对<strong>特征进行压缩</strong>，<strong>抑制通道维度中的不相关信息</strong>，<strong>降低计算成本</strong>。然后将压缩后的特征分别在<strong>通道维度上连接起来以进行特征融合</strong>。(torch.cat) 最后，<strong>融合三个特征图以获得最终的预测</strong>。</p>
<p>为了简单起见，我们使用 2D 分割来说明注意力图的计算。我们使用 Xi ∈ R Ci × Hi × Wi,i ∈ (3, 4, 5) 表示从网络编码器提取的特征图，其中 X5 表示最深的特征。特征压缩和融合计算如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p38uW.png" alt="image-20231127115911999"  />
</p>
<p>其中Z5 ∈ R C4 × H4 × W4表示X5和X4的融合特征，Z4 ∈ R C3 × H3 × W3表示X4和X3的融合特征，Wc5 ∈RC5×C4和Wc4 ∈RC4×C3表示相应的压缩卷积， W4 ∈ RC4×C4 表示融合 X5 和 X4 的融合卷积，⊕ 表示特征串联。</p>
<p>注意力引导解码器的输出计算如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3dMJ.png" alt="image-20231127120221195"  />
</p>
<p>其中 W3 ∈ R C3×C3 表示融合 X4 和 X3 的融合卷积，Wout ∈ R C3 × 1 表示输出卷积，σ 分别表示 Sigmoid 激活。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3Ddv.png" alt="image-20231127120200365"  />
</p>
<blockquote>
<p>2）中间监督：计算机视觉中的传统注意力机制自动生成注意力图，但<strong>注意力生成的过程通常是人类无法解释的</strong>，并且<strong>网络关注的区域可能与人类关注的区域不同</strong>。</p>
<p><strong>这种差距会限制注意力机制的性能和可解释性，有时还会导致网络容量的恶化</strong>。为了解决这些问题，我们在网络中引入了<strong>中间监督策略</strong>。在遵循<strong>从粗到细的方式的多病灶分割任务</strong>中，我们首先生成一个二元Ground Truth，其中前景表示所有类型的病灶，背景表示其他一切。在具有 C 种病变的多病变分割任务中，我们使用 Gi,i ∈ (1,&hellip;,C) 表示第 i 类病变的二元基本事实，其中前景表示特定病变背景代表其他一切。二进制真实值 Gb 计算如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p35GV.png" alt="image-20231127120613763"  />
</p>
<p>然后利用二元损失函数来计算<strong>二元真实值</strong> yb 和<strong>注意力引导解码器生成的注意力图</strong> Y 之间的<strong>二元损失 l</strong>：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p34eI.png" alt="image-20231127120701851"  />
</p>
<p>其中 Lb 表示二元损失函数。然后利用计算出的损失 l 来监督注意力引导解码器的参数更新。</p>
</blockquote>
<p>这个地方的中间监督主要是 要让中间的注意力机制起作用，不能随便生成。</p>
<blockquote>
<p>通过引入中间监督，生成的注意力由输入图像的二元真实值进行监督。这样，<strong>网络被迫学习多病灶分割任务的分解</strong>，即<strong>首先提取病灶区域</strong>，然后<strong>对病灶区域进行细粒度分类</strong>。这种分解降低了<strong>多病灶分割的难度</strong>，并且与当前在“黑匣子”中生成注意力的基于注意力的医学分割方法相比，具有更好的可解释性。</p>
</blockquote>
<h3 id="33-parameterized-skip-connections">3.3 Parameterized Skip Connections<a hidden class="anchor" aria-hidden="true" href="#33-parameterized-skip-connections">#</a></h3>
<blockquote>
<p>跳跃连接已广泛应用于流行的卷积网络中，包括U-Net [3]、ResNet [24]等。受[11]的启发，我们建议将<strong>注意力图</strong>集成到<strong>连接网络编码器和多网络</strong>的跳跃连接中，形成多级解码器。在跳跃连接中，我们还引入了额外的残差路径来<strong>恢复普通特征图</strong>并进一步<strong>提高分割性能</strong>。与传统的残差路径相比，<strong>残差路径的幅值因子</strong>αi,i ∈ (1, 2,…, 5)被设置为网络的可学习参数，并在反向传播过程中更新。我们相信这样的设置可以为网络增加额外的非线性能力并增强跳过连接的有效性。</p>
<p>我们使用 Fi,i ∈ (1, 2,&hellip;, 5) 表示来自网络编码器的<strong>普通特征图</strong>，Y 表示<strong>注意力图</strong>，精炼后的特征图 Fri,i ∈ (1, 2,. .., 5) 多类解码器接收的信息计算如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3j6j.png" alt="image-20231127121513541"  />
</p>
<p>然后将<strong>细化的特征图</strong>发送到<strong>多类解码器</strong>以进行最终的多类预测。</p>
</blockquote>
<h3 id="34-multi-class-decoder-with-deep-supervision">3.4 Multi-Class Decoder With Deep Supervision<a hidden class="anchor" aria-hidden="true" href="#34-multi-class-decoder-with-deep-supervision">#</a></h3>
<blockquote>
<p>U形分割网络中的解码器用于接收编码器发送的特征图，随着解码器中的<strong>特征通道数量的减少</strong>和<strong>空间分辨率的增加</strong>，<strong>分割性能逐步细化</strong>。然而，随着网络变深，最深的解码器块变得难以训练，这可能会限制最终的分割性能。<strong>深度监督策略已经被提出来训练深度卷积网络</strong>[25]、[26]。在所提出的先验注意网络中，辅助预测是从不同级别的解码器块中提取的，并使用相同的基本事实进行监督。我们使用 Pi,i ∈ (1, 2, 3) 表示来自多类解码器的辅助预测，Pm 表示最终的多类预测，g 表示真实值，Lm 表示多类损失函数。最终的多类损失计算如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3iP2.png" alt="image-20231127121708354"  />
</p>
<p>所提出的多类解码器的解码器块也与当前网络设置具有共同的设置，即卷积层、归一化层和非线性激活单元的堆栈。</p>
</blockquote>
<p>总结一下：</p>
<p>① 注意力机制 + 中间监督：最后三层特征融合 + 这一部分做深度监督（原来这样也叫创新）</p>
<p>② 跳跃连接的部分加了一个α因子 （感觉像权重一样的东西  ）</p>
<p>③ 多阶段的深度监督 （这个就算一个trick吧，大家都在用）， 不过这里变成了多类别</p>
<h2 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h2>
<h3 id="41-strong-baselines-and-evaluation-metrics">4.1 Strong Baselines and Evaluation Metrics<a hidden class="anchor" aria-hidden="true" href="#41-strong-baselines-and-evaluation-metrics">#</a></h3>
<blockquote>
<p>为了研究网络架构的性能差异，我们将所提出的先验注意网络与医学分割中最流行的方法进行了比较，包括 U-Net [3]、Attention U-Net [11] 和级联 U-Net，在两个 2D 中和 3D 分割任务。值得注意的是，与<strong>他们论文中提出的原始版本相比</strong>，基<strong>线方法根据网络拓扑方面的某些任务进行了修改和优化</strong>，以获<strong>得性能提升</strong>。我们将残差连接[24]、批量归一化[30]和来自 ImageNet 的预训练编码器引入到 2D COVIDlesion 分割任务的基线方法中，并且我们还将残差连接、实例归一化和 PReLU 激活引入到 3D 脑肿瘤分割任务中。除了网络拓扑之外，基线方法与所提出的先验注意网络共享相同的数据增强和训练配置。</p>
<p>对于 COVID-19 病变的 2D 分割，我们使用 Dice 指数、精度分数和召回分数来评估所提出的网络的性能。 Dice指数是一种用来衡量两个样本相似度的统计量，已广泛用于分割算法的评估。精确率衡量的是实际正确的阳性识别的比例，召回率衡量的是算法对阳性样本的敏感度。对于 BraTS 2020 挑战赛的 3D 分割，在在线门户上进行评估，并根据 Dice 指数和 95% Hausdorff 距离（HD）对算法进行排名。</p>
<p>我们使用 G 表示ground truth，P 表示密集预测，TP 表示正确预测的正样本，FP 表示错误预测的正样本，TN 表示正确预测的副样本，FN 表示错误预测的副样本。这些指标的计算方式如下：</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3Jhd.png" alt="image-20231127185553944"  />
</p>
</blockquote>
<p>这个HD（Hausdorff ）也是一种评估分割结果的方式 （alright  又多了一种指标）</p>
<h3 id="42-肺部-ct-切片中的-covid-19-病灶的-2d-多病灶分割">4.2 肺部 CT 切片中的 COVID-19 病灶的 2D 多病灶分割<a hidden class="anchor" aria-hidden="true" href="#42-肺部-ct-切片中的-covid-19-病灶的-2d-多病灶分割">#</a></h3>
<blockquote>
<p>1）数据：由于可用的开源COVID-19 CT分割数据集通常很小，因此利用<strong>两个独立的公开可用数据集</strong>，即<strong>COVID-19 CT分割数据集[27]和CC-CCII数据集[28]<strong>来验证所提出的方法二维分割任务中的方法。第一个数据集</strong>包含来自 40 多名患者的 100 个轴向 CT 切片</strong>，这些切片已重新缩放<strong>至 512 × 512 像素并进行灰度化</strong>。所有切片均由放射科医生<strong>用不同的标签</strong>进行分割，以识别<strong>不同类型的肺部感染</strong>。第二个数据集由 <strong>150 名 COVID-19 患者的 750 张 CT 切片</strong>组成，这些切片被手动分割为<strong>背景、肺野、毛玻璃混浊和实变</strong>。由于并<strong>非所有 750 个切片都包含病变</strong>，我们最终使用了 <strong>150 名患者的 549 个带注释的切片</strong>。对于这两个数据集，利用 <strong>5 倍交叉验证</strong>来评估所提出模型的性能。</p>
<p>折叠之间的数据根据患者进行分割，以避免潜在的数据泄漏。最终的标签和分割图包含 3 个类别，包括背景、<strong>毛玻璃不透明度</strong> (GGO) 和<strong>合并</strong> (CON.)。</p>
<p>2）实现细节：a）<strong>模型设置和损失函数</strong>：对于预训练网络编码器，我们采用来自ImageNet的预训练ResNeXt-50（32 × 4d）[31]作为基线方法和所提出的<strong>先验注意网络</strong>的编码器。对于解码器中的上采样，<strong>采用双线性插值</strong>，<strong>比例因子为2</strong>。对于中间监督和级联U-Net第一阶段的二元损失函数，我们采用<strong>Dice Loss [13]和Focal的线性组合损失</strong>[32]作为损失函数。对于最终输出的多类损失函数，我们采用<strong>Focal Tversky Loss</strong> [33]作为损失函数。</p>
<p>b) 训练细节：我们的模型是在 Ubuntu 16.04 服务器上使用 PyTorch 1.7.1 框架实现的。我们使用 NVIDIA RTX 2080 Ti GPU 来加速我们的训练过程。在我们的训练过程中使用<strong>Albumentations</strong> [34] 进行数据增强，以<strong>减少过度拟合</strong>并<strong>提高泛化能力</strong>。首先，将所有<strong>输入图像重新缩放为 560 × 560</strong>，然后进行<strong>随机亮度和对比度偏移以及随机仿射变换</strong>。然后将图像随机裁剪为 512 × 512，然后进行随机弹性变换，最后输入网络。<strong>该模型由 Adam 优化器优化</strong>，β1 = 0.9、β2 = 0.999、γ = 1e − 8。L2 正则化也用于减少过度拟合。我们将模型权重衰减设置为 1e − 5。初始学习率设置为 1e −4 并降低，然后采用余弦退火策略。批量大小设置为 4，模型训练 40 轮。该模型使用 5 倍交叉验证进行评估。</p>
<p>3）定量结果：我们在两个数据集上的实验中不同模型的详细比较分别如<strong>表一和表二</strong>所示。如图所示，我们提出的网络在毛玻璃不透明度和固结的 Dice 分数方面优于 U-Net、Attention U-Net。所提出的 <strong>PANet 以更少的参数和计算成本</strong>实现了与级联 U-Net 竞争的结果。由于这些模型在模型主干和训练策略上是相同的，很明显，所提出的<strong>注意力引导解码器、中间监督和深度监督</strong>的组合对分割性能有很大贡献。<strong>注意力引导解码器的利用</strong>有助于模型更准确地检测感染组织并生成与感染相关的注意力图，从而有利于解码器中的多类分割。</p>
<p>此外，中间监督和深度监督的引入促进了网络的收敛，这也有助于提高性能。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3zUK.png" alt="image-20231127190349627"  />
</p>
<p>好好好，这哥们儿，睁着眼睛说瞎话是吧（这Unet明明比你低啊，精度也没差多少啊）</p>
<blockquote>
<p>4）定性结果：不同模型在 2D COVID-19 切片上的视觉比较如图 4 所示。由于模型在 Dice 分数方面非常接近，因此乍一看这些模型的表现相似。但与 U-Net 和 Attention U-Net 相比，所提出的 PANet 在实变和微小病变的分割上表现更好。</p>
<p>与 U-Net 和 Attention U-Net 相比，PANet 产生更准确的分割掩模，并且与 Cascaded U-Net 相比，所提出的网络以更少的计算成本实现了有竞争力的结果。</p>
</blockquote>
<p>额 只要定量结果上去了，好像定性结果都是挑好的说吧？</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p31Fl.png" alt="image-20231128140016412"  />
</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3gdu.png" alt="image-20231128140032492"  />
</p>
<blockquote>
<ol start="5">
<li>) 消融实验：进行了几次消融实验来评估我们模型中组件的性能，如表 III 所示。</li>
</ol>
<p>a）具有深度监督的多类解码器的有效性：为了探索深度监督策略的贡献，我们建立了两个实验：No.1（U-Net）和No.2（U-Net + DS）。表三的结果表明，深度监督在一定程度上对绩效有所贡献。</p>
<p>b）注意力引导解码器的有效性：我们通过构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中所提出的注意力引导解码器的有效性。如表III所示，与实验1相比，注意力引导解码器的引入提供了显着的性能提升。这表明注意力引导解码器在所提出的网络中提供了有效的注意力图，从而指导解码器中的多类分割。</p>
<p>c）参数化跳跃连接的有效性：为了探索所提出的参数化跳跃连接的有效性，我们建立了两个实验4（U-Net + AGD*）和5（U-Net + AGD）。引入参数化跳跃连接后，分割性能得到了提高，几乎没有额外的参数或计算成本。</p>
<p>d）中间监督的有效性：为了研究所提出的 PANet 中中间监督策略的有效性，我们比较了第 3 号（U-Net + AGD w/o IS）和第 5 号（U-Net + AGD）。如表 III 所示，与第 3 种相比，具有中间监督的网络获得了额外的改进。此外，在比较第 6 种（U-Net + DS + AGD w/o IS）和第 7 种（PANet）时也可以观察到改进。 ）尽管改进相对较小。可以看出，深度监管的引入也对绩效产生了提升，因此中级监管对绩效的提升并不像以前那么显着。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3eST.png" alt="image-20231128140745555"  />
</p>
<h3 id="43-3d-multi-lesion-segmentation-of-brain-tumor-from-multi-modality-brain-mris">4.3 3D Multi-Lesion Segmentation of Brain Tumor From Multi-Modality Brain MRIs<a hidden class="anchor" aria-hidden="true" href="#43-3d-multi-lesion-segmentation-of-brain-tumor-from-multi-modality-brain-mris">#</a></h3>
<blockquote>
<p>1）数据：我们使用来自 BraTS 2020 挑战赛的开源多模态 MRI 数据集 [29]、[36] [37]。训练集由 369 个多对比 MRI 扫描组成，其中每个扫描包含四种模式，即原生 T1 加权、对比后 T1 加权 (T1Gd)、T2 加权 (T2) 和 T2 流体衰减反转恢复 (FLAIR) ）。</p>
<p>每次扫描都有相应的 4 类标签：背景（标签 0）、GD 增强肿瘤（ET，标签 4）、瘤周水肿（ED，标签 2）以及坏死和非增强肿瘤核心（NET/ NCR，标签 1)。验证集由 125 个多重对比 MRI 扫描组成，其模式与训练集和隐藏的基本事实相同。所有 MRI 扫描均去除颅骨，与相同的大脑模板 (SRI24) 对齐，并插值至 1mm3 分辨率。验证阶段通过在线门户进行，算法根据 3 个重叠肿瘤区域的性能进行排名，即增强肿瘤 (ET)、肿瘤核心 (ET + NET/NCR) 和整个肿瘤 (ET) + NET/NCR + ED）</p>
<p>2）实现细节：a）模型设置和损失函数：对于3D分割，由于缺乏开源预训练编码器，所有模型都是从头开始训练的。下采样通过跨步 <strong>3 × 3 × 3 填充卷积</strong>执行，<strong>上采样通过三线性插值</strong>实现。为了进一步提高在BraTS数据集上的性能，我们采用基于区域的训练策略（直接在重叠区域而不是独立标签上优化）并增强肿瘤抑制（如果增强肿瘤的预测体积为，则用坏死替换预测的增强肿瘤）小于某个阈值）在训练过程中。对于损失函数，我们采用 Dice Loss [13] 和 Cross Entropy Loss 的线性组合作为网络中<strong>二分类和多分类</strong>阶段的损失函数。</p>
<p>b) 训练细节：我们的模型是在 Ubuntu 服务器上使用 PyTorch 1.7.1 框架实现的。由于3D分割的训练，尤其是级联U-Net对显存的要求较高，因此我们使用NVIDIA RTX 2080 Ti GPU和NVIDIA RTX 3090 GPU分别训练单个模型和级联模型。</p>
<p>由于训练过程的显存占用大于11Gb，我们采用PyTorch框架提供的原生混合精度训练程序来节省显存使用并加速训练过程。人工智能医学开放网络（MONAI）项目[38]和TorchIO[39]分别用于训练和推理阶段的数据加载过程。数据增强是在训练过程中通过 MONAI 项目进行的。</p>
<p>首先，<strong>分别使用 z 分数标准化对所有模态进行标准化。然后通过随机翻转、随机强度偏移、随机强度缩放和弹性变换来增强图像。<strong>最后，我们将图像块随机裁剪为 <strong>128 × 128 × 128</strong> 并将其输入网络。对于推理，我</strong>们还采用基于补丁的推理管道来生成 BraTS 2020 验证集的预测</strong>。面片大小设置为 128 × 128 × 128，<strong>面片之间的重叠设置为 75%</strong>。重叠区域中的预测是重叠块的平均值。这种重叠配置可以被视为自集成并产生更好的分割性能。对于模型评估，我们进行了 2 个单独的评估程序来比较分割模型的性能。首先，<strong>我们对 BraTS 2020 训练集进行 5 倍交叉验证</strong>，以比较<strong>离散区域（增强肿瘤、瘤周水肿和非增强肿瘤核心）上的分割性能</strong>。然后，我们对 BraTS 2020 验证集进行评估，以比较重叠区域（增强肿瘤、肿瘤核心和整个肿瘤）的分割性能，其中我们可以将所提出的方法与最先进的方法进行比较BraTS 挑战。</p>
<p>3）定量结果：BraTS 2020 训练集的交叉验证性能已在表 IV 中报告。</p>
<p>所提出的 PANet 在 <strong>Dice 分数和 Hausdorff 距离方面</strong>优于所有其他网络。此外，我们还在 BraTS 2020 验证集上对训练后的模型进行了验证。通过这种方式，我们还将所提出的网络与模态配对学习（BraTS 2020 中的 Top-2 解决方案）[35] 和研究中的 Transformer TransBTS [23] 进行了比较，除了基于 U 的常用网络之外-网。</p>
<p>性能列于表五中。我们提出的 PANet 在 BraTS 2020 验证集上优于 U-Net、Attention U-Net、级联 U-Net 和 TransBTS。此外，所提出的 PANet 在 BraTS 2020 上实现了与 Top2 解决方案类似的 Dice 分数，并且具有更好的 Hausdorff 距离。另外，值得注意的是，与 U-Net 相比，所提出的 PANet 仅增加了 3.9% 的额外 GFlops，并且以更少的计算成本实现了比级联 U-Net 更好的性能。这些结果表明，所提出的先验注意网络在 BraTS 2020 数据集上的分割性能和计算效率之间取得了复杂的平衡。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3cMp.png" alt="image-20231128143139994"  />
</p>
<blockquote>
<p>4）定性结果：我们可视化具有<strong>不同分割难度</strong>的 3 幅 MRI 图像，以展示不同模型的性能。对于最简单的情况（BraTS20_Validation_077），所有模型都能够分割病变，而所提出的 PANet 获得最高的<strong>分割 Dice 分数</strong>。对于有一定难度的病例（BraTS20_Validation_028），Attention U-Net和级联U-Net在<strong>水肿分割方面都产生了严重的误报</strong>，导致整个<strong>肿瘤的Dice评分较低</strong>。对于最难的情况（BraTS20_Validation_076），由于<strong>增强肿瘤的假阳性分割</strong>，U-Net、Attention U-Net 和级联 U-Net 的性能并不乐观，而所提出的 PANet 产生了最好的分割性能。所提出的 PANet 的成功归功于具有中间监督的注意力引导解码器，特征图通过空间注意力图进行细化，这可以防止潜在的误报预测。</p>
</blockquote>
<p>关于医学影像中的轴位面（横断面）、冠状面、矢状面的解释</p>
<p>1.冠状面 （Coronal），又称额状面。即从左右方向，沿人体的长轴将人体纵切为前、后两部分的切面。这种提法只是为了在临床中将器官位置描述的更具体，英文名称是：Coronal section；</p>
<p>2.矢状面 (Sagittal)就是把人体分成左右两面的解剖面，于这个面平行的也是矢状面。出于这个位置的叫矢状位。矢状位的英文名称是：Median sagittal section；</p>
<p>3.水平位 (Axial)又称横断位，即左右、前后构成的面为水平位，英文名称是:Transverse section。</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p376m.png" alt="image-20231128143857189"  />
</p>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3Q9R.png" alt="image-20231128143306784"  />
</p>
<blockquote>
<p>5）消融分析：在 BraTS 2020 验证数据集上也进行了与 2D 分割类似的消融实验，以评估我们模型中呈现的组件的有效性，如表六所示。</p>
<p>a）具有深度监督的多类解码器的有效性：我们通过向网络解码器引入深度监督来构建实验2（U-Net + DS）。与基线（No.1）相比，实验No.2在增强肿瘤、肿瘤核心和整个肿瘤的分割性能方面提供了一定程度的性能提升。此外，当引入注意力引导解码器时，可以观察到性能的提高（第3和第6）。</p>
<p>b) 注意力引导解码器的有效性：我们构建实验 3（U-Net + AGD w/o IS）来研究所提出的网络中注意力引导解码器的有效性。与基线相比，注意力引导解码器在增强肿瘤和肿瘤核心的分割性能方面产生了显着的改善。这表明注意力引导解码器对从编码器提取的特征图产生有效的注意力，从而更容易区分病变。</p>
<p>c）参数化跳跃连接的有效性：我们建立了两个实验No.4（U-Net + AGD*）和No.5（UNet + AGD）来探索所提出的参数化跳跃连接的有效性。引入参数化跳跃连接后，在增强肿瘤和肿瘤核心方面分割性能有所提高，但整个肿瘤的分割性能略有下降。</p>
<p>d）中间监督的有效性：我们通过比较表六中的第3号和第5号来调查中间监督策略的有效性。在No.5（U-Net + AGD）中，通过引入中间监督，增强肿瘤和肿瘤核心的Dice得分显着提高。但是当引入深度监督时，中间监督的性能提升并不像以前那么显着（第6和第7），这与2D分割情况类似。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3OcN.png" alt="image-20231128143636223"  />
</p>
<h2 id="discussion-and-conclusion">DISCUSSION AND CONCLUSION<a hidden class="anchor" aria-hidden="true" href="#discussion-and-conclusion">#</a></h2>
<blockquote>
<p><strong>多病灶分割</strong>在临床场景中具有重要意义，因为<strong>某种疾病</strong>可能同时发生<strong>多种类型的感染，不同感染阶段的患者可能会出现不同类型的病灶</strong>。例如，<strong>磨玻璃样混浊</strong>（GGO）和<strong>实变</strong>（CON.）是COVID-19患者典型的肺部病变，前者<strong>通常发生在早期患者</strong>，而后者的增加<strong>可能表明病情恶化</strong>。胶质瘤可分为<strong>低级别胶质瘤</strong>（LGG）和<strong>胶质母细胞瘤</strong>，即<strong>高级别胶质瘤</strong>（GBM/HGG），并且在发生HGG的患者中更有可能发现<strong>强化肿瘤</strong>。因此，多病灶分割在患者的筛查和预后方面具有巨大的潜力。</p>
<p>多病灶分割问题可以分解为<strong>粗分割</strong>和<strong>细分割</strong>，粗分割是对病灶进行粗<strong>略分割</strong>，而细分割则<strong>基于前一分割</strong>，以产生最终的<strong>分割图</strong>。级联网络广泛用于多病变分割任务，因为这些算法背后的逻辑非常自然。然而，级联网络在潜在的临床部署中<strong>受到限制</strong>，因为它们<strong>缺乏灵活性</strong>并且对<strong>计算资源的要求很高</strong>。与现有的级联网络相比，我们开发了一种先验注意网络，它将粗分割和细分割集成到一个网络中。我们提出的网络架构的优点是<strong>分割性能</strong>和<strong>计算效率</strong>的平衡。通过将分割的两个步骤结合在一个网络中，所提出的<strong>先验注意网络</strong>能够实现多病灶分割的<strong>端到端训练</strong>，在训练和推理方面都具有更大的灵活性，并且在临床部署中具有更大的潜力。此外，我们设法保持所提出的先验注意网络的性能，在分割性能和运行效率之间实现复杂的平衡。</p>
<p>与级联 U-Net 相比，我们提出的先验注意力网络在 2D 和 3D 任务中都实现了更好的性能和效率，如图 6 所示。</p>
<p>总之，我们提出了一种新颖的分割网络，即先验注意网络，用于医学图像中的多病灶分割。受流行的从粗到细策略的启发，我们通过<strong>空间注意机制</strong>将级联网络的两个步骤聚合成一个网络。此外，我们引入了一种新颖的<strong>中间监督机制</strong>来指导与<strong>病变相关的注意图</strong>的生成，这可以指导解码器中的后续多类分割。所提出的网络在 2D 和 3D 医学图像（包括 CT 扫描和多模态 MRI）上进行评估。对于 2D 分割，与级联 U-Net 相比，所提出的先验注意力网络以更少的计算成本获得了有竞争力的结果。对于 3D 脑肿瘤分割，所提出的先验注意网络在 BraTS 2020 验证数据集上产生了最先进的性能，并且优于基于 U-Net 的其他基线方法。实验结果表明，该方法在医学影像中的许多多病灶分割任务中具有巨大的应用潜力，与二元分割相比可以提供更多信息，并有助于医生未来的临床诊断。</p>
</blockquote>
<p><img loading="lazy" src="https://i.imgs.ovh/2023/11/28/p3uCt.png" alt="image-20231128144442585"  />
</p>


  </div>



  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://swimmingliu.github.io/tags/unet/">Unet</a></li>
      <li><a href="https://swimmingliu.github.io/tags/deepsupervision/">DeepSupervision</a></li>
      <li><a href="https://swimmingliu.github.io/tags/intermediatesupervision/">IntermediateSupervision</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://swimmingliu.github.io/posts/diary/2023-%E5%9C%B0%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8/">
    <span class="title">Next »</span>
    <br>
    <span>地大服务器使用教程</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://swimmingliu.github.io/">SwimmingLiu&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
